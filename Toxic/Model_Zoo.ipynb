{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re, csv, codecs, operator, sys, gc\n",
    "from collections import defaultdict, OrderedDict\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import lightgbm as lgb\n",
    "from itertools import repeat\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from textblob import TextBlob    # For pos-tagging\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from scipy.special import logit, expit\n",
    "\n",
    "from keras import optimizers, initializers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Lambda, Embedding, Dropout, Activation, SpatialDropout1D, Reshape, \\\n",
    "GlobalAveragePooling1D, merge, Flatten, Bidirectional, CuDNNGRU, add, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras import backend as K\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'datasets/'\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "FAST_TEXT_EMBEDDING = 'pretrain_embedding/crawl-300d-2M.vec'\n",
    "GLOVE_EMBEDDING = 'pretrain_embedding/glove.840B.300d.txt'\n",
    "CLEAN_WORD_PATH = None\n",
    "TRAIN_DATA_FILE = 'train.csv'\n",
    "TEST_DATA_FILE = 'test.csv'\n",
    "MAX_SEQUENCE_LENGTH = 350\n",
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300\n",
    "FOLD_COUNT = 10\n",
    "BATCH_SIZE = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrain Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrain_embedding(file):\n",
    "    \"\"\"\n",
    "    Return a dictionary whose key is word, value is pretrained word embedding.\n",
    "    \"\"\"\n",
    "    print('Indexing word vectors')\n",
    "    embeddings_index = {}\n",
    "    f = open(file, 'r', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        try:\n",
    "            word = values[0]\n",
    "            coefs = np.array(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        except:\n",
    "            print(\"Error on: \", values[:3])\n",
    "    f.close()\n",
    "    print(\"Total %s word vectors\" % len(embeddings_index))\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_words(file):\n",
    "    \"\"\"\n",
    "    Return a dictionary whose key is typo, value is correct word.\n",
    "    \"\"\"\n",
    "    clean_word_dict = {}\n",
    "    with open(file, 'r', encoding='utf-8'):\n",
    "        for line in f:\n",
    "            line = line.strip('\\n')\n",
    "            typo, correct = line.split(',')\n",
    "            clean_word_dict[typo] = correct\n",
    "    return clean_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Total 2000000 word vectors\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = load_pretrain_embedding(FAST_TEXT_EMBEDDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings_index = load_pretrain_embedding(GLOVE_EMBEDDING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('datasets/train.csv')\n",
    "test_df = pd.read_csv('datasets/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comments = train_df['comment_text'].values\n",
    "test_comments = test_df['comment_text'].values\n",
    "train_comments_lengths = [len(s) for s in train_comments]\n",
    "test_comments_lengths = [len(s) for s in test_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_comments(arr):\n",
    "    print(\"MAX LENGTH:\\t\\t\", np.max(arr))\n",
    "    print(\"AVG LENGTH:\\t\\t\", np.average(arr))\n",
    "    print(\"MIN LENGTH:\\t\\t\", np.min(arr))\n",
    "    print(\"STANDARD DIVISION:\\t\", np.std(arr))\n",
    "    print(\"RANGE:\\t\\t\\t\", np.min(arr), \" to \", np.average(arr) + 2 * np.std(arr))\n",
    "    \n",
    "print(\"------Train------\")\n",
    "explore_comments(train_comments_lengths)\n",
    "\n",
    "print(\"------Test------\")\n",
    "explore_comments(test_comments_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name in list_classes:\n",
    "    print('{}\\n{}\\n'.format(class_name, train_df[class_name].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Cleaned Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEAN_WORD_PATH == None:\n",
    "    ignored_words = set(stopwords.words('english'))\n",
    "else:\n",
    "    ignored_words = load_clean_words(CLEAN_WORD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_files = sorted([f for f in listdir('datasets/feature_files/') if isfile(join('datasets/feature_files/', f))])\n",
    "for file in feature_files:\n",
    "    fixed_df = pd.read_csv('datasets/feature_files/' + file)\n",
    "    train_df = train_df.merge(fixed_df, on='id', how='left')\n",
    "    test_df = test_df.merge(fixed_df, on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_data(path):\n",
    "    files = sorted([f for f in listdir(path) if isfile(join(path, f))])\n",
    "    for i, file in enumerate(files):\n",
    "        temp_df = pd.read_csv(path + file)\n",
    "        print('Datasets before ensemble null number:', temp_df.isnull().sum().sum())\n",
    "        if i == 0:\n",
    "            ensembled = temp_df\n",
    "        else:\n",
    "            ensembled = ensembled.merge(temp_df, on='id', how='left')\n",
    "            print('Datasets after ensemble null number:', ensembled.isnull().sum().sum())\n",
    "    \n",
    "    column_numbers = ensembled.shape[1]\n",
    "    toxic = ensembled.iloc[:, [i for i in range(2, column_numbers, len(list_classes))]]\n",
    "    severe_toxic = ensembled.iloc[:, [i for i in range(3, column_numbers, len(list_classes))]]\n",
    "    obscene = ensembled.iloc[:, [i for i in range(4, column_numbers, len(list_classes))]]\n",
    "    threat = ensembled.iloc[:, [i for i in range(5, column_numbers, len(list_classes))]]\n",
    "    insult = ensembled.iloc[:, [i for i in range(6, column_numbers, len(list_classes))]]\n",
    "    identity_hate = ensembled.iloc[:, [i for i in range(7, column_numbers, len(list_classes))]]\n",
    "    \n",
    "    return toxic, severe_toxic, obscene, threat, insult, identity_hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_toxic, train_severe_toxic, train_obscene, train_threat, train_insult, train_identity_hate = ensemble_data('datasets/ensemble_files/train/')\n",
    "test_toxic, test_severe_toxic, test_obscene, test_threat, test_insult, test_identity_hate = ensemble_data('datasets/ensemble_files/test/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal = re.compile(r'[^?!.,:a-z\\d ]', re.IGNORECASE)\n",
    "# Regex to remove all numerics\n",
    "replace_numbers = re.compile(r'\\d+', re.IGNORECASE)\n",
    "\n",
    "word_count_dict = defaultdict(int)\n",
    "\n",
    "def clean_datasets(text, remove_stopwords=False, stem_words=False, count_null_words=True, clean_wiki_tokens=True):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \"\", text)\n",
    "    text = re.sub(r\"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}\", \"\", text)\n",
    "    \n",
    "    if clean_wiki_tokens:\n",
    "        text = re.sub(r\"\\.jpg\", \" \", text)\n",
    "        text = re.sub(r\"\\.png\", \" \", text)\n",
    "        text = re.sub(r\"\\.gif\", \" \", text)\n",
    "        text = re.sub(r\"\\.bmp\", \" \", text)\n",
    "        text = re.sub(r\"\\.pdf\", \" \", text)\n",
    "        text = re.sub(r\"image:\", \" \", text)\n",
    "        text = re.sub(r\"#([A-Fa-f0-9]{6}|[A-Fa-f0-9]{3})\", \" \",text)\n",
    "        \n",
    "        text = re.sub(r\"\\{\\|[^\\}]*\\|\\}\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[user:.*\\]\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[user:.*\\|\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[wikipedia:.*\\]\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[wikipedia:.*\\|\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[special:.*\\]\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[special:.*\\|\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[category:.*\\]\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[category:.*\\|\", \" \", text)\n",
    "        \n",
    "        text = re.sub(r\"wp:\", \" \", text)\n",
    "        text = re.sub(r\"file:\", \" \", text)\n",
    "    \n",
    "    # For Punctuation\n",
    "    text = re.sub(r\"”\", \"\\\"\", text)\n",
    "    text = re.sub(r\"“\", \"\\\"\", text)\n",
    "    text = re.sub(r\"´\", \"'\", text)\n",
    "    text = re.sub(r\"—\", \" \", text)\n",
    "    text = re.sub(r\"’\", \"'\", text)\n",
    "    text = re.sub(r\"‘\", \"'\", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\"−\", \" \", text)\n",
    "    text = re.sub(r\"\\?\", \" ? \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"#\", \" # \", text)\n",
    "    \n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"i’m\", \"i am\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"<3\", \" love \", text)\n",
    "        \n",
    "    text = replace_numbers.sub('', text)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        for typo, correct in ignored_words.items():\n",
    "            text = re.sub(typo, correct, text)\n",
    "    \n",
    "    if count_null_words:\n",
    "        text = text.split()\n",
    "        for t in text:\n",
    "            word_count_dict[t] += 1\n",
    "        text = \" \".join(text)\n",
    "        \n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_train = train_df['comment_text'].fillna('no comment').values\n",
    "list_sentences_test = test_df['comment_text'].fillna('no comment').values\n",
    "train_labels = train_df[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data cleaning...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "cleaned_train_comments, cleaned_test_comments = [], []\n",
    "print('Processing data cleaning...')\n",
    "\n",
    "for text in list_sentences_train:\n",
    "    cleaned_train_comments.append(clean_datasets(text))\n",
    "for text in list_sentences_test:\n",
    "    cleaned_test_comments.append(clean_datasets(text))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['comment_text_cleaned'] = cleaned_train_comments\n",
    "test_df['comment_text_cleaned'] = cleaned_test_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Meta (Extend) Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unknown_embedding(t, idx):\n",
    "    t = t.split()\n",
    "    res = 0\n",
    "    for word in t:\n",
    "        if word not in idx.keys():\n",
    "            res += 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_regexp_occ(regexp='', text=None):\n",
    "    \"\"\"\n",
    "    Simple way to calculate the number of occurence of a regex\n",
    "    \"\"\"\n",
    "    if len(text) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(re.findall(regexp, text)) / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_meta_feature(df):\n",
    "    df['total_length'] = df['comment_text'].apply(len)\n",
    "    df['capital'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.apply(lambda row: float(row['capital']) / float(row['total_length']), axis=1)\n",
    "    df[\"unknown_glove\"] = df['comment_text_cleaned'].apply(lambda x: count_unknown_embedding(x, glove_embeddings_index))\n",
    "    df[\"unknown_fasttext\"] = df['comment_text_cleaned'].apply(lambda x: count_unknown_embedding(x, embeddings_index))\n",
    "    df[\"unknown_glove_fasttext\"] = df[\"unknown_glove\"] + df[\"unknown_fasttext\"]\n",
    "    # Special Expressions Collection\n",
    "    df['num_exclamation_marks'] = df['comment_text'].apply(lambda comment: comment.count('!'))\n",
    "    df['num_question_marks'] = df['comment_text'].apply(lambda comment: comment.count('?'))\n",
    "    df['num_punctuation'] = df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '.,;:'))\n",
    "    df['num_symbols'] = df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '*&$%'))\n",
    "    df['num_words'] = df['comment_text'].apply(lambda comment: len(comment.split()))\n",
    "    df['num_unique_words'] = df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']\n",
    "    df['num_smilies'] = df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n",
    "    df[\"ant_slash_n\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\n\", x))\n",
    "    df[\"start_with_columns\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"^\\:+\", x))\n",
    "    df[\"has_emphasize_equal\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\={2}.+\\={2}\", x))\n",
    "    df[\"has_emphasize_quotes\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\\"{4}\\S+\\\"{4}\", x))\n",
    "    df[\"has_star\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\*\", x))\n",
    "    ## Check for dates\n",
    "    df[\"has_timestamp\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\d{2}|:\\d{2}\", x))\n",
    "    df[\"has_date_long\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{2}:\\d{2}, \\d{1,2} \\w+ \\d{4}\", x))    # example: 18:44, 8 December 2010\n",
    "    df[\"has_date_short\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{1,2} \\w+ \\d{4}\", x))\n",
    "    ## Check for http links\n",
    "    df[\"has_http\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"http[s]{0,1}://\\S+\", x))\n",
    "    df[\"has_ip\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(\"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}\", x))\n",
    "    ## Check for mail\n",
    "    df[\"has_mail\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', x))\n",
    "    ## Check for image\n",
    "    df[\"has_image\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r'image\\:', x))\n",
    "    # Dirty Languages Collection\n",
    "    df[\"nb_fk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ff]\\S{2}[Kk]\", x))\n",
    "    df[\"nb_sk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ss]\\S{2}[Kk]\", x))\n",
    "    df[\"nb_dk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[dD]ick\", x))\n",
    "    df[\"nb_you\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\W[Yy]ou\\W\", x))\n",
    "    df[\"nb_mother\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wmother\\W\", x))\n",
    "    df[\"nb_ng\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wnigger\\W\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating meta features...\")\n",
    "create_meta_feature(train_df)\n",
    "create_meta_feature(test_df)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = train_df.columns[9:]\n",
    "print(train_cols)\n",
    "column_numbers = len(train_df.columns)\n",
    "print(column_numbers, len(train_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_cols:\n",
    "    train_toxic[col] = train_df[col]\n",
    "    train_severe_toxic[col] = train_df[col]\n",
    "    train_obscene[col] = train_df[col]\n",
    "    train_threat[col] = train_df[col]\n",
    "    train_insult[col] = train_df[col]\n",
    "    train_identity_hate[col] = train_df[col]\n",
    "for col in train_cols:\n",
    "    test_toxic[col] = test_df[col]\n",
    "    test_severe_toxic[col] = test_df[col]\n",
    "    test_obscene[col] = test_df[col]\n",
    "    test_threat[col] = test_df[col]\n",
    "    test_insult[col] = test_df[col]\n",
    "    test_identity_hate[col] = test_df[col]\n",
    "train_ensembled_data = [train_toxic, train_severe_toxic, train_obscene, train_threat, train_insult, train_identity_hate]\n",
    "test_ensembled_data = [test_toxic, test_severe_toxic, test_obscene, test_threat, test_insult, test_identity_hate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del list_sentences_train, list_sentences_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312735"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_comment_text = pd.concat([train_df['comment_text_cleaned'], test_df['comment_text_cleaned']], axis=0).fillna(\"unknown\")\n",
    "nrow_train = train_df.shape[0]\n",
    "all_comment_text.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vect = TfidfVectorizer(max_features=5000, stop_words=ignored_words)\n",
    "word_vect.fit(all_comment_text)\n",
    "data = word_vect.fit_transform(all_comment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312735, 5000)\n"
     ]
    }
   ],
   "source": [
    "norm_data = MaxAbsScaler().fit_transform(data)\n",
    "print(norm_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=20000\n",
    ")\n",
    "word_vectorizer.fit(all_comment_text)\n",
    "train_word_features = word_vectorizer.transform(cleaned_train_comments)\n",
    "test_word_features = word_vectorizer.transform(cleaned_test_comments)\n",
    "print('Word vectorization process Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 6),\n",
    "    max_features=30000\n",
    ")\n",
    "char_vectorizer.fit(all_comment_text)\n",
    "train_char_features = char_vectorizer.transform(cleaned_train_comments)\n",
    "test_char_features = char_vectorizer.transform(cleaned_test_comments)\n",
    "print('Char vectorization process Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_comment_text\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf_features = hstack([train_char_features, train_word_features]).tocsr()\n",
    "test_tfidf_features = hstack([test_char_features, test_word_features]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_char_features, train_word_features, test_char_features, test_word_features\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary & Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='\"#$%&()+,-./:;<=>@[\\\\]^_`{|}~\\t\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically train vocab & tokenizer...\n",
      "Found 330549 unique tokens\n",
      "Shape of train_data tensor:  (159571, 350)\n",
      "Shape of train_label tensor:  (159571, 6)\n",
      "Shape of test_data tensor:  (153164, 350)\n"
     ]
    }
   ],
   "source": [
    "print('Automatically train vocab & tokenizer...')\n",
    "tokenizer.fit_on_texts(cleaned_train_comments + cleaned_test_comments)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(cleaned_train_comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(cleaned_test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of train_data tensor: ', train_data.shape)\n",
    "print('Shape of train_label tensor: ', train_labels.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor: ', test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, char_level=True, lower=False)\n",
    "\n",
    "print('Automatically train vocab & tokenizer...')\n",
    "tokenizer.fit_on_texts(cleaned_train_comments + cleaned_test_comments)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(cleaned_train_comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(cleaned_test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of train_data tensor: ', train_data.shape)\n",
    "print('Shape of train_label tensor: ', train_labels.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor: ', test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pos Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2pos(sentence):\n",
    "    try:\n",
    "        tag = TextBlob(sentence).tags\n",
    "    except:\n",
    "        print(sentence)\n",
    "        \n",
    "    updated_sentence = ' '.join([i[0] for i in tag])\n",
    "    tagged = ' '.join([i[1] for i in tag])\n",
    "    return updated_sentence, tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_word_index = {v: k for k, v in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pos_comments = []\n",
    "Pos_updated_sentence = []\n",
    "for text in tqdm(train_sequences):\n",
    "    text_ = ' '.join([inverse_word_index[word] for word in text])    # convert to word format\n",
    "    if not isinstance(text_, str):\n",
    "        print(text, '\\n', text_)\n",
    "    updated_sentence, tags = sent2pos(text_)\n",
    "    Pos_updated_sentence.append(updated_sentence)\n",
    "    Pos_comments.append(tags)\n",
    "    assert len(updated_sentence.split(' ')) == len(tags.split(' ')), \"T1 {} T2 {}\".format(len(text), len(tags.split()))\n",
    "    \n",
    "Pos_test_comments = []\n",
    "Pos_test_updated_sentence = []\n",
    "for text in tqdm(test_sequences):\n",
    "    text_ = ' '.join([inverse_word_index[word] for word in text])\n",
    "    updated_test_sentence, test_tags = sent2pos(text_)\n",
    "    Pos_test_updated_sentence.append(updated_test_sentence)\n",
    "    Pos_test_comments.append(test_tags)\n",
    "    assert len(updated_test_sentence.split(' ')) == len(test_tags.split(' ')), \"T1 {} T2 {}\".format(len(text), len(test_tags.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tokenizer = Tokenizer(num_words=50, filters='\"#$%&()+,-./:;<=>@[\\\\]^_`{|}~\\t\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Automatically train pos tokenizer...')\n",
    "pos_tokenizer.fit_on_texts(Pos_comments + Pos_test_comments)\n",
    "\n",
    "train_pos_sequences = pos_tokenizer.texts_to_sequences(Pos_comments)\n",
    "test_pos_sequences = pos_tokenizer.texts_to_sequences(Pos_test_comments)\n",
    "\n",
    "pos_word_index = pos_tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(pos_word_index))\n",
    "\n",
    "pos_train_data = pad_sequences(train_pos_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', pos_train_data.shape)\n",
    "\n",
    "pos_test_data = pad_sequences(test_pos_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', pos_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second time valid for tokenzier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Automatically train pos tokenizer secondly...')\n",
    "cleaned_train_comments, cleaned_test_comments = [], []\n",
    "for text in Pos_updated_sentence:\n",
    "    cleaned_train_comments.append(clean_datasets(text))\n",
    "for text in Pos_test_updated_sentence:\n",
    "    cleaned_test_comments.append(clean_datasets(text))\n",
    "    \n",
    "train_sequences = tokenizer.texts_to_sequences(cleaned_train_comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(cleaned_test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of train_data tensor:', train_data.shape)\n",
    "print('Shape of train_label tensor:', train_labels.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['cleaned_comment_text'] = cleaned_train_comments\n",
    "test_df['cleaned_comment_text'] = cleaned_test_comments\n",
    "train_df.to_csv(PATH + 'cleaned_train.csv', index=False)\n",
    "test_df.to_csv(PATH + 'cleaned_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cleaned_train_comments, cleaned_test_comments\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer for Pytorch Variable (Batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batches_generator(inputs, targets, batch_size, row_shuffle=False):\n",
    "    inputs_data_size = len(inputs)\n",
    "    targets_data_size = len(targets)\n",
    "    assert inputs_data_size == targets_data_size, \"The length of inputs({}) and targets({}) must be consistent.\".format(inputs_data_size, targets_data_size)\n",
    "    \n",
    "    if row_shuffle:\n",
    "        for input_seqs in inputs:\n",
    "            np.random.shuffle(input_seqs)\n",
    "            \n",
    "    shuffled_input, shuffled_target = shuffle(inputs, targets)\n",
    "    mini_batches = [\n",
    "        (shuffled_input[k: k + batch_size], shuffled_target[k: k + batch_size])\n",
    "        for k in range(0, inputs_data_size, batch_size)\n",
    "    ]\n",
    "    dp = EmbeddingDropout(p=0.2)\n",
    "    \n",
    "    for batch_xs, batch_ys in mini_batches:\n",
    "        lengths = [len(s) for s in batch_xs]\n",
    "        max_length = min(MAX_SEQUENCE_LENGTH, max(lengths))\n",
    "        batch_tensors = pad_sequences(batch_xs, maxlen=max_length, padding='post', truncating='pre')\n",
    "        \n",
    "        lengths_var = Variable(torch.Tensor(lengths), requires_grad=False)\n",
    "        inputs_tensor = torch.from_numpy(batch_tensors).long()\n",
    "        inputs_dropped_tensor = dp.forward(inputs_tensor)\n",
    "        inputs_var = Variable(inputs_dropped_tensor)\n",
    "        targets_var = Variable(torch.from_numpy(batch_ys).long())\n",
    "        \n",
    "        if use_cuda:\n",
    "            inputs_var = inputs_var.cuda()\n",
    "            targets_var = targets_var.cuda()\n",
    "            lengths_var = lengths_var.cuda()\n",
    "        yield (inputs_var, lengths_var), targets_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batches_generator(inputs, batch_size):\n",
    "    inputs_data_size = len(inputs)\n",
    "    mini_batches = [\n",
    "        inputs[k: k + batch_size]\n",
    "        for k in range(0, inputs_data_size, batch_size)\n",
    "    ]\n",
    "    \n",
    "    for batch_xs in mini_batches:\n",
    "        lengths = [len(s) for s in batch_xs]\n",
    "        max_length = min(MAX_SEQUENCE_LENGTH, max(lengths))\n",
    "        batch_tensors = pad_sequences(batch_xs, maxlen=max_length, padding='post', truncating='pre')\n",
    "        \n",
    "        lengths_var = Variable(torch.Tensor(lengths))\n",
    "        inputs_var = Variable(torch.from_numpy(batch_tensors).long())\n",
    "        \n",
    "        if use_cuda:\n",
    "            inputs_var = inputs_var.cuda()\n",
    "            lengths_var = lengths_var.cuda()\n",
    "            \n",
    "        yield (inputs_var, lengths_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embedding (Build a matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix...\n",
      "Null_word embeddings: 24244\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix...')\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "null_words = open(PATH + 'null_words.txt', 'w', encoding='utf-8')\n",
    "\n",
    "for word, idx in word_index.items():\n",
    "    if idx >= MAX_NB_WORDS:\n",
    "        null_words.write(word + ', ' + str(word_count_dict[word]) + '\\n')\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "    else:\n",
    "        null_words.write(word + ', ' + str(word_count_dict[word]) + '\\n')\n",
    "print('Null_word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting null_words...\n",
      "Sorting operation Done!\n"
     ]
    }
   ],
   "source": [
    "print('Sorting null_words...')\n",
    "null_dict = {}\n",
    "with open(PATH + 'null_words.txt', 'r', encoding='utf-8') as nullword:\n",
    "    for line in nullword:\n",
    "        word, count = line.strip('\\n').split(', ')\n",
    "        null_dict[word] = int(count)\n",
    "\n",
    "null_dict = sorted(null_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "with open(PATH + 'null_words.txt', 'w', encoding='utf-8') as output:\n",
    "    for word, count in null_dict:\n",
    "        output.write(word + ', ' + str(count) + '\\n')\n",
    "print('Sorting operation Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model with Using Tfidf Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=FOLD_COUNT, shuffle=False)\n",
    "tfidf_models = []\n",
    "for i, (train_idx, test_idx) in enumerate(kfold.split(train_tfidf_features)):\n",
    "    print('## In fold {} ##'.format(i + 1))\n",
    "    classifier = LogisticRegression(solver='sag', C=12.0)\n",
    "    \n",
    "    for class_name in list_classes:\n",
    "        print('Processing {} ...'.format(class_name))\n",
    "        train_target = train_df[class_name][train_idx]\n",
    "        \n",
    "        classifier.fit(train_tfidf_features[train_idx], train_target)\n",
    "        y_pred = classifier.predict(train_tfidf_features[train_idx])\n",
    "        \n",
    "        print('Training accuracy is {}'.format(accuracy_score(y_pred, train_target)))\n",
    "        \n",
    "        val_target = train_df[class_name][test_idx]\n",
    "        val_pred = classifier.predict(train_tfidf_features[test_idx])\n",
    "        \n",
    "        print('Validation accuracy is {}'.format(accuracy_score(val_pred, val_target)))\n",
    "        \n",
    "    tfidf_models.append(classifier)\n",
    "print('K-fold cross validation Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(tfidf_models):\n",
    "    print('## In Model {} ##'.format(i + 1))\n",
    "    # predictions = {'id': test_df['id']}\n",
    "    predictions = OrderedDict()\n",
    "    predictions['id'] = test_df['id']\n",
    "    \n",
    "    for class_name in list_classes:\n",
    "        predictions[class_name] = model.predict_proba(test_tfidf_features)[:, 1]\n",
    "        print('Predict the proba for {} Done!'.format(class_name))\n",
    "        print(predictions.keys())\n",
    "    \n",
    "    print(predictions.keys())\n",
    "    submission = pd.DataFrame.from_dict(predictions)\n",
    "    submission.to_csv('results/TFIDF_Based/TFIDF_Logistic_Regression_Submission_{}.csv'.format(i), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = ['results/TFIDF_Based/TFIDF_Logistic_Regression_Submission_{}.csv'.format(i) for i in range(0, FOLD_COUNT)]\n",
    "bagging(result_list, 'results/TFIDF_Based/TFIDF_bagging.csv')\n",
    "print('Bagging operation Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExtraTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "# et_predictions = {'id': test_df['id']}\n",
    "et_predictions = OrderedDict()\n",
    "et_predictions['id'] = test_df['id']\n",
    "\n",
    "for class_name in list_classes:\n",
    "    train_target = train_df[class_name]\n",
    "    classifier = ExtraTreesClassifier(n_estimators=30)\n",
    "    \n",
    "    cv_score = np.mean(cross_val_score(classifier, train_tfidf_features, train_target, cv=10, scoring='roc_auc'))\n",
    "    accs.append(cv_score)\n",
    "    print('CV Score for class {} is {}'.format(class_name, cv_score))\n",
    "    \n",
    "    classifier.fit(train_tfidf_features, train_target)\n",
    "    print(classifier.feature_importances_)\n",
    "    et_predictions[class_name] = classifier.predict_proba(test_tfidf_features)[:, 1]\n",
    "    \n",
    "submission = pd.DataFrame.from_dict(et_predictions)\n",
    "submission.to_csv('result/TFIDF_Based/TFIDF_ExtraTreesClassifier_Submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light-GBM For Ensembled_Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_every_feature_model(feature_data, label, feature_name, feature_test_data, fold_count, predict=False):\n",
    "    predictions = np.zeros(shape=(len(feature_test_data)))\n",
    "    fold_size = len(feature_data) // fold_count\n",
    "    \n",
    "    print('Feature name is {}'.format(feature_name))\n",
    "    auc_score = 0\n",
    "    for fold_id in range(0, fold_count):\n",
    "        print('## Fold In {} ##'.format(fold_id + 1))\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "        if fold_id == fold_size - 1:\n",
    "            fold_end = len(feature_data)\n",
    "            \n",
    "        train_x = np.concatenate((feature_data[:fold_start], feature_data[fold_end:]))\n",
    "        train_y = np.concatenate((label[:fold_start], label[fold_end:]))\n",
    "        \n",
    "        val_x = feature_data[fold_start:fold_end]\n",
    "        val_y = label[fold_start:fold_end]\n",
    "        \n",
    "        lgb_train = lgb.Dataset(train_x, train_y)\n",
    "        lgb_val = lgb.Dataset(val_x, val_y)\n",
    "        \n",
    "        lgbm_model = lgb.LGBMClassifier(max_depth=5, metric='auc', n_estimators=10000, num_leaves=32, boosting_type='gbdt', \\\n",
    "                                       learning_rate=0.01, feature_fraction=0.3, bagging_fraction=0.8, bagging_freq=5, reg_lambda=0)\n",
    "        lgbm_model.fit(X=train_x, y=train_y, eval_metric=['auc', 'binary_logloss'], eval_set=(val_x, val_y), early_stopping_rounds=1000, verbose=500)\n",
    "        auc_score += lgbm_model.best_score_['valid_0']['auc']\n",
    "        lgb.plot_importance(lgbm_model, max_num_features=30)\n",
    "        plt.show()\n",
    "        if predict:\n",
    "            prediction = lgbm_model.predict_proba(feature_test_data)[:, 1]\n",
    "            predictions += prediction\n",
    "            del lgbm_model\n",
    "    predictions /= fold_count\n",
    "    print('Training LightGBM Done!')\n",
    "    return predictions, auc_score / fold_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, auc_scores = [], []\n",
    "for i, feature_name in enumerate(list_classes):\n",
    "    prediction, auc = fit_every_feature_model(train_ensembled_data[i], train_df[feature_name].values, feature_name, test_ensembled_data[i], 10, predict=True)\n",
    "    auc_scores.append(auc)\n",
    "    predictions.append(prediction)\n",
    "print('Overall AUC Score is {}'.format(sum(auc_scores) / 6))\n",
    "print('For each:'.format(auc_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('datasets/sample_submission.csv')\n",
    "for i, feature_name in enumerate(list_classes):\n",
    "    submission[feature_name] = predictions[i]\n",
    "submission.to_csv('results/LightGBM/LightGBM_with_Meta_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Styles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAMP = 'model_pool/Keras/av_pos_cnn/pavel_cnn_%.2f_%.2f'%(0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_model_by_auc(model, batch_size, train_x, train_y, val_x, val_y):\n",
    "    best_auc = -1\n",
    "    best_weight = None\n",
    "    best_epoch = 0\n",
    "    current_epoch = 1\n",
    "    \n",
    "    while True:\n",
    "        model.fit(train_x, train_y, batch_size=batch_size, epoch=1, validation_data=[val_x, val_y])\n",
    "        y_pred = model.predict(val_x, batch_size=batch_size)\n",
    "        current_auc = roc_auc_score(val_y, y_pred)\n",
    "        print('Epoch {} auc {:.6f} best_auc {:.6f}'.format(current_epoch, current_auc, best_auc))\n",
    "        current_epoch += 1\n",
    "        if best_auc < current_auc or best_auc == -1:\n",
    "            best_auc = current_auc\n",
    "            best_weight = model.get_weights()\n",
    "            best_epoch = current_epoch\n",
    "        else:\n",
    "            # early stop\n",
    "            if current_epoch - best_epoch == 5:\n",
    "                break\n",
    "                \n",
    "    model.set_weights(best_weights)\n",
    "    return model, best_auc\n",
    "\n",
    "def _train_model_by_logloss(model, batch_size, train_x, pos_train_x, train_y, val_x, pos_val_x, val_y, fold_id):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=7)\n",
    "    best_model_path = STAMP + str(fold_id) + '.h5'\n",
    "    model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n",
    "    train_data = {'Onehot': train_x, 'POS': pos_train_x}\n",
    "    val_data = {'Onehot': val_x, 'POS': pos_val_x}\n",
    "    \n",
    "    hist = model.fit(train_data, train_y, validation_data=(val_data, val_y), epochs=50, batch_size=batch_size, shuffle=True, callbacks=[early_stopping, model_checkpoint])\n",
    "    best_val_score = min(hist.history['val_loss'])\n",
    "    predictions = model.predict(val_data)\n",
    "    auc = roc_auc_score(val_y, predictions)\n",
    "    print('AUC Score', auc)\n",
    "    return model, best_val_score, auc, predictions\n",
    "\n",
    "def train_folds(x, pos_x, y, fold_count, batch_size, get_model_func):\n",
    "    fold_size = len(x) // fold_count\n",
    "    models = []\n",
    "    fold_predictions = []\n",
    "    score, total_auc = 0, 0\n",
    "    for fold_id in range(0, fold_count):\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "        \n",
    "        if fold_id == fold_count - 1:\n",
    "            fold_end = len(x)\n",
    "            \n",
    "        train_x = np.concatenate((x[:fold_start], x[fold_end:]))\n",
    "        train_y = np.concatenate((y[:fold_start], y[fold_end:]))\n",
    "        \n",
    "        val_x = x[fold_start: fold_end]\n",
    "        val_y = y[fold_start: fold_end]\n",
    "        \n",
    "        pos_train_x = np.concatenate((pos_x[:fold_start], pos_x[fold_end:]))\n",
    "        pos_val_x = pos_x[fold_start: fold_end]\n",
    "        print('## In fold {} ## : '.format(fold_id + 1))\n",
    "        model, best_val_score, auc, fold_prediction = _train_model_by_logloss(get_model_func, batch_size, train_x, pos_train_x, train_y, val_x, pos_val_x, val_y, fold_id)\n",
    "        \n",
    "        score += best_val_score\n",
    "        total_auc += auc\n",
    "        fold_predictions.append(fold_prediction)\n",
    "        models.append(model)\n",
    "    return models, score / fold_count, total_auc / fold_count , fold_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_log_loss(y, y_pred):\n",
    "    total_loss = 0\n",
    "    for j in range(6):\n",
    "        loss = log_loss(y[:, j], y_pred[:, j])\n",
    "        total_loss += loss\n",
    "    total_loss /= 6.\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "epoch_num = 100\n",
    "early_stop_round = 12\n",
    "MODELSTAMP = 'model_pool/Pytorch/rhn/pavel_rhn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_decay(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr'] * 0.93\n",
    "    return optimizer\n",
    "\n",
    "def _train_batch(model, inputs_var, targets_var, optimizer):\n",
    "    optimizer.zero_grad()\n",
    "    model.train()\n",
    "    preds_var = model.forward(inputs_var)\n",
    "    \n",
    "    # Pytorch version requires torch.cuda.FloatTensor rather than torch.cuda.LongTensor\n",
    "    preds_var = preds_var.type(torch.cuda.FloatTensor)\n",
    "    targets_var = targets_var.type(torch.cuda.FloatTensor)\n",
    "    \n",
    "    # training\n",
    "    loss = F.binary_cross_entropy_with_logits(preds_var, targets_var)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "    optimizer.step()\n",
    "    return optimizer, loss.data.cpu().numpy()\n",
    "\n",
    "def _pytorch_train_model_by_logloss(model, batch_size, train_x, train_y, val_x, val_y, fold_id):\n",
    "    '''\n",
    "    Train for 6-labels at the same time.\n",
    "    '''\n",
    "    print(\"## Training on fold {} ##\".format(fold_id))\n",
    "    \n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    best_auc, best_logloss, best_epoch, current_epoch = -1, -1, 0, 1\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "    \n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_logloss = 0.0\n",
    "        for batch_id, (inputs_var, targets_var) in enumerate(mini_batches_generator(train_x, train_y, batch_size, row_shuffle=True)):\n",
    "            optimizer, loss = _train_batch(model=model, inputs_var=inputs_var, targets_var=targets_var, optimizer=optimizer)\n",
    "            \n",
    "            epoch_logloss += loss\n",
    "            if batch_id % 40 == 0:\n",
    "                print(\"Epoch: {} Batch: {} Log-loss: {}\".format(epoch + 1, batch_id, loss))\n",
    "                \n",
    "        print(\"Epoch average log-loss: {}\".format(epoch_logloss / batch_id))\n",
    "        val_pred = model.predict(val_x)\n",
    "        \n",
    "        current_logloss = self_log_loss(val_y, val_pred)\n",
    "        current_epoch += 1\n",
    "        if best_logloss > current_logloss or best_logloss == -1:\n",
    "            best_logloss = current_logloss\n",
    "            model.save(MODELSTAMP + '-TEMP.pt')\n",
    "            best_auc = roc_auc_score(val_y, val_pred)\n",
    "            best_epoch = current_epoch\n",
    "        else:\n",
    "            if current_epoch - best_epoch == early_stop_round:\n",
    "                break\n",
    "        print(\"In Epoch: {}, val_loss: {}, best_val_loss: {}, best_auc: {}\".format(epoch + 1, current_logloss, best_logloss, best_auc))\n",
    "        optimizer = learning_rate_decay(optimizer)\n",
    "        \n",
    "    model.load(MODELSTAMP + '-TEMP.pt')\n",
    "    best_val_pred = model.predict(val_x)\n",
    "    model.save(MODELSTAMP + str(fold_id) + '.pt')\n",
    "    return model, best_logloss, best_auc, best_val_pred\n",
    "\n",
    "def pytorch_train_folds(x, y, fold_count, batch_size, get_model_func, skip_fold=0):\n",
    "    fold_size = len(x) // fold_count\n",
    "    models = []\n",
    "    fold_predictions = []\n",
    "    score, total_auc = 0, 0\n",
    "    for fold_id in range(0, fold_count):\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "        \n",
    "        if fold_id == fold_count - 1:\n",
    "            fold_end = len(x)\n",
    "            \n",
    "        train_x = np.concatenate((x[:fold_start], x[fold_end:]))\n",
    "        train_y = np.concatenate((y[:fold_start], y[fold_end:]))\n",
    "        \n",
    "        val_x = x[fold_start: fold_end]\n",
    "        val_y = y[fold_start: fold_end]\n",
    "        \n",
    "        if fold_id < skip_fold:\n",
    "            model = get_model_func()\n",
    "            model.load(MODELSTAMP + str(fold_id) + '.pt')\n",
    "            model = model.eval()\n",
    "            model = model.cuda()\n",
    "            fold_prediction = model.predict(val_x)\n",
    "            auc = roc_auc_score(val_y, fold_prediction)\n",
    "            bst_val_score = self_log_loss(y=val_y, y_pred=fold_prediction)\n",
    "        \n",
    "        else:\n",
    "            model, best_val_score, auc, fold_prediction = _pytorch_train_model_by_logloss(get_model_func(), batch_size, train_x, train_y, val_x, val_y, fold_id)\n",
    "        \n",
    "        score += best_val_score\n",
    "        total_auc += auc\n",
    "        fold_predictions.append(fold_prediction)\n",
    "        models.append(model)\n",
    "    return models, score / fold_count, total_auc / fold_count , fold_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Model for computing a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter per channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "        \n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        \n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        attn_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(attn_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, attn_weights]\n",
    "        return result\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "    \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMaxPooling(Layer):\n",
    "    \"\"\"\n",
    "    K-max pooling layer that extracts the k-highest activations from a sequence (2nd dimension).\n",
    "    With TensorFlow Backend.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_spec = InputSpec(ndim=3)\n",
    "        self.k = k\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[2] * self.k)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # top_k function can only be applied along the last dimension\n",
    "        shifted_input = tf.transpose(inputs, [0, 2, 1])\n",
    "        top_k = tf.nn.top_k(shifted_input, self.k, True, None)[0]\n",
    "        return Flatten()(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_av_cnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    filter_nums = 300\n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.25)(embedding_sequences)\n",
    "    \n",
    "    conv_0 = Conv1D(filter_nums, 1, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_1 = Conv1D(filter_nums, 2, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_2 = Conv1D(filter_nums, 3, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_3 = Conv1D(filter_nums, 4, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    \n",
    "    attn_0 = AttentionWeightedAverage()(conv_0)\n",
    "    avg_0 = GlobalAveragePooling1D()(conv_0)\n",
    "    maxpool_0 = GlobalMaxPooling1D()(conv_0)\n",
    "    \n",
    "    attn_1 = AttentionWeightedAverage()(conv_1)\n",
    "    avg_1 = GlobalAveragePooling1D()(conv_1)\n",
    "    maxpool_1 = GlobalMaxPooling1D()(conv_1)\n",
    "    \n",
    "    attn_2 = AttentionWeightedAverage()(conv_2)\n",
    "    avg_2 = GlobalAveragePooling1D()(conv_2)\n",
    "    maxpool_2 = GlobalMaxPooling1D()(conv_2)\n",
    "    \n",
    "    attn_3 = AttentionWeightedAverage()(conv_3)\n",
    "    avg_3 = GlobalAveragePooling1D()(conv_3)\n",
    "    maxpool_3 = GlobalMaxPooling1D()(conv_3)\n",
    "    \n",
    "    merged_maxpool = merge([maxpool_0, maxpool_1, maxpool_2, maxpool_3], mode='concat', concat_axis=1)\n",
    "    merged_attn = merge([attn_0, attn_1, attn_2, attn_3], mode='concat', concat_axis=1)\n",
    "    merged_avg = merge([avg_0, avg_1, avg_2, avg_3], mode='concat', concat_axis=1)\n",
    "    merged_tensor = merge([merged_maxpool, merged_attn, merged_avg], mode='concat', concat_axis=1)\n",
    "    \n",
    "    output = Dropout(0.7)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_av_pos_cnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words, \n",
    "                                embedding_dim, \n",
    "                                weights=[embedding_matrix], \n",
    "                                input_length=max_sequence_length, \n",
    "                                trainable=False)\n",
    "    \n",
    "    pos_embedding_layer = Embedding(50,\n",
    "                                    30,\n",
    "                                    input_length=max_sequence_length,\n",
    "                                    trainable=True)\n",
    "    \n",
    "    filter_nums = 325\n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    pos_input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='POS')\n",
    "    \n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    pos_sequences = pos_embedding_layer(pos_input_layer)\n",
    "    merged_embedding_layer = concatenate([embedding_sequences, pos_sequences])\n",
    "    final_embedding_sequences = SpatialDropout1D(0.25)(merged_embedding_layer)\n",
    "    \n",
    "    conv_0 = Conv1D(filter_nums, 1, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_1 = Conv1D(filter_nums, 2, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_2 = Conv1D(filter_nums, 3, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_3 = Conv1D(filter_nums, 4, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "\n",
    "    attn_0 = AttentionWeightedAverage()(conv_0)\n",
    "    avg_0 = GlobalAveragePooling1D()(conv_0)\n",
    "    maxpool_0 = GlobalMaxPooling1D()(conv_0)\n",
    "    \n",
    "    attn_1 = AttentionWeightedAverage()(conv_1)\n",
    "    avg_1 = GlobalAveragePooling1D()(conv_1)\n",
    "    maxpool_1 = GlobalMaxPooling1D()(conv_1)\n",
    "    \n",
    "    attn_2 = AttentionWeightedAverage()(conv_2)\n",
    "    avg_2 = GlobalAveragePooling1D()(conv_2)\n",
    "    maxpool_2 = GlobalMaxPooling1D()(conv_2)\n",
    "    \n",
    "    attn_3 = AttentionWeightedAverage()(conv_3)\n",
    "    avg_3 = GlobalAveragePooling1D()(conv_3)\n",
    "    maxpool_3 = GlobalMaxPooling1D()(conv_3)\n",
    "    \n",
    "    merged_tensor_maxpool = merge([maxpool_0, maxpool_1, maxpool_2, maxpool_3], mode='concat', concat_axis=1)\n",
    "    merged_tensor_attn = merge([attn_0, attn_1, attn_2, attn_3], mode='concat', concat_axis=1)\n",
    "    merged_tensor_avg = merge([avg_0, avg_1, avg_2, avg_3], mode='concat', concat_axis=1)\n",
    "    merged_tensor = merge([merged_tensor_maxpool, merged_tensor_attn, merged_tensor_avg], mode='concat', concat_axis=1)\n",
    "    \n",
    "    output = Dropout(0.7)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=[input_layer, pos_input_layer], outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmax_text_cnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    filter_nums = 180\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(embedding_sequences)\n",
    "    \n",
    "    conv_0 = Conv1D(filter_nums, 1, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_1 = Conv1D(filter_nums, 2, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_2 = Conv1D(filter_nums, 3, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_3 = Conv1D(filter_nums, 4, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    \n",
    "    maxpool_0 = KMaxPooling(k=3)(conv_0)\n",
    "    maxpool_1 = KMaxPooling(k=3)(conv_1)\n",
    "    maxpool_2 = KMaxPooling(k=3)(conv_2)\n",
    "    maxpool_3 = KMaxPooling(k=3)(conv_3)\n",
    "    \n",
    "    merged_tensor = merge([maxpool_0, maxpool_1, maxpool_2, maxpool_3], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.6)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rcnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    recurrent_units = 64\n",
    "    filter_nums = 128\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.2)(embedding_sequences)\n",
    "    \n",
    "    rnn_layer = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(final_embedding_sequences)\n",
    "    conv_layer = Conv1D(filter_nums, 2, kernel_initializer='normal', padding='valid', activation='relu', strides=1)(rnn_layer)\n",
    "    \n",
    "    maxpool = GlobalMaxPooling1D()(conv_layer)\n",
    "    attn = AttentionWeightedAverage()(conv_layer)\n",
    "    avg = GlobalAveragePooling1D()(conv_layer)\n",
    "    \n",
    "    merged_tensor = merge([maxpool, attn, avg], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.5)(merged_tensor)\n",
    "    output = Dense(units=120, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_av_rnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    recurrent_units = 64\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.25)(embedding_sequences)\n",
    "    \n",
    "    rnn_layer_0 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(final_embedding_sequences)\n",
    "    rnn_layer_1 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(rnn_layer_0)\n",
    "    merged_rnn_layer = merge([rnn_layer_0, rnn_layer_1], mode='concat', concat_axis=2)\n",
    "    \n",
    "    last_layer = Lambda(lambda t: t[:, -1], name='last_layer')(merged_rnn_layer)\n",
    "    maxpool = GlobalMaxPooling1D()(merged_rnn_layer)\n",
    "    attn = AttentionWeightedAverage()(merged_rnn_layer)\n",
    "    avg = GlobalAveragePooling1D()(merged_rnn_layer)\n",
    "    \n",
    "    merged_tensor = merge([last_layer, maxpool, attn, avg], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.5)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_av_pos_rnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    pos_embedding_layer = Embedding(50,\n",
    "                                    35,\n",
    "                                    input_length=max_sequence_length,\n",
    "                                    trainable=True)\n",
    "    \n",
    "    recurrent_units = 64\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    pos_input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='POS')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    pos_sequences = pos_embedding_layer(pos_input_layer)\n",
    "    merged_embedding_layer = concatenate([embedding_sequences, pos_sequences], axis=2)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.2)(merged_embedding_layer)\n",
    "    \n",
    "    rnn_layer_0 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(final_embedding_sequences)\n",
    "    rnn_layer_0 = SpatialDropout1D(0.3)(rnn_layer_0)\n",
    "    rnn_layer_1 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(rnn_layer_0)\n",
    "    \n",
    "    last_layer = Lambda(lambda t: t[:, -1], name='last_layer')(rnn_layer_1)\n",
    "    maxpool = GlobalMaxPooling1D()(rnn_layer_1)\n",
    "    attn = AttentionWeightedAverage()(rnn_layer_1)\n",
    "    avg = GlobalAveragePooling1D()(rnn_layer_1)\n",
    "    \n",
    "    merged_tensor = merge([last_layer, maxpool, attn, avg], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.5)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=[input_layer, pos_input_layer], outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dropout_bigru(nb_words, embedding_dims, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Emebdding(nb_words,\n",
    "                                embedding_dims,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    recurrent_units = 64\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.2)(embedding_sequences)\n",
    "    \n",
    "    rnn_layer = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(final_embedding_sequences)\n",
    "    rnn_layer = Dropout(0.35)(rnn_layer)\n",
    "    rnn_layer = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(rnn_layer)\n",
    "    \n",
    "    last_layer = Lambda(lambda t: t[:, -1])(rnn_layer)\n",
    "    maxpool = GlobalMaxPooling1D()(rnn_layer)\n",
    "    avg = GlobalAveragePooling1D()(rnn_layer)\n",
    "    \n",
    "    merged_tensor = merge([last_layer, maxpool, avg], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.5)(merged_tensor)\n",
    "    output = Dense(units=72, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        super(DotAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn_vector = nn.Parameter(torch.Tensor(1, hidden_size), requires_grad=True)\n",
    "        \n",
    "        init.xavier_uniform(self.attn_vector.data)\n",
    "        \n",
    "    def forward(self, inputs, lengths=None):\n",
    "        \"\"\"\n",
    "        Return a scalar (All hiddens to only one weight for one output)\n",
    "        (batch_size, max_len, hidden_size) * (batch_size, hidden_size, 1) --> (batch_size, max_len, 1)\n",
    "        \"\"\"\n",
    "        batch_size, max_len = inputs.size()[:2]\n",
    "        \n",
    "        weights = torch.bmm(inputs,\n",
    "                            self.attn_vector              # (1, hidden)\n",
    "                            .unsqueeze(0)                 # (1, 1, hidden)\n",
    "                            .transpose(2, 1)              # (1, hidden, 1)\n",
    "                            .repeat(batch_size, 1, 1))    # (batch_size, hidden, 1)\n",
    "        \n",
    "        attn_energies = F.softmax(F.relu(weights.squeeze()))\n",
    "        \n",
    "        # create mask based on the sentence length\n",
    "        mask = Variable(torch.ones(attn_energies.size())).cuda()\n",
    "        for i, l in enumerate(lengths):\n",
    "            if l < max_len:\n",
    "                mask[:, l:] = 0\n",
    "                \n",
    "        # apply mask and renormalize attention scores (weights)\n",
    "        masked = attn_energies * mask\n",
    "        _sums = masked.sum(-1).expand_as(attn_energies)\n",
    "        attention_weights = masked.div(_sum)\n",
    "        \n",
    "        # apply attention weights\n",
    "        weighted = torch.mul(inputs, attention_weights.unsqueeze(-1).expand_as(inputs))\n",
    "        \n",
    "        # get the final fixed vector representations of the sentences\n",
    "        representations = weighted.sum(1).squeeze()\n",
    "        \n",
    "        return representations, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDropout():\n",
    "    \"\"\"\n",
    "    Implement of word embedding dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, p=0.5):\n",
    "        super(EmbeddingDropout, self).__init__()\n",
    "        if p < 0 or p > 1:\n",
    "            raise ValueError(\"dropout probability has to be between 0 and 1, but got {}\".format(p))\n",
    "            \n",
    "        self.p = p\n",
    "        self.trainable = True\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        if self.p > 0 and self.trainable:\n",
    "            dim = inputs.dim()\n",
    "            if dim == 1:\n",
    "                inputs = inputs.view(1, -1)\n",
    "            batch_size = inputs.size(0)\n",
    "            for i in range(batch_size):\n",
    "                x = np.unique(inputs[i].numpy())\n",
    "                x = np.nonzero(x)[0]\n",
    "                if len(x) == 0:\n",
    "                    return inputs\n",
    "                x = torch.from_numpy(x)\n",
    "                noise = x.new().resize_as_(x)\n",
    "                noise.bernoulli_(self.p)\n",
    "                x = x.mul(noise)\n",
    "                for value in x:\n",
    "                    if value > 0:\n",
    "                        mask = inputs[i].eq(value)\n",
    "                        inputs[i].masked_fill_(mask, 0)\n",
    "            if dim == 1:\n",
    "                inputs = inputs.view(-1)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDropout(nn.Module):\n",
    "    \n",
    "    def __init__(self, p=0.5):\n",
    "        super(SequentialDropout, self).__init__()\n",
    "        if p < 0 or p > 1:\n",
    "            raise ValueError(\"dropout probability has to be between 0 and 1, but got {}\".format(p))\n",
    "            \n",
    "        self.p = p\n",
    "        self.restart = True\n",
    "        self.trainable = True\n",
    "        \n",
    "    def _make_noise(self, inputs):\n",
    "        return Variable(inputs.data.new().resize_as_(inputs.data))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        if self.p > 0 and self.trainable:\n",
    "            if self.restart:\n",
    "                self.noise = self._make_noise(inputs)\n",
    "                self.noise.data.bernoulli_(1 - self.p).div_(1 - self.p)\n",
    "                if self.p == 1:\n",
    "                    self.noise.data.fill_(0)\n",
    "                self.noise = self.noise.expand_as(inputs)\n",
    "                self.restart = False\n",
    "            return inputs.mul(self.noise)\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    def end_of_sequence(self):\n",
    "        self.restart = True\n",
    "        \n",
    "    def backward(self, grad_output):\n",
    "        self.end_of_sequence()\n",
    "        if self.p > 0 and self.trainable:\n",
    "            return grad_output.mul(self.noise)\n",
    "        else:\n",
    "            return grad_output\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return type(self).__name__ + '({:.4f})'.format(self.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  0,  0],\n",
      "        [ 5,  3,  2,  2,  0]])\n",
      "tensor([[ 1,  2,  3,  0,  0],\n",
      "        [ 5,  3,  2,  2,  0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:3: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Test Embedding & Sequential Dropout\n",
    "seq_drop_model = SequentialDropout(p=0.5)\n",
    "input_data= Variable(torch.ones(1, 10), volatile=True)\n",
    "\n",
    "dist_total = torch.zeros(1)\n",
    "output_last = seq_drop_model(input_data)\n",
    "for i in range(50):\n",
    "    output_new = seq_drop_model(input_data)\n",
    "    dist_total += torch.dist(output_new, output_last).data\n",
    "    output_last = output_new\n",
    "    \n",
    "if not torch.equal(dist_total, torch.zeros(1)):\n",
    "    print('Error')\n",
    "    print(dist_total)\n",
    "    \n",
    "seq_drop_model.end_of_sequence()\n",
    "\n",
    "dist_total = torch.zeros(1)\n",
    "for i in range(50):\n",
    "    dist_total += torch.dist(output_last, seq_drop_model(input_data)).data\n",
    "    seq_drop_model.end_of_sequence()\n",
    "    \n",
    "if torch.equal(dist_total, torch.zeros(1)):\n",
    "    print('Error')\n",
    "    \n",
    "emb_drop_model = EmbeddingDropout(p=0.15)\n",
    "input_data = torch.Tensor([[1,2,3,0,0], [5,3,2,2,0]]).long()\n",
    "print(input_data)\n",
    "print(emb_drop_model.forward(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractGRUCell(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, bias_ih=True, bias_hh=False):\n",
    "        super(AbstractGRUCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias_ih = bias_ih\n",
    "        self.bias_hh = bias_hh\n",
    "        \n",
    "        self.weight_wr = nn.Linear(input_size, hidden_size, bias=bias_ih)\n",
    "        self.weight_wz = nn.Linear(input_size, hidden_size, bias=bias_ih)\n",
    "        self.weight_wh = nn.Linear(input_size, hidden_size, bias=bias_ih)\n",
    "        self.weight_ur = nn.Linear(hidden_size, hidden_size, bias=bias_hh)\n",
    "        self.weight_uz = nn.Linear(hidden_size, hidden_size, bias=bias_hh)\n",
    "        self.weight_uh = nn.Linear(hidden_size, hidden_size, bias=bias_hh)\n",
    "        \n",
    "    def forward(self, x, hx=None):\n",
    "        # Interface\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(AbstractGRUCell):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, bias_ih=True, bias_hh=False):\n",
    "        super(GRUCell, self).__init__(input_size, hidden_size, bias_ih, bias_hh)\n",
    "        \n",
    "    def forward(self, x, hx=None):\n",
    "        if hx is None:\n",
    "            hx = Variable(x.data.new().resize_((x.size(0), self.hidden_size)).fill_(0))\n",
    "        r = F.sigmoid(self.weight_wr(x) + self.weight_ur(hx))\n",
    "        z = F.sigmoid(self.weight_wz(x) + self.weight_uz(hx))\n",
    "        ht = F.tanh(self.weight_wh(x) + self.weight_uh(r * hx))\n",
    "        hx = (1 - i) * hx  + z * ht\n",
    "        return hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianGRUCell(AbstractGRUCell):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, bias_ih=True, bias_hh=False, dropout=0.25):\n",
    "        super(BayesianGRUCell, self).__init__(input_size, hidden_size, bias_ih, bias_hh)\n",
    "        self.dropout = dropout\n",
    "        self.set_dropout(self.dropout)\n",
    "        \n",
    "    def set_dropout(self, dropout):\n",
    "        self.drop_wr = SequentialDropout(p=dropout)\n",
    "        self.drop_wz = SequentialDropout(p=dropout)\n",
    "        self.drop_wh = SequentialDropout(p=dropout)\n",
    "        self.drop_ur = SequentialDropout(p=dropout)\n",
    "        self.drop_uz = SequentialDropout(p=dropout)\n",
    "        self.drop_uh = SequentialDropout(p=dropout)\n",
    "    \n",
    "    def end_of_sequence(self):\n",
    "        self.drop_wr.end_of_sequence()\n",
    "        self.drop_wz.end_of_sequence()\n",
    "        self.drop_wh.end_of_sequence()\n",
    "        self.drop_ur.end_of_sequence()\n",
    "        self.drop_uz.end_of_sequence()\n",
    "        self.drop_uh.end_of_sequence()\n",
    "        \n",
    "    def forward(self, x, hx=None):\n",
    "        if hx is None:\n",
    "            hx = Variable(x.data.new().resize_((x.size(0), self.hidden_size)).fill_(0))\n",
    "        x_wr = self.drop_wr(x)\n",
    "        x_wz = self.drop_wz(x)\n",
    "        x_wh = self.drop_wh(x)\n",
    "        x_ur = self.drop_ur(hx)\n",
    "        x_uz = self.drop_uz(hx)\n",
    "        x_uh = self.drop_uh(hx)\n",
    "        r = F.sigmoid(self.weight_wr(x_wr) + self.weight_ur(x_ur))\n",
    "        z = F.sigmoid(self.weight_wz(x_wz) + self.weight_uz(x_uz))\n",
    "        ht = F.tanh(self.weight_wh(x_wh) + self.weight_uh(r * x_uh))\n",
    "        hx = (1 - z) * hx + z * ht\n",
    "        return hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RHNCell(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, is_first_layer, recurrent_dropout):\n",
    "        super(RHNCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.is_first_layer = is_first_layer\n",
    "        \n",
    "        self.set_dropout(recurrent_dropout)\n",
    "        \n",
    "        if self.is_first_layer:\n",
    "            self.W_H = nn.Linear(input_size, hidden_size)\n",
    "            self.W_C = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        self.R_H = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.R_C = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        \n",
    "    def set_dropout(self, dropout):\n",
    "        self.dropout = dropout\n",
    "        self.drop_ir = SequentialDropout(p=self.dropout)\n",
    "        self.drop_ii = SequentialDropout(p=self.dropout)\n",
    "        self.drop_hr = SequentialDropout(p=self.dropout)\n",
    "        self.drop_hi = SequentialDropout(p=self.dropout)\n",
    "        \n",
    "    def end_of_sequence(self):\n",
    "        self.drop_ir.end_of_sequence()\n",
    "        self.drop_ii.end_of_sequence()\n",
    "        self.drop_hr.end_of_sequence()\n",
    "        self.drop_hi.end_of_sequence()\n",
    "        \n",
    "    def forward(self, _input, prev_hidden):\n",
    "        c_i = self.drop_hr(prev_hidden)\n",
    "        h_i = self.drop_hi(prev_hidden)\n",
    "        \n",
    "        if self.is_first_layer:\n",
    "            x_i = self.drop_ii(_input)\n",
    "            x_r = self.drop_ir(_input)\n",
    "            h1 = nn.Tanh()(self.W_H(x_i) + self.R_H(h_i))\n",
    "            t1 = nn.Sigmoid()(self.W_C(x_r) + self.R_C(c_i))\n",
    "        else:\n",
    "            h1 = nn.Tanh()(self.R_H(h_i))\n",
    "            t1 = nn.Sigmoid()(self.R_C(c_i))\n",
    "            \n",
    "        h = (h1 * t1) + (prev_hidden * (1 - t1))\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractGRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, bias_ih=True, bias_hh=False):\n",
    "        super(AbstractGRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias_ih = bias_ih\n",
    "        self.bias_hh = bias_hh\n",
    "        self.gru_cell = None\n",
    "        self._load_gru_cell()\n",
    "        \n",
    "    def _load_gru_cell(self):\n",
    "        # Interface\n",
    "        raise NotImplementError\n",
    "        \n",
    "    def forward(self, x, hx=None, max_length=None):\n",
    "        batch_size = x.size(0)\n",
    "        seq_length = x.size(1)\n",
    "        if max_length is None:\n",
    "            max_length = seq_length\n",
    "        output = []\n",
    "        for i in range(max_length):\n",
    "            # hidden output of every time-step\n",
    "            hx = self.gru_cell(x[:, i, :], hx=hx)\n",
    "            output.append(hx.view(batch_size, 1, self.hidden_size))\n",
    "        result = torch.cat(output, 1)\n",
    "        return result, hx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(AbstractGRU):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, bias_ih=True, bias_hh=False):\n",
    "        super(GRU, self).__init__(input_size, hidden_size, bias_ih, bias_hh)\n",
    "        \n",
    "    def _load_gru_cell(self):\n",
    "        self.gru_cell = GRUCell(self.input_size, self.hidden_size, self.bias_ih, self.hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiBayesianGRU(AbstractGRU):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, bias_ih=True, bias_hh=False, dropout=0.25):\n",
    "        self.dropout = dropout\n",
    "        super(BiBayesianGRU, self).__init__(input_size, hidden_size, bias_ih, bias_hh)\n",
    "        \n",
    "    def _load_gru_cell(self):\n",
    "        self.gru_cell = BayesianGRUCell(self.input_size, self.hidden_size, self.bias_ih, self.bias_hh, dropout=self.dropout)\n",
    "        \n",
    "    def set_dropout(self, dropout):\n",
    "        self.dropout = dropout\n",
    "        self.gru_cell.set_dropout(dropout)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        return Variable(torch.zeros(1, batch_size, self.hidden_size)).cuda()\n",
    "    \n",
    "    def forward(self, x, hx=None, max_length=None, lengths=None):\n",
    "        batch_size = x.size(0)\n",
    "        seq_length = x.size(1)\n",
    "        if max_length is None:\n",
    "            max_length = seq_length\n",
    "        lefts, rights = [], []\n",
    "        \n",
    "        # left part\n",
    "        lhx = self.init_hidden(batch_size)\n",
    "        for i in range(max_length):\n",
    "            new_hx = self.gru_cell(x[:, i, :], hx=hx)\n",
    "            mask = (i < lengths).float().unsqueeze(1).expand_as(new_hx)\n",
    "            lhx = new_hx * mask + lhx * (1 - mask)\n",
    "            lefts.append(lhx.view(batch_size, 1, self.hidden_size))\n",
    "        self.gru_cell.end_of_sequence()\n",
    "        lefts = torch.cat(lefts, 1)\n",
    "        \n",
    "        # right part\n",
    "        rhx = self.init_hidden(batch_size)\n",
    "        for i in range(max_length - 1, -1, -1):\n",
    "            new_hx = self.gru_cell(x[:, i, :], hx=hx)\n",
    "            mask = (i < lengths).float().unsqueeze(1).expand_as(new_hx)\n",
    "            rhx = new_hx * mask + rhx * (1 - mask)\n",
    "            rights.append(rhx.view(batch_size, 1, self.hidden_size))\n",
    "        self.gru_cell.end_of_sequence()\n",
    "        rights = torch.cat(rights, 1)\n",
    "        \n",
    "        output = torch.cat((lefts, rights), dim=2)\n",
    "        return output, lhx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelManager(object):\n",
    "    \n",
    "    def __init__(self, path=None):\n",
    "        self.path = path\n",
    "        \n",
    "    def save_model(self, model, path=None):\n",
    "        path = self.path if path is None else path\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print(\"Model has been saved as %s.\\n\" % path)\n",
    "        \n",
    "    def load_model(self, model, path=None):\n",
    "        path = self.path if path is None else path\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        model.eval()\n",
    "        print(\"A pre-trained model at %s has been loaded.\" % path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BaseModel, self).__init__()\n",
    "        print(\"Choose the torch base model.\")\n",
    "        self.manager = ModelManager()\n",
    "        \n",
    "    def save(self, path):\n",
    "        self.manager.save_model(self, path)\n",
    "        \n",
    "    def load(self, path):\n",
    "        self.manager.load_model(self, path)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def predict(self, x, batch_size=256, verbose=0):\n",
    "        self.eval_model()\n",
    "        predictions = []\n",
    "        for batch_xs in test_batches_generator(x, batch_size):\n",
    "            preds_var = self.forward(batch_xs)\n",
    "            preds_logits = nn.Sigmoid()(preds_var)\n",
    "            predictions.append(preds_logits.data.cpu().numpy())\n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        return predictions\n",
    "        \n",
    "    def set_dropout(self, p):\n",
    "        pass\n",
    "    \n",
    "    def train_model(self, p):\n",
    "        self.set_dropout(p)\n",
    "        self.train()\n",
    "        \n",
    "    def eval_model(self):\n",
    "        self.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUClassifier(BaseModel):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, embedding):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=self.input_size, hidden_size=self.hidden_size, batch_first=True, num_layers=2, dropout=0.35, bidirectional=True)\n",
    "        \n",
    "        self.attn = DotAttention(hidden_size=2 * hidden_size)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            OrderDict([\n",
    "                ('gru_dropout', nn.Dropout(0.5)),\n",
    "                ('h1', nn.Linear(self.hidden_size * 6, 108)),\n",
    "                ('relu1', nn.ReLU()),\n",
    "                ('out', nn.Linear(108, 6)),\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "    def set_dropout(self, p):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, _input, hidden=None, lengths=None):\n",
    "        _input, lengths = _input\n",
    "        embedded = self.embedding(_input)\n",
    "        \n",
    "        out, _ = self.gru(embedded)\n",
    "        last = out[:, -1, :]\n",
    "        attn, _ = self.attn.forward(out)\n",
    "        max_num, _ = torch.max(out, dim=1)\n",
    "        concatenated = torch.cat([last, max_num, attn], dim=1)\n",
    "        result = self.classifier(concatenated)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianGRUClassifier(BaseModel):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, embedding):\n",
    "        super(BayesianGRUClassifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        \n",
    "        self.gru1 = BiBayesianGRU(input_size=self.input_size, hidden_size=self.hidden_size, dropout=0.3)\n",
    "        self.gru2 = BiBayesianGRU(input_size=2 * self.hidden_size, hidden_size=hidden_size, dropout=0.3)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            OrderedDict([\n",
    "                ('gru_dropout', nn.Dropout(0.5)),\n",
    "                ('h1', nn.Linear(self.hidden_size * 4, 72)),\n",
    "                ('relu1', nn.ReLU()),\n",
    "                ('out', nn.Linear(72, 6)),\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "    def set_dropout(self, p):\n",
    "        self.gru1.set_dropout(p)\n",
    "        self.gru2.set_dropout(p)\n",
    "        \n",
    "    def forward(self, _input, hidden=None, lengths=None):\n",
    "        _input, lengths = _input\n",
    "        embedded = self.embedding(_input)\n",
    "        \n",
    "        out1, _ = self.gru1.forward(embedded, lengths=lengths)\n",
    "        out2, _ = self.gru2.forward(out1, lengths=lengths)\n",
    "        \n",
    "        last = out2[:, -1, :]\n",
    "        max_num, _ = torch.max(out2, dim=1)\n",
    "                \n",
    "        concatenated = torch.cat([last, max_num], dim=1)\n",
    "        result = self.classifier(concatenated)\n",
    "                \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentHighwayClassifier(BaseModel):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, recurrent_length, embedding, recurrent_dropout=0.3):\n",
    "        super(RecurrentHighwayClassifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.L = recurrent_length\n",
    "        self.recurrent_dropout = recurrent_dropout\n",
    "        self.highways = nn.ModuleList()\n",
    "        self.highways.append(RHNCell(self.input_size, self.hidden_size, is_first_layer=True, recurrent_dropout=self.recurrent_dropout))\n",
    "        \n",
    "        for _ in range(self.L - 1):\n",
    "            self.highways.append(RHNCell(self.input_size, self.hidden_size, is_first_layer=False, recurrent_dropout=self.recurrent_dropout))\n",
    "            \n",
    "        self.embedding = embedding\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            OrderedDict([\n",
    "                ('h1_dropout', nn.Dropout(0.5)),\n",
    "                ('h1', nn.Linear(self.hidden_size * 4, 74)),\n",
    "                ('relu1', nn.ReLU()),\n",
    "                ('out', nn.Linear(74, 6)),\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "    def init_state(self, batch_size):\n",
    "        hidden = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
    "        return hidden\n",
    "    \n",
    "    def set_dropout(self, p):\n",
    "        for rhn_cell in self.highways:\n",
    "            rhn_cell.set_dropout(p)\n",
    "            \n",
    "    def forward(self, _input, hidden=None, lengths=None):\n",
    "        '''\n",
    "        Input including input_sequences and sequence_length.\n",
    "        '''\n",
    "        _input, lengths = _input\n",
    "        batch_size = _input.size(0)\n",
    "        seq_length = _input.size(1)\n",
    "        \n",
    "        if hidden is None:\n",
    "            hidden = self.init_state(batch_size)\n",
    "        embed_batch = self.embedding(_input)\n",
    "        \n",
    "        lefts, rights = [], []\n",
    "        \n",
    "        for time in range(seq_length):\n",
    "            for tick in range(self.L):\n",
    "                next_hidden = self.highways[tick](embed_batch[:, time, :], hidden)\n",
    "                mask = (time < lengths).float().unsqueeze(1).expand_as(next_hidden)\n",
    "                hidden = next_hidden * mask + hidden * (1 - mask)\n",
    "            lefts.append(hidden.unsqueeze(1))\n",
    "        lefts = torch.cat(lefts, dim=1)\n",
    "        \n",
    "        for rhn_cell in self.highways:\n",
    "            rhn_cell.end_of_sequence()\n",
    "            \n",
    "        for time in range(seq_length - 1, -1, -1):\n",
    "            for tick in range(self.L):\n",
    "                next_hidden = self.highways[tick](embed_batch[:, time, :], hidden)\n",
    "                mask = (time < lengths).float().unsqueeze(1).expand_as(next_hidden)\n",
    "                hidden = next_hidden * mask + hidden * (1 - mask)\n",
    "            rights.append(hidden.unsqueeze(1))\n",
    "        rights = torch.cat(rights, dim=1)\n",
    "        \n",
    "        for rhn_cell in self.highways:\n",
    "            rhn_cell.end_of_sequence()\n",
    "            \n",
    "        outputs = torch.cat([lefts, rights], dim=2)\n",
    "        \n",
    "        last = outputs[:, -1, :]\n",
    "        max_num, _ = torch.max(outputs, dim=1)\n",
    "        \n",
    "        concatenated = torch.cat([last, max_num], dim=1)\n",
    "        result = self.classifier(concatenated)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'fasttext-avcnn-pos-' + str(nb_words) + 'vocabulary-' + str(MAX_SEQUENCE_LENGTH) + 'length'\n",
    "model = get_av_pos_cnn(nb_words, EMBEDDING_DIM, embedding_matrix, MAX_SEQUENCE_LENGTH, 6)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, val_loss, total_auc, fold_predictions = train_folds(train_data, pos_train_data, train_labels, FOLD_COUNT, BATCH_SIZE, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Overall val-loss: {}, AUC {}'.format(val_loss, total_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bgru_network():\n",
    "    embedding = nn.Embedding(MAX_NB_WORDS, EMBEDDING_DIM)\n",
    "    embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "    embedding.weight.requires_grad=False\n",
    "    return BayesianGRUClassifier(input_size=EMBEDDING_DIM, hidden_size=60, embedding=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recurrent_higtway_classifier():\n",
    "    embedding = nn.Embedding(MAX_NB_WORDS, EMBEDDING_DIM)\n",
    "    embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "    embedding.weight.requires_grad=False\n",
    "    return RecurrentHighwayClassifier(input_size=EMBEDDING_DIM, hidden_size=60, embedding=embedding, recurrent_length=2, recurrent_dropout=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose the torch base model.\n",
      "## Training on fold 0 ##\n",
      "Epoch: 1 Batch: 0 Log-loss: 0.6909138560295105\n",
      "Epoch: 1 Batch: 40 Log-loss: 0.09789622575044632\n",
      "Epoch: 1 Batch: 80 Log-loss: 0.1381751447916031\n",
      "Epoch: 1 Batch: 120 Log-loss: 0.08953512459993362\n",
      "Epoch: 1 Batch: 160 Log-loss: 0.0665510818362236\n",
      "Epoch: 1 Batch: 200 Log-loss: 0.08710252493619919\n",
      "Epoch: 1 Batch: 240 Log-loss: 0.05753612890839577\n",
      "Epoch: 1 Batch: 280 Log-loss: 0.07375114411115646\n",
      "Epoch: 1 Batch: 320 Log-loss: 0.07871861010789871\n",
      "Epoch: 1 Batch: 360 Log-loss: 0.07296248525381088\n",
      "Epoch: 1 Batch: 400 Log-loss: 0.06677073985338211\n",
      "Epoch: 1 Batch: 440 Log-loss: 0.04778880253434181\n",
      "Epoch: 1 Batch: 480 Log-loss: 0.034908317029476166\n",
      "Epoch: 1 Batch: 520 Log-loss: 0.05689697340130806\n",
      "Epoch: 1 Batch: 560 Log-loss: 0.06855592131614685\n",
      "Epoch average log-loss: 0.09195422819216868\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 1, val_loss: 0.053436464985758686, best_val_loss: 0.053436464985758686, best_auc: 0.9699356743256872\n",
      "Epoch: 2 Batch: 0 Log-loss: 0.0558086596429348\n",
      "Epoch: 2 Batch: 40 Log-loss: 0.06307733058929443\n",
      "Epoch: 2 Batch: 80 Log-loss: 0.05515977367758751\n",
      "Epoch: 2 Batch: 120 Log-loss: 0.06056787446141243\n",
      "Epoch: 2 Batch: 160 Log-loss: 0.055623918771743774\n",
      "Epoch: 2 Batch: 200 Log-loss: 0.054323915392160416\n",
      "Epoch: 2 Batch: 240 Log-loss: 0.03695756569504738\n",
      "Epoch: 2 Batch: 280 Log-loss: 0.06170410290360451\n",
      "Epoch: 2 Batch: 320 Log-loss: 0.04969862103462219\n",
      "Epoch: 2 Batch: 360 Log-loss: 0.05030548945069313\n",
      "Epoch: 2 Batch: 400 Log-loss: 0.058320943266153336\n",
      "Epoch: 2 Batch: 440 Log-loss: 0.06052224710583687\n",
      "Epoch: 2 Batch: 480 Log-loss: 0.03484116867184639\n",
      "Epoch: 2 Batch: 520 Log-loss: 0.04284931719303131\n",
      "Epoch: 2 Batch: 560 Log-loss: 0.05498535558581352\n",
      "Epoch average log-loss: 0.05487893834444029\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 2, val_loss: 0.04872165129200204, best_val_loss: 0.04872165129200204, best_auc: 0.9756470291124497\n",
      "Epoch: 3 Batch: 0 Log-loss: 0.06211179122328758\n",
      "Epoch: 3 Batch: 40 Log-loss: 0.04384206235408783\n",
      "Epoch: 3 Batch: 80 Log-loss: 0.03129642829298973\n",
      "Epoch: 3 Batch: 120 Log-loss: 0.06637551635503769\n",
      "Epoch: 3 Batch: 160 Log-loss: 0.043630246073007584\n",
      "Epoch: 3 Batch: 200 Log-loss: 0.05515213683247566\n",
      "Epoch: 3 Batch: 240 Log-loss: 0.05255049094557762\n",
      "Epoch: 3 Batch: 280 Log-loss: 0.06360111385583878\n",
      "Epoch: 3 Batch: 320 Log-loss: 0.055984895676374435\n",
      "Epoch: 3 Batch: 360 Log-loss: 0.04674144089221954\n",
      "Epoch: 3 Batch: 400 Log-loss: 0.051881104707717896\n",
      "Epoch: 3 Batch: 440 Log-loss: 0.04872872307896614\n",
      "Epoch: 3 Batch: 480 Log-loss: 0.07209400087594986\n",
      "Epoch: 3 Batch: 520 Log-loss: 0.0634450614452362\n",
      "Epoch: 3 Batch: 560 Log-loss: 0.04363568499684334\n",
      "Epoch average log-loss: 0.051725234680010806\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 3, val_loss: 0.046716116748067776, best_val_loss: 0.046716116748067776, best_auc: 0.9780477341241185\n",
      "Epoch: 4 Batch: 0 Log-loss: 0.033052362501621246\n",
      "Epoch: 4 Batch: 40 Log-loss: 0.05039798095822334\n",
      "Epoch: 4 Batch: 80 Log-loss: 0.03851189836859703\n",
      "Epoch: 4 Batch: 120 Log-loss: 0.044309671968221664\n",
      "Epoch: 4 Batch: 160 Log-loss: 0.04276280105113983\n",
      "Epoch: 4 Batch: 200 Log-loss: 0.050934404134750366\n",
      "Epoch: 4 Batch: 240 Log-loss: 0.04427148029208183\n",
      "Epoch: 4 Batch: 280 Log-loss: 0.050108399242162704\n",
      "Epoch: 4 Batch: 320 Log-loss: 0.05980503559112549\n",
      "Epoch: 4 Batch: 360 Log-loss: 0.03559982031583786\n",
      "Epoch: 4 Batch: 400 Log-loss: 0.04858504235744476\n",
      "Epoch: 4 Batch: 440 Log-loss: 0.04109179601073265\n",
      "Epoch: 4 Batch: 480 Log-loss: 0.05837513133883476\n",
      "Epoch: 4 Batch: 520 Log-loss: 0.07028035074472427\n",
      "Epoch: 4 Batch: 560 Log-loss: 0.02743592858314514\n",
      "Epoch average log-loss: 0.04983951938338578\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 4, val_loss: 0.04590619433772513, best_val_loss: 0.04590619433772513, best_auc: 0.9798796854962645\n",
      "Epoch: 5 Batch: 0 Log-loss: 0.05427160486578941\n",
      "Epoch: 5 Batch: 40 Log-loss: 0.05108611658215523\n",
      "Epoch: 5 Batch: 80 Log-loss: 0.06420300155878067\n",
      "Epoch: 5 Batch: 120 Log-loss: 0.035206787288188934\n",
      "Epoch: 5 Batch: 160 Log-loss: 0.042571425437927246\n",
      "Epoch: 5 Batch: 200 Log-loss: 0.042223405092954636\n",
      "Epoch: 5 Batch: 240 Log-loss: 0.04652855917811394\n",
      "Epoch: 5 Batch: 280 Log-loss: 0.04497930034995079\n",
      "Epoch: 5 Batch: 320 Log-loss: 0.027512671425938606\n",
      "Epoch: 5 Batch: 360 Log-loss: 0.04955877736210823\n",
      "Epoch: 5 Batch: 400 Log-loss: 0.04692070186138153\n",
      "Epoch: 5 Batch: 440 Log-loss: 0.05276845768094063\n",
      "Epoch: 5 Batch: 480 Log-loss: 0.04259278252720833\n",
      "Epoch: 5 Batch: 520 Log-loss: 0.06182912364602089\n",
      "Epoch: 5 Batch: 560 Log-loss: 0.0479995533823967\n",
      "Epoch average log-loss: 0.048384557611175945\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 5, val_loss: 0.04427789630868819, best_val_loss: 0.04427789630868819, best_auc: 0.9818224120212772\n",
      "Epoch: 6 Batch: 0 Log-loss: 0.038214024156332016\n",
      "Epoch: 6 Batch: 40 Log-loss: 0.05325740948319435\n",
      "Epoch: 6 Batch: 80 Log-loss: 0.047100141644477844\n",
      "Epoch: 6 Batch: 120 Log-loss: 0.05393074080348015\n",
      "Epoch: 6 Batch: 160 Log-loss: 0.03769759088754654\n",
      "Epoch: 6 Batch: 200 Log-loss: 0.06343216449022293\n",
      "Epoch: 6 Batch: 240 Log-loss: 0.03812532499432564\n",
      "Epoch: 6 Batch: 280 Log-loss: 0.027552291750907898\n",
      "Epoch: 6 Batch: 320 Log-loss: 0.04887448251247406\n",
      "Epoch: 6 Batch: 360 Log-loss: 0.03789491951465607\n",
      "Epoch: 6 Batch: 400 Log-loss: 0.03709361329674721\n",
      "Epoch: 6 Batch: 440 Log-loss: 0.037376537919044495\n",
      "Epoch: 6 Batch: 480 Log-loss: 0.044650305062532425\n",
      "Epoch: 6 Batch: 520 Log-loss: 0.040720563381910324\n",
      "Epoch: 6 Batch: 560 Log-loss: 0.04470569267868996\n",
      "Epoch average log-loss: 0.046971422789751416\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 6, val_loss: 0.043548629832587525, best_val_loss: 0.043548629832587525, best_auc: 0.9843158786906537\n",
      "Epoch: 7 Batch: 0 Log-loss: 0.04789289832115173\n",
      "Epoch: 7 Batch: 40 Log-loss: 0.05402764305472374\n",
      "Epoch: 7 Batch: 80 Log-loss: 0.0499873124063015\n",
      "Epoch: 7 Batch: 120 Log-loss: 0.03715670853853226\n",
      "Epoch: 7 Batch: 160 Log-loss: 0.0533299446105957\n",
      "Epoch: 7 Batch: 200 Log-loss: 0.06741533428430557\n",
      "Epoch: 7 Batch: 240 Log-loss: 0.05172919109463692\n",
      "Epoch: 7 Batch: 280 Log-loss: 0.03948754072189331\n",
      "Epoch: 7 Batch: 320 Log-loss: 0.04883529245853424\n",
      "Epoch: 7 Batch: 360 Log-loss: 0.053645968437194824\n",
      "Epoch: 7 Batch: 400 Log-loss: 0.039766576141119\n",
      "Epoch: 7 Batch: 440 Log-loss: 0.05241669341921806\n",
      "Epoch: 7 Batch: 480 Log-loss: 0.055830907076597214\n",
      "Epoch: 7 Batch: 520 Log-loss: 0.07501109689474106\n",
      "Epoch: 7 Batch: 560 Log-loss: 0.04541369527578354\n",
      "Epoch average log-loss: 0.0460596647712269\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 7, val_loss: 0.04268428107733929, best_val_loss: 0.04268428107733929, best_auc: 0.9852163471057805\n",
      "Epoch: 8 Batch: 0 Log-loss: 0.05382123962044716\n",
      "Epoch: 8 Batch: 40 Log-loss: 0.04006661847233772\n",
      "Epoch: 8 Batch: 80 Log-loss: 0.045420560985803604\n",
      "Epoch: 8 Batch: 120 Log-loss: 0.07984919846057892\n",
      "Epoch: 8 Batch: 160 Log-loss: 0.02921287529170513\n",
      "Epoch: 8 Batch: 200 Log-loss: 0.044377997517585754\n",
      "Epoch: 8 Batch: 240 Log-loss: 0.05730601027607918\n",
      "Epoch: 8 Batch: 280 Log-loss: 0.052137479186058044\n",
      "Epoch: 8 Batch: 320 Log-loss: 0.03794084116816521\n",
      "Epoch: 8 Batch: 360 Log-loss: 0.034603748470544815\n",
      "Epoch: 8 Batch: 400 Log-loss: 0.04616674408316612\n",
      "Epoch: 8 Batch: 440 Log-loss: 0.040963586419820786\n",
      "Epoch: 8 Batch: 480 Log-loss: 0.038270290940999985\n",
      "Epoch: 8 Batch: 520 Log-loss: 0.04518168792128563\n",
      "Epoch: 8 Batch: 560 Log-loss: 0.022462444379925728\n",
      "Epoch average log-loss: 0.04532254814569439\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 8, val_loss: 0.04219335998167446, best_val_loss: 0.04219335998167446, best_auc: 0.9851661256526131\n",
      "Epoch: 9 Batch: 0 Log-loss: 0.061531174927949905\n",
      "Epoch: 9 Batch: 40 Log-loss: 0.040130503475666046\n",
      "Epoch: 9 Batch: 80 Log-loss: 0.04710060730576515\n",
      "Epoch: 9 Batch: 120 Log-loss: 0.02562928944826126\n",
      "Epoch: 9 Batch: 160 Log-loss: 0.03084486909210682\n",
      "Epoch: 9 Batch: 200 Log-loss: 0.030595742166042328\n",
      "Epoch: 9 Batch: 240 Log-loss: 0.044311925768852234\n",
      "Epoch: 9 Batch: 280 Log-loss: 0.040633875876665115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Batch: 320 Log-loss: 0.029772600159049034\n",
      "Epoch: 9 Batch: 360 Log-loss: 0.03644464537501335\n",
      "Epoch: 9 Batch: 400 Log-loss: 0.04935148358345032\n",
      "Epoch: 9 Batch: 440 Log-loss: 0.04174357280135155\n",
      "Epoch: 9 Batch: 480 Log-loss: 0.03916306048631668\n",
      "Epoch: 9 Batch: 520 Log-loss: 0.03705568239092827\n",
      "Epoch: 9 Batch: 560 Log-loss: 0.051291849464178085\n",
      "Epoch average log-loss: 0.0445412934624723\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 9, val_loss: 0.041479047999132455, best_val_loss: 0.041479047999132455, best_auc: 0.9860580803359587\n",
      "Epoch: 10 Batch: 0 Log-loss: 0.05114366486668587\n",
      "Epoch: 10 Batch: 40 Log-loss: 0.04128624126315117\n",
      "Epoch: 10 Batch: 80 Log-loss: 0.043263595551252365\n",
      "Epoch: 10 Batch: 120 Log-loss: 0.03587421774864197\n",
      "Epoch: 10 Batch: 160 Log-loss: 0.04254782199859619\n",
      "Epoch: 10 Batch: 200 Log-loss: 0.047971997410058975\n",
      "Epoch: 10 Batch: 240 Log-loss: 0.052927348762750626\n",
      "Epoch: 10 Batch: 280 Log-loss: 0.036784302443265915\n",
      "Epoch: 10 Batch: 320 Log-loss: 0.06007503345608711\n",
      "Epoch: 10 Batch: 360 Log-loss: 0.033233653753995895\n",
      "Epoch: 10 Batch: 400 Log-loss: 0.03791012987494469\n",
      "Epoch: 10 Batch: 440 Log-loss: 0.04194960370659828\n",
      "Epoch: 10 Batch: 480 Log-loss: 0.03429202735424042\n",
      "Epoch: 10 Batch: 520 Log-loss: 0.04335729777812958\n",
      "Epoch: 10 Batch: 560 Log-loss: 0.03512809798121452\n",
      "Epoch average log-loss: 0.044020159000397796\n",
      "In Epoch: 10, val_loss: 0.04179287792887546, best_val_loss: 0.041479047999132455, best_auc: 0.9860580803359587\n",
      "Epoch: 11 Batch: 0 Log-loss: 0.04691596329212189\n",
      "Epoch: 11 Batch: 40 Log-loss: 0.04206620529294014\n",
      "Epoch: 11 Batch: 80 Log-loss: 0.03759196028113365\n",
      "Epoch: 11 Batch: 120 Log-loss: 0.03649871423840523\n",
      "Epoch: 11 Batch: 160 Log-loss: 0.04836968705058098\n",
      "Epoch: 11 Batch: 200 Log-loss: 0.03245435282588005\n",
      "Epoch: 11 Batch: 240 Log-loss: 0.043700989335775375\n",
      "Epoch: 11 Batch: 280 Log-loss: 0.041032809764146805\n",
      "Epoch: 11 Batch: 320 Log-loss: 0.03223007544875145\n",
      "Epoch: 11 Batch: 360 Log-loss: 0.057690124958753586\n",
      "Epoch: 11 Batch: 400 Log-loss: 0.04501080513000488\n",
      "Epoch: 11 Batch: 440 Log-loss: 0.039492104202508926\n",
      "Epoch: 11 Batch: 480 Log-loss: 0.043430667370557785\n",
      "Epoch: 11 Batch: 520 Log-loss: 0.04903965815901756\n",
      "Epoch: 11 Batch: 560 Log-loss: 0.034234512597322464\n",
      "Epoch average log-loss: 0.04362692908783044\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 11, val_loss: 0.041046796385299296, best_val_loss: 0.041046796385299296, best_auc: 0.9873194616994333\n",
      "Epoch: 12 Batch: 0 Log-loss: 0.03553109988570213\n",
      "Epoch: 12 Batch: 40 Log-loss: 0.04525801166892052\n",
      "Epoch: 12 Batch: 80 Log-loss: 0.04069800302386284\n",
      "Epoch: 12 Batch: 120 Log-loss: 0.049075204879045486\n",
      "Epoch: 12 Batch: 160 Log-loss: 0.0686555728316307\n",
      "Epoch: 12 Batch: 200 Log-loss: 0.043465375900268555\n",
      "Epoch: 12 Batch: 240 Log-loss: 0.03072010539472103\n",
      "Epoch: 12 Batch: 280 Log-loss: 0.03605549782514572\n",
      "Epoch: 12 Batch: 320 Log-loss: 0.04007633030414581\n",
      "Epoch: 12 Batch: 360 Log-loss: 0.025226689875125885\n",
      "Epoch: 12 Batch: 400 Log-loss: 0.05240738391876221\n",
      "Epoch: 12 Batch: 440 Log-loss: 0.029453875496983528\n",
      "Epoch: 12 Batch: 480 Log-loss: 0.04552731290459633\n",
      "Epoch: 12 Batch: 520 Log-loss: 0.04243597015738487\n",
      "Epoch: 12 Batch: 560 Log-loss: 0.04676087573170662\n",
      "Epoch average log-loss: 0.04309067346621305\n",
      "In Epoch: 12, val_loss: 0.041053336899249775, best_val_loss: 0.041046796385299296, best_auc: 0.9873194616994333\n",
      "Epoch: 13 Batch: 0 Log-loss: 0.05179345980286598\n",
      "Epoch: 13 Batch: 40 Log-loss: 0.03680683672428131\n",
      "Epoch: 13 Batch: 80 Log-loss: 0.051794830709695816\n",
      "Epoch: 13 Batch: 120 Log-loss: 0.0471663661301136\n",
      "Epoch: 13 Batch: 160 Log-loss: 0.04497576877474785\n",
      "Epoch: 13 Batch: 200 Log-loss: 0.03273520618677139\n",
      "Epoch: 13 Batch: 240 Log-loss: 0.059338510036468506\n",
      "Epoch: 13 Batch: 280 Log-loss: 0.06361741572618484\n",
      "Epoch: 13 Batch: 320 Log-loss: 0.041220977902412415\n",
      "Epoch: 13 Batch: 360 Log-loss: 0.03503454476594925\n",
      "Epoch: 13 Batch: 400 Log-loss: 0.03148624673485756\n",
      "Epoch: 13 Batch: 440 Log-loss: 0.04640625789761543\n",
      "Epoch: 13 Batch: 480 Log-loss: 0.03296797350049019\n",
      "Epoch: 13 Batch: 520 Log-loss: 0.03803890570998192\n",
      "Epoch: 13 Batch: 560 Log-loss: 0.045188698917627335\n",
      "Epoch average log-loss: 0.04283446150366217\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 13, val_loss: 0.04093161847162844, best_val_loss: 0.04093161847162844, best_auc: 0.9872046877578243\n",
      "Epoch: 14 Batch: 0 Log-loss: 0.025875838473439217\n",
      "Epoch: 14 Batch: 40 Log-loss: 0.04369661211967468\n",
      "Epoch: 14 Batch: 80 Log-loss: 0.05731378123164177\n",
      "Epoch: 14 Batch: 120 Log-loss: 0.0444810576736927\n",
      "Epoch: 14 Batch: 160 Log-loss: 0.05153154954314232\n",
      "Epoch: 14 Batch: 200 Log-loss: 0.036728423088788986\n",
      "Epoch: 14 Batch: 240 Log-loss: 0.037200864404439926\n",
      "Epoch: 14 Batch: 280 Log-loss: 0.0488947369158268\n",
      "Epoch: 14 Batch: 320 Log-loss: 0.02720363438129425\n",
      "Epoch: 14 Batch: 360 Log-loss: 0.045499324798583984\n",
      "Epoch: 14 Batch: 400 Log-loss: 0.046170663088560104\n",
      "Epoch: 14 Batch: 440 Log-loss: 0.0446271188557148\n",
      "Epoch: 14 Batch: 480 Log-loss: 0.03643740341067314\n",
      "Epoch: 14 Batch: 520 Log-loss: 0.031989067792892456\n",
      "Epoch: 14 Batch: 560 Log-loss: 0.04324289783835411\n",
      "Epoch average log-loss: 0.042438259833891476\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 14, val_loss: 0.04019001452869767, best_val_loss: 0.04019001452869767, best_auc: 0.9883250988857104\n",
      "Epoch: 15 Batch: 0 Log-loss: 0.038067370653152466\n",
      "Epoch: 15 Batch: 40 Log-loss: 0.032917775213718414\n",
      "Epoch: 15 Batch: 80 Log-loss: 0.05885523185133934\n",
      "Epoch: 15 Batch: 120 Log-loss: 0.051980238407850266\n",
      "Epoch: 15 Batch: 160 Log-loss: 0.04868841543793678\n",
      "Epoch: 15 Batch: 200 Log-loss: 0.02997531183063984\n",
      "Epoch: 15 Batch: 240 Log-loss: 0.04303516820073128\n",
      "Epoch: 15 Batch: 280 Log-loss: 0.030433414503932\n",
      "Epoch: 15 Batch: 320 Log-loss: 0.037607207894325256\n",
      "Epoch: 15 Batch: 360 Log-loss: 0.057063039392232895\n",
      "Epoch: 15 Batch: 400 Log-loss: 0.04427482187747955\n",
      "Epoch: 15 Batch: 440 Log-loss: 0.03906945511698723\n",
      "Epoch: 15 Batch: 480 Log-loss: 0.03642650693655014\n",
      "Epoch: 15 Batch: 520 Log-loss: 0.05275069549679756\n",
      "Epoch: 15 Batch: 560 Log-loss: 0.05091174691915512\n",
      "Epoch average log-loss: 0.04224139284342528\n",
      "In Epoch: 15, val_loss: 0.04049362359032704, best_val_loss: 0.04019001452869767, best_auc: 0.9883250988857104\n",
      "Epoch: 16 Batch: 0 Log-loss: 0.04840729758143425\n",
      "Epoch: 16 Batch: 40 Log-loss: 0.04277202859520912\n",
      "Epoch: 16 Batch: 80 Log-loss: 0.036479342728853226\n",
      "Epoch: 16 Batch: 120 Log-loss: 0.0404704324901104\n",
      "Epoch: 16 Batch: 160 Log-loss: 0.034854087978601456\n",
      "Epoch: 16 Batch: 200 Log-loss: 0.032195430248975754\n",
      "Epoch: 16 Batch: 240 Log-loss: 0.04897977039217949\n",
      "Epoch: 16 Batch: 280 Log-loss: 0.04590579494833946\n",
      "Epoch: 16 Batch: 320 Log-loss: 0.03882132098078728\n",
      "Epoch: 16 Batch: 360 Log-loss: 0.05149020627140999\n",
      "Epoch: 16 Batch: 400 Log-loss: 0.044545188546180725\n",
      "Epoch: 16 Batch: 440 Log-loss: 0.02632935158908367\n",
      "Epoch: 16 Batch: 480 Log-loss: 0.039017800241708755\n",
      "Epoch: 16 Batch: 520 Log-loss: 0.04398765042424202\n",
      "Epoch: 16 Batch: 560 Log-loss: 0.041807543486356735\n",
      "Epoch average log-loss: 0.042026859056204555\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 16, val_loss: 0.040156374273107746, best_val_loss: 0.040156374273107746, best_auc: 0.9885184712916283\n",
      "Epoch: 17 Batch: 0 Log-loss: 0.03564339503645897\n",
      "Epoch: 17 Batch: 40 Log-loss: 0.043289992958307266\n",
      "Epoch: 17 Batch: 80 Log-loss: 0.03668041527271271\n",
      "Epoch: 17 Batch: 120 Log-loss: 0.05232053995132446\n",
      "Epoch: 17 Batch: 160 Log-loss: 0.036387890577316284\n",
      "Epoch: 17 Batch: 200 Log-loss: 0.04115874692797661\n",
      "Epoch: 17 Batch: 240 Log-loss: 0.0321173369884491\n",
      "Epoch: 17 Batch: 280 Log-loss: 0.03793308138847351\n",
      "Epoch: 17 Batch: 320 Log-loss: 0.027776522561907768\n",
      "Epoch: 17 Batch: 360 Log-loss: 0.05132977291941643\n",
      "Epoch: 17 Batch: 400 Log-loss: 0.0362553596496582\n",
      "Epoch: 17 Batch: 440 Log-loss: 0.043678123503923416\n",
      "Epoch: 17 Batch: 480 Log-loss: 0.03392854705452919\n",
      "Epoch: 17 Batch: 520 Log-loss: 0.047203246504068375\n",
      "Epoch: 17 Batch: 560 Log-loss: 0.052596449851989746\n",
      "Epoch average log-loss: 0.04168298448702054\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 17, val_loss: 0.03986488318055153, best_val_loss: 0.03986488318055153, best_auc: 0.9889411308654562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 Batch: 0 Log-loss: 0.032104719430208206\n",
      "Epoch: 18 Batch: 40 Log-loss: 0.04511968418955803\n",
      "Epoch: 18 Batch: 80 Log-loss: 0.03888026997447014\n",
      "Epoch: 18 Batch: 120 Log-loss: 0.04201468452811241\n",
      "Epoch: 18 Batch: 160 Log-loss: 0.030432039871811867\n",
      "Epoch: 18 Batch: 200 Log-loss: 0.029706992208957672\n",
      "Epoch: 18 Batch: 240 Log-loss: 0.05270456150174141\n",
      "Epoch: 18 Batch: 280 Log-loss: 0.04667198285460472\n",
      "Epoch: 18 Batch: 320 Log-loss: 0.03768365830183029\n",
      "Epoch: 18 Batch: 360 Log-loss: 0.038321856409311295\n",
      "Epoch: 18 Batch: 400 Log-loss: 0.04278748854994774\n",
      "Epoch: 18 Batch: 440 Log-loss: 0.035271767526865005\n",
      "Epoch: 18 Batch: 480 Log-loss: 0.05709590017795563\n",
      "Epoch: 18 Batch: 520 Log-loss: 0.047594550997018814\n",
      "Epoch: 18 Batch: 560 Log-loss: 0.046552181243896484\n",
      "Epoch average log-loss: 0.04169747363443353\n",
      "In Epoch: 18, val_loss: 0.04024400187725147, best_val_loss: 0.03986488318055153, best_auc: 0.9889411308654562\n",
      "Epoch: 19 Batch: 0 Log-loss: 0.03365618735551834\n",
      "Epoch: 19 Batch: 40 Log-loss: 0.030477074906229973\n",
      "Epoch: 19 Batch: 80 Log-loss: 0.029727822169661522\n",
      "Epoch: 19 Batch: 120 Log-loss: 0.05059996247291565\n",
      "Epoch: 19 Batch: 160 Log-loss: 0.03787052258849144\n",
      "Epoch: 19 Batch: 200 Log-loss: 0.03323109820485115\n",
      "Epoch: 19 Batch: 240 Log-loss: 0.04463348910212517\n",
      "Epoch: 19 Batch: 280 Log-loss: 0.039089642465114594\n",
      "Epoch: 19 Batch: 320 Log-loss: 0.04450227692723274\n",
      "Epoch: 19 Batch: 360 Log-loss: 0.05825622007250786\n",
      "Epoch: 19 Batch: 400 Log-loss: 0.034584276378154755\n",
      "Epoch: 19 Batch: 440 Log-loss: 0.04995546117424965\n",
      "Epoch: 19 Batch: 480 Log-loss: 0.030181078240275383\n",
      "Epoch: 19 Batch: 520 Log-loss: 0.04500121250748634\n",
      "Epoch: 19 Batch: 560 Log-loss: 0.03723369538784027\n",
      "Epoch average log-loss: 0.041455809023630406\n",
      "In Epoch: 19, val_loss: 0.040047018263726065, best_val_loss: 0.03986488318055153, best_auc: 0.9889411308654562\n",
      "Epoch: 20 Batch: 0 Log-loss: 0.040249668061733246\n",
      "Epoch: 20 Batch: 40 Log-loss: 0.043597180396318436\n",
      "Epoch: 20 Batch: 80 Log-loss: 0.050164107233285904\n",
      "Epoch: 20 Batch: 120 Log-loss: 0.018679775297641754\n",
      "Epoch: 20 Batch: 160 Log-loss: 0.03599840775132179\n",
      "Epoch: 20 Batch: 200 Log-loss: 0.059584423899650574\n",
      "Epoch: 20 Batch: 240 Log-loss: 0.04621243104338646\n",
      "Epoch: 20 Batch: 280 Log-loss: 0.03511277958750725\n",
      "Epoch: 20 Batch: 320 Log-loss: 0.032484859228134155\n",
      "Epoch: 20 Batch: 360 Log-loss: 0.04083403944969177\n",
      "Epoch: 20 Batch: 400 Log-loss: 0.04112396016716957\n",
      "Epoch: 20 Batch: 440 Log-loss: 0.03328670561313629\n",
      "Epoch: 20 Batch: 480 Log-loss: 0.028401779010891914\n",
      "Epoch: 20 Batch: 520 Log-loss: 0.07596981525421143\n",
      "Epoch: 20 Batch: 560 Log-loss: 0.02918872982263565\n",
      "Epoch average log-loss: 0.04133206896616944\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 20, val_loss: 0.039459108235534196, best_val_loss: 0.039459108235534196, best_auc: 0.9889863701864182\n",
      "Epoch: 21 Batch: 0 Log-loss: 0.04530969262123108\n",
      "Epoch: 21 Batch: 40 Log-loss: 0.061168670654296875\n",
      "Epoch: 21 Batch: 80 Log-loss: 0.038287583738565445\n",
      "Epoch: 21 Batch: 120 Log-loss: 0.04487967491149902\n",
      "Epoch: 21 Batch: 160 Log-loss: 0.04182133451104164\n",
      "Epoch: 21 Batch: 200 Log-loss: 0.04509823024272919\n",
      "Epoch: 21 Batch: 240 Log-loss: 0.041077032685279846\n",
      "Epoch: 21 Batch: 280 Log-loss: 0.031931933015584946\n",
      "Epoch: 21 Batch: 320 Log-loss: 0.03302915394306183\n",
      "Epoch: 21 Batch: 360 Log-loss: 0.04003743827342987\n",
      "Epoch: 21 Batch: 400 Log-loss: 0.046105559915304184\n",
      "Epoch: 21 Batch: 440 Log-loss: 0.038804858922958374\n",
      "Epoch: 21 Batch: 480 Log-loss: 0.04215546324849129\n",
      "Epoch: 21 Batch: 520 Log-loss: 0.042443737387657166\n",
      "Epoch: 21 Batch: 560 Log-loss: 0.03605574369430542\n",
      "Epoch average log-loss: 0.0409265934589452\n",
      "In Epoch: 21, val_loss: 0.04001154893621275, best_val_loss: 0.039459108235534196, best_auc: 0.9889863701864182\n",
      "Epoch: 22 Batch: 0 Log-loss: 0.032801080495119095\n",
      "Epoch: 22 Batch: 40 Log-loss: 0.027022967115044594\n",
      "Epoch: 22 Batch: 80 Log-loss: 0.04064707085490227\n",
      "Epoch: 22 Batch: 120 Log-loss: 0.05087578669190407\n",
      "Epoch: 22 Batch: 160 Log-loss: 0.04544423520565033\n",
      "Epoch: 22 Batch: 200 Log-loss: 0.0312877781689167\n",
      "Epoch: 22 Batch: 240 Log-loss: 0.032711636275053024\n",
      "Epoch: 22 Batch: 280 Log-loss: 0.055195778608322144\n",
      "Epoch: 22 Batch: 320 Log-loss: 0.04196542128920555\n",
      "Epoch: 22 Batch: 360 Log-loss: 0.049067508429288864\n",
      "Epoch: 22 Batch: 400 Log-loss: 0.04690450802445412\n",
      "Epoch: 22 Batch: 440 Log-loss: 0.04263977333903313\n",
      "Epoch: 22 Batch: 480 Log-loss: 0.021674400195479393\n",
      "Epoch: 22 Batch: 520 Log-loss: 0.049281682819128036\n",
      "Epoch: 22 Batch: 560 Log-loss: 0.04641415923833847\n",
      "Epoch average log-loss: 0.04102970015457166\n",
      "In Epoch: 22, val_loss: 0.03962804945335283, best_val_loss: 0.039459108235534196, best_auc: 0.9889863701864182\n",
      "Epoch: 23 Batch: 0 Log-loss: 0.049392327666282654\n",
      "Epoch: 23 Batch: 40 Log-loss: 0.03603655844926834\n",
      "Epoch: 23 Batch: 80 Log-loss: 0.04550426825881004\n",
      "Epoch: 23 Batch: 120 Log-loss: 0.048966649919748306\n",
      "Epoch: 23 Batch: 160 Log-loss: 0.03594013676047325\n",
      "Epoch: 23 Batch: 200 Log-loss: 0.0584443099796772\n",
      "Epoch: 23 Batch: 240 Log-loss: 0.05391564592719078\n",
      "Epoch: 23 Batch: 280 Log-loss: 0.035508621484041214\n",
      "Epoch: 23 Batch: 320 Log-loss: 0.030509425327181816\n",
      "Epoch: 23 Batch: 360 Log-loss: 0.04097573086619377\n",
      "Epoch: 23 Batch: 400 Log-loss: 0.030405761674046516\n",
      "Epoch: 23 Batch: 440 Log-loss: 0.047846075147390366\n",
      "Epoch: 23 Batch: 480 Log-loss: 0.03958550840616226\n",
      "Epoch: 23 Batch: 520 Log-loss: 0.041084256023168564\n",
      "Epoch: 23 Batch: 560 Log-loss: 0.04565122723579407\n",
      "Epoch average log-loss: 0.04054034447763115\n",
      "In Epoch: 23, val_loss: 0.039576146260329195, best_val_loss: 0.039459108235534196, best_auc: 0.9889863701864182\n",
      "Epoch: 24 Batch: 0 Log-loss: 0.050160035490989685\n",
      "Epoch: 24 Batch: 40 Log-loss: 0.04810705780982971\n",
      "Epoch: 24 Batch: 80 Log-loss: 0.037038300186395645\n",
      "Epoch: 24 Batch: 120 Log-loss: 0.03620542958378792\n",
      "Epoch: 24 Batch: 160 Log-loss: 0.03120129369199276\n",
      "Epoch: 24 Batch: 200 Log-loss: 0.03982895240187645\n",
      "Epoch: 24 Batch: 240 Log-loss: 0.056748781353235245\n",
      "Epoch: 24 Batch: 280 Log-loss: 0.03426374867558479\n",
      "Epoch: 24 Batch: 320 Log-loss: 0.024156907573342323\n",
      "Epoch: 24 Batch: 360 Log-loss: 0.06133788824081421\n",
      "Epoch: 24 Batch: 400 Log-loss: 0.03536742553114891\n",
      "Epoch: 24 Batch: 440 Log-loss: 0.04382588341832161\n",
      "Epoch: 24 Batch: 480 Log-loss: 0.02550102025270462\n",
      "Epoch: 24 Batch: 520 Log-loss: 0.040463197976350784\n",
      "Epoch: 24 Batch: 560 Log-loss: 0.040335774421691895\n",
      "Epoch average log-loss: 0.04070242555545909\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 24, val_loss: 0.03943215243460011, best_val_loss: 0.03943215243460011, best_auc: 0.989097115935709\n",
      "Epoch: 25 Batch: 0 Log-loss: 0.04282447695732117\n",
      "Epoch: 25 Batch: 40 Log-loss: 0.04612058773636818\n",
      "Epoch: 25 Batch: 80 Log-loss: 0.052331387996673584\n",
      "Epoch: 25 Batch: 120 Log-loss: 0.03919206187129021\n",
      "Epoch: 25 Batch: 160 Log-loss: 0.04370225965976715\n",
      "Epoch: 25 Batch: 200 Log-loss: 0.03556639701128006\n",
      "Epoch: 25 Batch: 240 Log-loss: 0.04572778567671776\n",
      "Epoch: 25 Batch: 280 Log-loss: 0.03116474486887455\n",
      "Epoch: 25 Batch: 320 Log-loss: 0.0518752783536911\n",
      "Epoch: 25 Batch: 360 Log-loss: 0.031767915934324265\n",
      "Epoch: 25 Batch: 400 Log-loss: 0.04023262485861778\n",
      "Epoch: 25 Batch: 440 Log-loss: 0.030031854286789894\n",
      "Epoch: 25 Batch: 480 Log-loss: 0.040332015603780746\n",
      "Epoch: 25 Batch: 520 Log-loss: 0.03330840915441513\n",
      "Epoch: 25 Batch: 560 Log-loss: 0.03358166664838791\n",
      "Epoch average log-loss: 0.04059197869417923\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 25, val_loss: 0.03927607626804285, best_val_loss: 0.03927607626804285, best_auc: 0.9886309826532315\n",
      "Epoch: 26 Batch: 0 Log-loss: 0.042231325060129166\n",
      "Epoch: 26 Batch: 40 Log-loss: 0.04687544330954552\n",
      "Epoch: 26 Batch: 80 Log-loss: 0.03507247939705849\n",
      "Epoch: 26 Batch: 120 Log-loss: 0.03548580780625343\n",
      "Epoch: 26 Batch: 160 Log-loss: 0.03072635643184185\n",
      "Epoch: 26 Batch: 200 Log-loss: 0.03615014627575874\n",
      "Epoch: 26 Batch: 240 Log-loss: 0.030559659004211426\n",
      "Epoch: 26 Batch: 280 Log-loss: 0.04006744176149368\n",
      "Epoch: 26 Batch: 320 Log-loss: 0.054824765771627426\n",
      "Epoch: 26 Batch: 360 Log-loss: 0.04031790420413017\n",
      "Epoch: 26 Batch: 400 Log-loss: 0.02728274278342724\n",
      "Epoch: 26 Batch: 440 Log-loss: 0.039065781980752945\n",
      "Epoch: 26 Batch: 480 Log-loss: 0.022797955200076103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 Batch: 520 Log-loss: 0.0369560644030571\n",
      "Epoch: 26 Batch: 560 Log-loss: 0.0576544888317585\n",
      "Epoch average log-loss: 0.04041304625570774\n",
      "In Epoch: 26, val_loss: 0.03928247877733998, best_val_loss: 0.03927607626804285, best_auc: 0.9886309826532315\n",
      "Epoch: 27 Batch: 0 Log-loss: 0.044079214334487915\n",
      "Epoch: 27 Batch: 40 Log-loss: 0.039344530552625656\n",
      "Epoch: 27 Batch: 80 Log-loss: 0.04814763739705086\n",
      "Epoch: 27 Batch: 120 Log-loss: 0.04104357957839966\n",
      "Epoch: 27 Batch: 160 Log-loss: 0.057558655738830566\n",
      "Epoch: 27 Batch: 200 Log-loss: 0.05325515568256378\n",
      "Epoch: 27 Batch: 240 Log-loss: 0.04926185682415962\n",
      "Epoch: 27 Batch: 280 Log-loss: 0.04049883782863617\n",
      "Epoch: 27 Batch: 320 Log-loss: 0.052252158522605896\n",
      "Epoch: 27 Batch: 360 Log-loss: 0.031765151768922806\n",
      "Epoch: 27 Batch: 400 Log-loss: 0.029723526909947395\n",
      "Epoch: 27 Batch: 440 Log-loss: 0.033663827925920486\n",
      "Epoch: 27 Batch: 480 Log-loss: 0.03940055891871452\n",
      "Epoch: 27 Batch: 520 Log-loss: 0.05548809468746185\n",
      "Epoch: 27 Batch: 560 Log-loss: 0.04620860889554024\n",
      "Epoch average log-loss: 0.040407658188736865\n",
      "In Epoch: 27, val_loss: 0.03992399223513034, best_val_loss: 0.03927607626804285, best_auc: 0.9886309826532315\n",
      "Epoch: 28 Batch: 0 Log-loss: 0.04096535965800285\n",
      "Epoch: 28 Batch: 40 Log-loss: 0.05133712664246559\n",
      "Epoch: 28 Batch: 80 Log-loss: 0.04146015644073486\n",
      "Epoch: 28 Batch: 120 Log-loss: 0.05222463980317116\n",
      "Epoch: 28 Batch: 160 Log-loss: 0.036397650837898254\n",
      "Epoch: 28 Batch: 200 Log-loss: 0.05162348970770836\n",
      "Epoch: 28 Batch: 240 Log-loss: 0.041908588260412216\n",
      "Epoch: 28 Batch: 280 Log-loss: 0.050002023577690125\n",
      "Epoch: 28 Batch: 320 Log-loss: 0.03346521407365799\n",
      "Epoch: 28 Batch: 360 Log-loss: 0.03776296228170395\n",
      "Epoch: 28 Batch: 400 Log-loss: 0.06716872751712799\n",
      "Epoch: 28 Batch: 440 Log-loss: 0.04278206452727318\n",
      "Epoch: 28 Batch: 480 Log-loss: 0.04703609645366669\n",
      "Epoch: 28 Batch: 520 Log-loss: 0.03928164020180702\n",
      "Epoch: 28 Batch: 560 Log-loss: 0.04145005717873573\n",
      "Epoch average log-loss: 0.04032128643510597\n",
      "In Epoch: 28, val_loss: 0.04014715296880193, best_val_loss: 0.03927607626804285, best_auc: 0.9886309826532315\n",
      "Epoch: 29 Batch: 0 Log-loss: 0.043932024389505386\n",
      "Epoch: 29 Batch: 40 Log-loss: 0.04377587512135506\n",
      "Epoch: 29 Batch: 80 Log-loss: 0.04235529899597168\n",
      "Epoch: 29 Batch: 120 Log-loss: 0.03403780981898308\n",
      "Epoch: 29 Batch: 160 Log-loss: 0.02728092484176159\n",
      "Epoch: 29 Batch: 200 Log-loss: 0.0378345362842083\n",
      "Epoch: 29 Batch: 240 Log-loss: 0.0267545897513628\n",
      "Epoch: 29 Batch: 280 Log-loss: 0.03679833188652992\n",
      "Epoch: 29 Batch: 320 Log-loss: 0.03114709071815014\n",
      "Epoch: 29 Batch: 360 Log-loss: 0.0369390994310379\n",
      "Epoch: 29 Batch: 400 Log-loss: 0.04720227047801018\n",
      "Epoch: 29 Batch: 440 Log-loss: 0.02960766851902008\n",
      "Epoch: 29 Batch: 480 Log-loss: 0.03681163862347603\n",
      "Epoch: 29 Batch: 520 Log-loss: 0.04041942209005356\n",
      "Epoch: 29 Batch: 560 Log-loss: 0.03742915391921997\n",
      "Epoch average log-loss: 0.040096863389148245\n",
      "In Epoch: 29, val_loss: 0.03979974191681833, best_val_loss: 0.03927607626804285, best_auc: 0.9886309826532315\n",
      "Epoch: 30 Batch: 0 Log-loss: 0.036907751113176346\n",
      "Epoch: 30 Batch: 40 Log-loss: 0.04709716513752937\n",
      "Epoch: 30 Batch: 80 Log-loss: 0.03607959672808647\n",
      "Epoch: 30 Batch: 120 Log-loss: 0.038587041199207306\n",
      "Epoch: 30 Batch: 160 Log-loss: 0.035683199763298035\n",
      "Epoch: 30 Batch: 200 Log-loss: 0.04715223237872124\n",
      "Epoch: 30 Batch: 240 Log-loss: 0.052188608795404434\n",
      "Epoch: 30 Batch: 280 Log-loss: 0.02675248123705387\n",
      "Epoch: 30 Batch: 320 Log-loss: 0.06812693178653717\n",
      "Epoch: 30 Batch: 360 Log-loss: 0.03351091966032982\n",
      "Epoch: 30 Batch: 400 Log-loss: 0.026976199820637703\n",
      "Epoch: 30 Batch: 440 Log-loss: 0.049096617847681046\n",
      "Epoch: 30 Batch: 480 Log-loss: 0.044720377773046494\n",
      "Epoch: 30 Batch: 520 Log-loss: 0.06053095683455467\n",
      "Epoch: 30 Batch: 560 Log-loss: 0.04673397168517113\n",
      "Epoch average log-loss: 0.03996807698359979\n",
      "In Epoch: 30, val_loss: 0.03942034645635116, best_val_loss: 0.03927607626804285, best_auc: 0.9886309826532315\n",
      "Epoch: 31 Batch: 0 Log-loss: 0.0435231477022171\n",
      "Epoch: 31 Batch: 40 Log-loss: 0.05608831346035004\n",
      "Epoch: 31 Batch: 80 Log-loss: 0.04443180188536644\n",
      "Epoch: 31 Batch: 120 Log-loss: 0.04020366817712784\n",
      "Epoch: 31 Batch: 160 Log-loss: 0.04759553074836731\n",
      "Epoch: 31 Batch: 200 Log-loss: 0.05077296867966652\n",
      "Epoch: 31 Batch: 240 Log-loss: 0.05139918625354767\n",
      "Epoch: 31 Batch: 280 Log-loss: 0.04097921773791313\n",
      "Epoch: 31 Batch: 320 Log-loss: 0.04049459472298622\n",
      "Epoch: 31 Batch: 360 Log-loss: 0.04278909042477608\n",
      "Epoch: 31 Batch: 400 Log-loss: 0.040497273206710815\n",
      "Epoch: 31 Batch: 440 Log-loss: 0.04678340256214142\n",
      "Epoch: 31 Batch: 480 Log-loss: 0.04003586992621422\n",
      "Epoch: 31 Batch: 520 Log-loss: 0.03391697630286217\n",
      "Epoch: 31 Batch: 560 Log-loss: 0.05528264865279198\n",
      "Epoch average log-loss: 0.040263636616457786\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 31, val_loss: 0.03905088162707284, best_val_loss: 0.03905088162707284, best_auc: 0.9891555659193183\n",
      "Epoch: 32 Batch: 0 Log-loss: 0.05218449607491493\n",
      "Epoch: 32 Batch: 40 Log-loss: 0.053895000368356705\n",
      "Epoch: 32 Batch: 80 Log-loss: 0.0483611524105072\n",
      "Epoch: 32 Batch: 120 Log-loss: 0.028696740046143532\n",
      "Epoch: 32 Batch: 160 Log-loss: 0.048834387212991714\n",
      "Epoch: 32 Batch: 200 Log-loss: 0.054638978093862534\n",
      "Epoch: 32 Batch: 240 Log-loss: 0.054306965321302414\n",
      "Epoch: 32 Batch: 280 Log-loss: 0.03906213119626045\n",
      "Epoch: 32 Batch: 320 Log-loss: 0.05329412594437599\n",
      "Epoch: 32 Batch: 360 Log-loss: 0.035799916833639145\n",
      "Epoch: 32 Batch: 400 Log-loss: 0.04767683148384094\n",
      "Epoch: 32 Batch: 440 Log-loss: 0.04148540273308754\n",
      "Epoch: 32 Batch: 480 Log-loss: 0.04461058974266052\n",
      "Epoch: 32 Batch: 520 Log-loss: 0.037091419100761414\n",
      "Epoch: 32 Batch: 560 Log-loss: 0.028934776782989502\n",
      "Epoch average log-loss: 0.03987254196378801\n",
      "In Epoch: 32, val_loss: 0.03947664108722782, best_val_loss: 0.03905088162707284, best_auc: 0.9891555659193183\n",
      "Epoch: 33 Batch: 0 Log-loss: 0.04856167733669281\n",
      "Epoch: 33 Batch: 40 Log-loss: 0.0420319139957428\n",
      "Epoch: 33 Batch: 80 Log-loss: 0.03351461514830589\n",
      "Epoch: 33 Batch: 120 Log-loss: 0.046874236315488815\n",
      "Epoch: 33 Batch: 160 Log-loss: 0.03656245395541191\n",
      "Epoch: 33 Batch: 200 Log-loss: 0.028489505872130394\n",
      "Epoch: 33 Batch: 240 Log-loss: 0.02775510959327221\n",
      "Epoch: 33 Batch: 280 Log-loss: 0.06085781380534172\n",
      "Epoch: 33 Batch: 320 Log-loss: 0.030907973647117615\n",
      "Epoch: 33 Batch: 360 Log-loss: 0.03759988397359848\n",
      "Epoch: 33 Batch: 400 Log-loss: 0.046674326062202454\n",
      "Epoch: 33 Batch: 440 Log-loss: 0.04196682199835777\n",
      "Epoch: 33 Batch: 480 Log-loss: 0.04002757743000984\n",
      "Epoch: 33 Batch: 520 Log-loss: 0.0446733795106411\n",
      "Epoch: 33 Batch: 560 Log-loss: 0.03364567458629608\n",
      "Epoch average log-loss: 0.03982488156761974\n",
      "In Epoch: 33, val_loss: 0.03918375957722683, best_val_loss: 0.03905088162707284, best_auc: 0.9891555659193183\n",
      "Epoch: 34 Batch: 0 Log-loss: 0.041910842061042786\n",
      "Epoch: 34 Batch: 40 Log-loss: 0.04054474085569382\n",
      "Epoch: 34 Batch: 80 Log-loss: 0.03424263373017311\n",
      "Epoch: 34 Batch: 120 Log-loss: 0.03767160698771477\n",
      "Epoch: 34 Batch: 160 Log-loss: 0.036563094705343246\n",
      "Epoch: 34 Batch: 200 Log-loss: 0.040829483419656754\n",
      "Epoch: 34 Batch: 240 Log-loss: 0.02930028922855854\n",
      "Epoch: 34 Batch: 280 Log-loss: 0.03709215670824051\n",
      "Epoch: 34 Batch: 320 Log-loss: 0.03167573735117912\n",
      "Epoch: 34 Batch: 360 Log-loss: 0.028029846027493477\n",
      "Epoch: 34 Batch: 400 Log-loss: 0.040156226605176926\n",
      "Epoch: 34 Batch: 440 Log-loss: 0.04656008258461952\n",
      "Epoch: 34 Batch: 480 Log-loss: 0.04575483873486519\n",
      "Epoch: 34 Batch: 520 Log-loss: 0.035624951124191284\n",
      "Epoch: 34 Batch: 560 Log-loss: 0.03985404223203659\n",
      "Epoch average log-loss: 0.03974142087556954\n",
      "In Epoch: 34, val_loss: 0.039748221537360996, best_val_loss: 0.03905088162707284, best_auc: 0.9891555659193183\n",
      "Epoch: 35 Batch: 0 Log-loss: 0.039274297654628754\n",
      "Epoch: 35 Batch: 40 Log-loss: 0.05403168871998787\n",
      "Epoch: 35 Batch: 80 Log-loss: 0.02977866865694523\n",
      "Epoch: 35 Batch: 120 Log-loss: 0.041374411433935165\n",
      "Epoch: 35 Batch: 160 Log-loss: 0.05076378956437111\n",
      "Epoch: 35 Batch: 200 Log-loss: 0.049971938133239746\n",
      "Epoch: 35 Batch: 240 Log-loss: 0.02506186068058014\n",
      "Epoch: 35 Batch: 280 Log-loss: 0.031858738511800766\n",
      "Epoch: 35 Batch: 320 Log-loss: 0.04526764526963234\n",
      "Epoch: 35 Batch: 360 Log-loss: 0.03642367944121361\n",
      "Epoch: 35 Batch: 400 Log-loss: 0.041829902678728104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 Batch: 440 Log-loss: 0.03520004823803902\n",
      "Epoch: 35 Batch: 480 Log-loss: 0.058036744594573975\n",
      "Epoch: 35 Batch: 520 Log-loss: 0.04026423394680023\n",
      "Epoch: 35 Batch: 560 Log-loss: 0.031175360083580017\n",
      "Epoch average log-loss: 0.03995569550897926\n",
      "In Epoch: 35, val_loss: 0.039889466217785934, best_val_loss: 0.03905088162707284, best_auc: 0.9891555659193183\n",
      "Epoch: 36 Batch: 0 Log-loss: 0.03982589766383171\n",
      "Epoch: 36 Batch: 40 Log-loss: 0.03836211934685707\n",
      "Epoch: 36 Batch: 80 Log-loss: 0.0388035774230957\n",
      "Epoch: 36 Batch: 120 Log-loss: 0.03995800390839577\n",
      "Epoch: 36 Batch: 160 Log-loss: 0.04431477561593056\n",
      "Epoch: 36 Batch: 200 Log-loss: 0.025040989741683006\n",
      "Epoch: 36 Batch: 240 Log-loss: 0.04572805389761925\n",
      "Epoch: 36 Batch: 280 Log-loss: 0.03557261452078819\n",
      "Epoch: 36 Batch: 320 Log-loss: 0.033415939658880234\n",
      "Epoch: 36 Batch: 360 Log-loss: 0.04244757071137428\n",
      "Epoch: 36 Batch: 400 Log-loss: 0.038047727197408676\n",
      "Epoch: 36 Batch: 440 Log-loss: 0.05842546746134758\n",
      "Epoch: 36 Batch: 480 Log-loss: 0.04790733382105827\n",
      "Epoch: 36 Batch: 520 Log-loss: 0.038568947464227676\n",
      "Epoch: 36 Batch: 560 Log-loss: 0.05863964557647705\n",
      "Epoch average log-loss: 0.03965469383900719\n",
      "In Epoch: 36, val_loss: 0.03949658166761525, best_val_loss: 0.03905088162707284, best_auc: 0.9891555659193183\n",
      "Epoch: 37 Batch: 0 Log-loss: 0.0283199530094862\n",
      "Epoch: 37 Batch: 40 Log-loss: 0.05471305176615715\n",
      "Epoch: 37 Batch: 80 Log-loss: 0.045104142278432846\n",
      "Epoch: 37 Batch: 120 Log-loss: 0.04710933938622475\n",
      "Epoch: 37 Batch: 160 Log-loss: 0.04489697888493538\n",
      "Epoch: 37 Batch: 200 Log-loss: 0.0388772152364254\n",
      "Epoch: 37 Batch: 240 Log-loss: 0.040912430733442307\n",
      "Epoch: 37 Batch: 280 Log-loss: 0.030707748606801033\n",
      "Epoch: 37 Batch: 320 Log-loss: 0.036662399768829346\n",
      "Epoch: 37 Batch: 360 Log-loss: 0.04084297642111778\n",
      "Epoch: 37 Batch: 400 Log-loss: 0.026320023462176323\n",
      "Epoch: 37 Batch: 440 Log-loss: 0.05010920390486717\n",
      "Epoch: 37 Batch: 480 Log-loss: 0.039930347353219986\n",
      "Epoch: 37 Batch: 520 Log-loss: 0.0437454991042614\n",
      "Epoch: 37 Batch: 560 Log-loss: 0.034373026341199875\n",
      "Epoch average log-loss: 0.039679752572971794\n",
      "In Epoch: 37, val_loss: 0.03959814609859405, best_val_loss: 0.03905088162707284, best_auc: 0.9891555659193183\n",
      "Epoch: 38 Batch: 0 Log-loss: 0.03266565874218941\n",
      "Epoch: 38 Batch: 40 Log-loss: 0.028272578492760658\n",
      "Epoch: 38 Batch: 80 Log-loss: 0.04696856811642647\n",
      "Epoch: 38 Batch: 120 Log-loss: 0.040893372148275375\n",
      "Epoch: 38 Batch: 160 Log-loss: 0.03784092143177986\n",
      "Epoch: 38 Batch: 200 Log-loss: 0.017385633662343025\n",
      "Epoch: 38 Batch: 240 Log-loss: 0.02998392842710018\n",
      "Epoch: 38 Batch: 280 Log-loss: 0.06389117985963821\n",
      "Epoch: 38 Batch: 320 Log-loss: 0.05034545436501503\n",
      "Epoch: 38 Batch: 360 Log-loss: 0.03968427702784538\n",
      "Epoch: 38 Batch: 400 Log-loss: 0.04102180525660515\n",
      "Epoch: 38 Batch: 440 Log-loss: 0.040150467306375504\n",
      "Epoch: 38 Batch: 480 Log-loss: 0.03413413092494011\n",
      "Epoch: 38 Batch: 520 Log-loss: 0.04151151329278946\n",
      "Epoch: 38 Batch: 560 Log-loss: 0.03686336427927017\n",
      "Epoch average log-loss: 0.03963648647602115\n",
      "In Epoch: 38, val_loss: 0.03919118497706358, best_val_loss: 0.03905088162707284, best_auc: 0.9891555659193183\n",
      "Epoch: 39 Batch: 0 Log-loss: 0.035911284387111664\n",
      "Epoch: 39 Batch: 40 Log-loss: 0.040323466062545776\n",
      "Epoch: 39 Batch: 80 Log-loss: 0.03182581067085266\n",
      "Epoch: 39 Batch: 120 Log-loss: 0.040433358401060104\n",
      "Epoch: 39 Batch: 160 Log-loss: 0.028843119740486145\n",
      "Epoch: 39 Batch: 200 Log-loss: 0.03424932807683945\n",
      "Epoch: 39 Batch: 240 Log-loss: 0.036336783319711685\n",
      "Epoch: 39 Batch: 280 Log-loss: 0.030697986483573914\n",
      "Epoch: 39 Batch: 320 Log-loss: 0.03413771465420723\n",
      "Epoch: 39 Batch: 360 Log-loss: 0.04096336290240288\n",
      "Epoch: 39 Batch: 400 Log-loss: 0.05189639329910278\n",
      "Epoch: 39 Batch: 440 Log-loss: 0.035408806055784225\n",
      "Epoch: 39 Batch: 480 Log-loss: 0.044777851551771164\n",
      "Epoch: 39 Batch: 520 Log-loss: 0.03737472742795944\n",
      "Epoch: 39 Batch: 560 Log-loss: 0.044765371829271317\n",
      "Epoch average log-loss: 0.03956749374379537\n",
      "In Epoch: 39, val_loss: 0.03952899225614349, best_val_loss: 0.03905088162707284, best_auc: 0.9891555659193183\n",
      "Epoch: 40 Batch: 0 Log-loss: 0.04326942563056946\n",
      "Epoch: 40 Batch: 40 Log-loss: 0.04793376848101616\n",
      "Epoch: 40 Batch: 80 Log-loss: 0.041813794523477554\n",
      "Epoch: 40 Batch: 120 Log-loss: 0.044087354093790054\n",
      "Epoch: 40 Batch: 160 Log-loss: 0.02542705088853836\n",
      "Epoch: 40 Batch: 200 Log-loss: 0.05213582143187523\n",
      "Epoch: 40 Batch: 240 Log-loss: 0.03819132596254349\n",
      "Epoch: 40 Batch: 280 Log-loss: 0.05246545374393463\n",
      "Epoch: 40 Batch: 320 Log-loss: 0.038875143975019455\n",
      "Epoch: 40 Batch: 360 Log-loss: 0.032665152102708817\n",
      "Epoch: 40 Batch: 400 Log-loss: 0.04913477227091789\n",
      "Epoch: 40 Batch: 440 Log-loss: 0.056173715740442276\n",
      "Epoch: 40 Batch: 480 Log-loss: 0.056093454360961914\n",
      "Epoch: 40 Batch: 520 Log-loss: 0.03133178874850273\n",
      "Epoch: 40 Batch: 560 Log-loss: 0.025557778775691986\n",
      "Epoch average log-loss: 0.039568161744890466\n",
      "In Epoch: 40, val_loss: 0.0392863663209395, best_val_loss: 0.03905088162707284, best_auc: 0.9891555659193183\n",
      "Epoch: 41 Batch: 0 Log-loss: 0.049690719693899155\n",
      "Epoch: 41 Batch: 40 Log-loss: 0.0368829108774662\n",
      "Epoch: 41 Batch: 80 Log-loss: 0.03625180944800377\n",
      "Epoch: 41 Batch: 120 Log-loss: 0.028933510184288025\n",
      "Epoch: 41 Batch: 160 Log-loss: 0.04839921370148659\n",
      "Epoch: 41 Batch: 200 Log-loss: 0.03097412921488285\n",
      "Epoch: 41 Batch: 240 Log-loss: 0.031209684908390045\n",
      "Epoch: 41 Batch: 280 Log-loss: 0.03970916196703911\n",
      "Epoch: 41 Batch: 320 Log-loss: 0.042157310992479324\n",
      "Epoch: 41 Batch: 360 Log-loss: 0.030885756015777588\n",
      "Epoch: 41 Batch: 400 Log-loss: 0.04016295075416565\n",
      "Epoch: 41 Batch: 440 Log-loss: 0.05666952207684517\n",
      "Epoch: 41 Batch: 480 Log-loss: 0.028351260349154472\n",
      "Epoch: 41 Batch: 520 Log-loss: 0.040141187608242035\n",
      "Epoch: 41 Batch: 560 Log-loss: 0.03623238578438759\n",
      "Epoch average log-loss: 0.03972894122624504\n",
      "In Epoch: 41, val_loss: 0.03944079401647555, best_val_loss: 0.03905088162707284, best_auc: 0.9891555659193183\n",
      "Epoch: 42 Batch: 0 Log-loss: 0.03107493370771408\n",
      "Epoch: 42 Batch: 40 Log-loss: 0.03606046363711357\n",
      "Epoch: 42 Batch: 80 Log-loss: 0.04040839895606041\n",
      "Epoch: 42 Batch: 120 Log-loss: 0.02388080395758152\n",
      "Epoch: 42 Batch: 160 Log-loss: 0.04275020956993103\n",
      "Epoch: 42 Batch: 200 Log-loss: 0.043533194810152054\n",
      "Epoch: 42 Batch: 240 Log-loss: 0.041929107159376144\n",
      "Epoch: 42 Batch: 280 Log-loss: 0.03089294582605362\n",
      "Epoch: 42 Batch: 320 Log-loss: 0.03365107253193855\n",
      "Epoch: 42 Batch: 360 Log-loss: 0.027326742187142372\n",
      "Epoch: 42 Batch: 400 Log-loss: 0.04051247611641884\n",
      "Epoch: 42 Batch: 440 Log-loss: 0.03783825784921646\n",
      "Epoch: 42 Batch: 480 Log-loss: 0.036486461758613586\n",
      "Epoch: 42 Batch: 520 Log-loss: 0.041850317269563675\n",
      "Epoch: 42 Batch: 560 Log-loss: 0.026710249483585358\n",
      "Epoch average log-loss: 0.03961003685170519\n",
      "In Epoch: 42, val_loss: 0.0394202980344339, best_val_loss: 0.03905088162707284, best_auc: 0.9891555659193183\n",
      "Epoch: 43 Batch: 0 Log-loss: 0.04207757115364075\n",
      "Epoch: 43 Batch: 40 Log-loss: 0.032654646784067154\n",
      "Epoch: 43 Batch: 80 Log-loss: 0.03359726443886757\n",
      "Epoch: 43 Batch: 120 Log-loss: 0.04395805299282074\n",
      "Epoch: 43 Batch: 160 Log-loss: 0.06375496834516525\n",
      "Epoch: 43 Batch: 200 Log-loss: 0.04581347480416298\n",
      "Epoch: 43 Batch: 240 Log-loss: 0.044775236397981644\n",
      "Epoch: 43 Batch: 280 Log-loss: 0.05147513374686241\n",
      "Epoch: 43 Batch: 320 Log-loss: 0.044248420745134354\n",
      "Epoch: 43 Batch: 360 Log-loss: 0.028065605089068413\n",
      "Epoch: 43 Batch: 400 Log-loss: 0.0221052635461092\n",
      "Epoch: 43 Batch: 440 Log-loss: 0.049187857657670975\n",
      "Epoch: 43 Batch: 480 Log-loss: 0.043734002858400345\n",
      "Epoch: 43 Batch: 520 Log-loss: 0.04654229059815407\n",
      "Epoch: 43 Batch: 560 Log-loss: 0.03923831507563591\n",
      "Epoch average log-loss: 0.03953264938733939\n",
      "A pre-trained model at model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt has been loaded.\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn0.pt.\n",
      "\n",
      "Choose the torch base model.\n",
      "## Training on fold 1 ##\n",
      "Epoch: 1 Batch: 0 Log-loss: 0.7158599495887756\n",
      "Epoch: 1 Batch: 40 Log-loss: 0.16771207749843597\n",
      "Epoch: 1 Batch: 80 Log-loss: 0.10899440199136734\n",
      "Epoch: 1 Batch: 120 Log-loss: 0.08883681893348694\n",
      "Epoch: 1 Batch: 160 Log-loss: 0.07584380358457565\n",
      "Epoch: 1 Batch: 200 Log-loss: 0.06169506534934044\n",
      "Epoch: 1 Batch: 240 Log-loss: 0.06934633105993271\n",
      "Epoch: 1 Batch: 280 Log-loss: 0.07203543931245804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Batch: 320 Log-loss: 0.06599095463752747\n",
      "Epoch: 1 Batch: 360 Log-loss: 0.05448780581355095\n",
      "Epoch: 1 Batch: 400 Log-loss: 0.07055876404047012\n",
      "Epoch: 1 Batch: 440 Log-loss: 0.053293779492378235\n",
      "Epoch: 1 Batch: 480 Log-loss: 0.05364666506648064\n",
      "Epoch: 1 Batch: 520 Log-loss: 0.04730821028351784\n",
      "Epoch: 1 Batch: 560 Log-loss: 0.04034842923283577\n",
      "Epoch average log-loss: 0.08703740330945169\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 1, val_loss: 0.052948672087820954, best_val_loss: 0.052948672087820954, best_auc: 0.9712086616844072\n",
      "Epoch: 2 Batch: 0 Log-loss: 0.05144227668642998\n",
      "Epoch: 2 Batch: 40 Log-loss: 0.055775925517082214\n",
      "Epoch: 2 Batch: 80 Log-loss: 0.04900525137782097\n",
      "Epoch: 2 Batch: 120 Log-loss: 0.05491988733410835\n",
      "Epoch: 2 Batch: 160 Log-loss: 0.07973628491163254\n",
      "Epoch: 2 Batch: 200 Log-loss: 0.07194353640079498\n",
      "Epoch: 2 Batch: 240 Log-loss: 0.03282599523663521\n",
      "Epoch: 2 Batch: 280 Log-loss: 0.03812393918633461\n",
      "Epoch: 2 Batch: 320 Log-loss: 0.06153355538845062\n",
      "Epoch: 2 Batch: 360 Log-loss: 0.042002853006124496\n",
      "Epoch: 2 Batch: 400 Log-loss: 0.040207743644714355\n",
      "Epoch: 2 Batch: 440 Log-loss: 0.062006231397390366\n",
      "Epoch: 2 Batch: 480 Log-loss: 0.061228856444358826\n",
      "Epoch: 2 Batch: 520 Log-loss: 0.038257528096437454\n",
      "Epoch: 2 Batch: 560 Log-loss: 0.06138073280453682\n",
      "Epoch average log-loss: 0.05189696328847536\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 2, val_loss: 0.05006096817884805, best_val_loss: 0.05006096817884805, best_auc: 0.9770223445141305\n",
      "Epoch: 3 Batch: 0 Log-loss: 0.045109447091817856\n",
      "Epoch: 3 Batch: 40 Log-loss: 0.05404151603579521\n",
      "Epoch: 3 Batch: 80 Log-loss: 0.0486532486975193\n",
      "Epoch: 3 Batch: 120 Log-loss: 0.062292858958244324\n",
      "Epoch: 3 Batch: 160 Log-loss: 0.06271681189537048\n",
      "Epoch: 3 Batch: 200 Log-loss: 0.04457884654402733\n",
      "Epoch: 3 Batch: 240 Log-loss: 0.061217013746500015\n",
      "Epoch: 3 Batch: 280 Log-loss: 0.04325982555747032\n",
      "Epoch: 3 Batch: 320 Log-loss: 0.03277532383799553\n",
      "Epoch: 3 Batch: 360 Log-loss: 0.04428810998797417\n",
      "Epoch: 3 Batch: 400 Log-loss: 0.0488097183406353\n",
      "Epoch: 3 Batch: 440 Log-loss: 0.03416210785508156\n",
      "Epoch: 3 Batch: 480 Log-loss: 0.04607975482940674\n",
      "Epoch: 3 Batch: 520 Log-loss: 0.04157010093331337\n",
      "Epoch: 3 Batch: 560 Log-loss: 0.03998924046754837\n",
      "Epoch average log-loss: 0.04888143396736788\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 3, val_loss: 0.04718786177116668, best_val_loss: 0.04718786177116668, best_auc: 0.981705386685538\n",
      "Epoch: 4 Batch: 0 Log-loss: 0.038896769285202026\n",
      "Epoch: 4 Batch: 40 Log-loss: 0.052067458629608154\n",
      "Epoch: 4 Batch: 80 Log-loss: 0.04944291710853577\n",
      "Epoch: 4 Batch: 120 Log-loss: 0.040380049496889114\n",
      "Epoch: 4 Batch: 160 Log-loss: 0.04602348804473877\n",
      "Epoch: 4 Batch: 200 Log-loss: 0.050359368324279785\n",
      "Epoch: 4 Batch: 240 Log-loss: 0.034097108989953995\n",
      "Epoch: 4 Batch: 280 Log-loss: 0.06719797104597092\n",
      "Epoch: 4 Batch: 320 Log-loss: 0.052407246083021164\n",
      "Epoch: 4 Batch: 360 Log-loss: 0.04352368041872978\n",
      "Epoch: 4 Batch: 400 Log-loss: 0.044472843408584595\n",
      "Epoch: 4 Batch: 440 Log-loss: 0.038647886365652084\n",
      "Epoch: 4 Batch: 480 Log-loss: 0.03970902040600777\n",
      "Epoch: 4 Batch: 520 Log-loss: 0.05287826433777809\n",
      "Epoch: 4 Batch: 560 Log-loss: 0.02877231314778328\n",
      "Epoch average log-loss: 0.047072779640023196\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 4, val_loss: 0.04551940149491992, best_val_loss: 0.04551940149491992, best_auc: 0.9837761727727149\n",
      "Epoch: 5 Batch: 0 Log-loss: 0.042071834206581116\n",
      "Epoch: 5 Batch: 40 Log-loss: 0.04137303680181503\n",
      "Epoch: 5 Batch: 80 Log-loss: 0.043316930532455444\n",
      "Epoch: 5 Batch: 120 Log-loss: 0.04159637913107872\n",
      "Epoch: 5 Batch: 160 Log-loss: 0.040230412036180496\n",
      "Epoch: 5 Batch: 200 Log-loss: 0.029076486825942993\n",
      "Epoch: 5 Batch: 240 Log-loss: 0.052244458347558975\n",
      "Epoch: 5 Batch: 280 Log-loss: 0.03407928720116615\n",
      "Epoch: 5 Batch: 320 Log-loss: 0.021175643429160118\n",
      "Epoch: 5 Batch: 360 Log-loss: 0.05284297838807106\n",
      "Epoch: 5 Batch: 400 Log-loss: 0.040844324976205826\n",
      "Epoch: 5 Batch: 440 Log-loss: 0.06282728910446167\n",
      "Epoch: 5 Batch: 480 Log-loss: 0.0419873483479023\n",
      "Epoch: 5 Batch: 520 Log-loss: 0.03206295892596245\n",
      "Epoch: 5 Batch: 560 Log-loss: 0.0481518916785717\n",
      "Epoch average log-loss: 0.045591937015498324\n",
      "In Epoch: 5, val_loss: 0.045788626552226874, best_val_loss: 0.04551940149491992, best_auc: 0.9837761727727149\n",
      "Epoch: 6 Batch: 0 Log-loss: 0.03783193975687027\n",
      "Epoch: 6 Batch: 40 Log-loss: 0.04547050595283508\n",
      "Epoch: 6 Batch: 80 Log-loss: 0.03810384124517441\n",
      "Epoch: 6 Batch: 120 Log-loss: 0.03890950605273247\n",
      "Epoch: 6 Batch: 160 Log-loss: 0.0314226932823658\n",
      "Epoch: 6 Batch: 200 Log-loss: 0.04935947060585022\n",
      "Epoch: 6 Batch: 240 Log-loss: 0.049817126244306564\n",
      "Epoch: 6 Batch: 280 Log-loss: 0.043625492602586746\n",
      "Epoch: 6 Batch: 320 Log-loss: 0.059122245758771896\n",
      "Epoch: 6 Batch: 360 Log-loss: 0.06663823127746582\n",
      "Epoch: 6 Batch: 400 Log-loss: 0.03612248972058296\n",
      "Epoch: 6 Batch: 440 Log-loss: 0.03657184913754463\n",
      "Epoch: 6 Batch: 480 Log-loss: 0.05395929142832756\n",
      "Epoch: 6 Batch: 520 Log-loss: 0.033681925386190414\n",
      "Epoch: 6 Batch: 560 Log-loss: 0.045040685683488846\n",
      "Epoch average log-loss: 0.04497028669076306\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 6, val_loss: 0.04453272229354413, best_val_loss: 0.04453272229354413, best_auc: 0.9855859412778027\n",
      "Epoch: 7 Batch: 0 Log-loss: 0.031037678942084312\n",
      "Epoch: 7 Batch: 40 Log-loss: 0.037027593702077866\n",
      "Epoch: 7 Batch: 80 Log-loss: 0.03484579175710678\n",
      "Epoch: 7 Batch: 120 Log-loss: 0.031584445387125015\n",
      "Epoch: 7 Batch: 160 Log-loss: 0.051429633051157\n",
      "Epoch: 7 Batch: 200 Log-loss: 0.04554017260670662\n",
      "Epoch: 7 Batch: 240 Log-loss: 0.0435434989631176\n",
      "Epoch: 7 Batch: 280 Log-loss: 0.050475236028432846\n",
      "Epoch: 7 Batch: 320 Log-loss: 0.04931402578949928\n",
      "Epoch: 7 Batch: 360 Log-loss: 0.025667792186141014\n",
      "Epoch: 7 Batch: 400 Log-loss: 0.060601234436035156\n",
      "Epoch: 7 Batch: 440 Log-loss: 0.04853884503245354\n",
      "Epoch: 7 Batch: 480 Log-loss: 0.05022215470671654\n",
      "Epoch: 7 Batch: 520 Log-loss: 0.04963851347565651\n",
      "Epoch: 7 Batch: 560 Log-loss: 0.03489542752504349\n",
      "Epoch average log-loss: 0.04434976801941437\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 7, val_loss: 0.044213729148783625, best_val_loss: 0.044213729148783625, best_auc: 0.986365087579351\n",
      "Epoch: 8 Batch: 0 Log-loss: 0.034491632133722305\n",
      "Epoch: 8 Batch: 40 Log-loss: 0.03760430961847305\n",
      "Epoch: 8 Batch: 80 Log-loss: 0.04237184301018715\n",
      "Epoch: 8 Batch: 120 Log-loss: 0.03968019783496857\n",
      "Epoch: 8 Batch: 160 Log-loss: 0.05523735657334328\n",
      "Epoch: 8 Batch: 200 Log-loss: 0.05314522609114647\n",
      "Epoch: 8 Batch: 240 Log-loss: 0.04025844484567642\n",
      "Epoch: 8 Batch: 280 Log-loss: 0.03549705445766449\n",
      "Epoch: 8 Batch: 320 Log-loss: 0.044702883809804916\n",
      "Epoch: 8 Batch: 360 Log-loss: 0.027291422709822655\n",
      "Epoch: 8 Batch: 400 Log-loss: 0.0659860149025917\n",
      "Epoch: 8 Batch: 440 Log-loss: 0.040324367582798004\n",
      "Epoch: 8 Batch: 480 Log-loss: 0.036413852125406265\n",
      "Epoch: 8 Batch: 520 Log-loss: 0.04192006587982178\n",
      "Epoch: 8 Batch: 560 Log-loss: 0.040252406150102615\n",
      "Epoch average log-loss: 0.043738056566300136\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 8, val_loss: 0.0440217350360698, best_val_loss: 0.0440217350360698, best_auc: 0.9869510209992023\n",
      "Epoch: 9 Batch: 0 Log-loss: 0.046848196536302567\n",
      "Epoch: 9 Batch: 40 Log-loss: 0.04047280550003052\n",
      "Epoch: 9 Batch: 80 Log-loss: 0.03069656528532505\n",
      "Epoch: 9 Batch: 120 Log-loss: 0.05048177018761635\n",
      "Epoch: 9 Batch: 160 Log-loss: 0.03407558426260948\n",
      "Epoch: 9 Batch: 200 Log-loss: 0.0470355786383152\n",
      "Epoch: 9 Batch: 240 Log-loss: 0.05101529881358147\n",
      "Epoch: 9 Batch: 280 Log-loss: 0.03510056063532829\n",
      "Epoch: 9 Batch: 320 Log-loss: 0.04714002087712288\n",
      "Epoch: 9 Batch: 360 Log-loss: 0.024904610589146614\n",
      "Epoch: 9 Batch: 400 Log-loss: 0.0555284284055233\n",
      "Epoch: 9 Batch: 440 Log-loss: 0.049293842166662216\n",
      "Epoch: 9 Batch: 480 Log-loss: 0.036262091249227524\n",
      "Epoch: 9 Batch: 520 Log-loss: 0.03996984288096428\n",
      "Epoch: 9 Batch: 560 Log-loss: 0.03793346881866455\n",
      "Epoch average log-loss: 0.043140680053537446\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 9, val_loss: 0.043782811054202335, best_val_loss: 0.043782811054202335, best_auc: 0.9868052301503779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Batch: 0 Log-loss: 0.049696892499923706\n",
      "Epoch: 10 Batch: 40 Log-loss: 0.045723553746938705\n",
      "Epoch: 10 Batch: 80 Log-loss: 0.03784541040658951\n",
      "Epoch: 10 Batch: 120 Log-loss: 0.05052245035767555\n",
      "Epoch: 10 Batch: 160 Log-loss: 0.0345381535589695\n",
      "Epoch: 10 Batch: 200 Log-loss: 0.035462457686662674\n",
      "Epoch: 10 Batch: 240 Log-loss: 0.0559101365506649\n",
      "Epoch: 10 Batch: 280 Log-loss: 0.03880618140101433\n",
      "Epoch: 10 Batch: 320 Log-loss: 0.057563215494155884\n",
      "Epoch: 10 Batch: 360 Log-loss: 0.05677991732954979\n",
      "Epoch: 10 Batch: 400 Log-loss: 0.05409931018948555\n",
      "Epoch: 10 Batch: 440 Log-loss: 0.05427367612719536\n",
      "Epoch: 10 Batch: 480 Log-loss: 0.05000513419508934\n",
      "Epoch: 10 Batch: 520 Log-loss: 0.041558366268873215\n",
      "Epoch: 10 Batch: 560 Log-loss: 0.04385824874043465\n",
      "Epoch average log-loss: 0.04262197836568313\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 10, val_loss: 0.04353801126065093, best_val_loss: 0.04353801126065093, best_auc: 0.9876119636687406\n",
      "Epoch: 11 Batch: 0 Log-loss: 0.056450095027685165\n",
      "Epoch: 11 Batch: 40 Log-loss: 0.02744296006858349\n",
      "Epoch: 11 Batch: 80 Log-loss: 0.04397829249501228\n",
      "Epoch: 11 Batch: 120 Log-loss: 0.035165343433618546\n",
      "Epoch: 11 Batch: 160 Log-loss: 0.04161517694592476\n",
      "Epoch: 11 Batch: 200 Log-loss: 0.017940595746040344\n",
      "Epoch: 11 Batch: 240 Log-loss: 0.03184876963496208\n",
      "Epoch: 11 Batch: 280 Log-loss: 0.03361039236187935\n",
      "Epoch: 11 Batch: 320 Log-loss: 0.032966095954179764\n",
      "Epoch: 11 Batch: 360 Log-loss: 0.03262876346707344\n",
      "Epoch: 11 Batch: 400 Log-loss: 0.04997866973280907\n",
      "Epoch: 11 Batch: 440 Log-loss: 0.03536795824766159\n",
      "Epoch: 11 Batch: 480 Log-loss: 0.04733823612332344\n",
      "Epoch: 11 Batch: 520 Log-loss: 0.05000339820981026\n",
      "Epoch: 11 Batch: 560 Log-loss: 0.04314243420958519\n",
      "Epoch average log-loss: 0.042125455446408264\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 11, val_loss: 0.04295208049985821, best_val_loss: 0.04295208049985821, best_auc: 0.987416798378587\n",
      "Epoch: 12 Batch: 0 Log-loss: 0.045308783650398254\n",
      "Epoch: 12 Batch: 40 Log-loss: 0.04544489458203316\n",
      "Epoch: 12 Batch: 80 Log-loss: 0.047303494065999985\n",
      "Epoch: 12 Batch: 120 Log-loss: 0.038395173847675323\n",
      "Epoch: 12 Batch: 160 Log-loss: 0.037750545889139175\n",
      "Epoch: 12 Batch: 200 Log-loss: 0.028527529910206795\n",
      "Epoch: 12 Batch: 240 Log-loss: 0.03193047270178795\n",
      "Epoch: 12 Batch: 280 Log-loss: 0.04313407838344574\n",
      "Epoch: 12 Batch: 320 Log-loss: 0.02988457679748535\n",
      "Epoch: 12 Batch: 360 Log-loss: 0.04256349802017212\n",
      "Epoch: 12 Batch: 400 Log-loss: 0.024512046948075294\n",
      "Epoch: 12 Batch: 440 Log-loss: 0.03628220036625862\n",
      "Epoch: 12 Batch: 480 Log-loss: 0.03268808126449585\n",
      "Epoch: 12 Batch: 520 Log-loss: 0.036470264196395874\n",
      "Epoch: 12 Batch: 560 Log-loss: 0.04691719636321068\n",
      "Epoch average log-loss: 0.04180155536159873\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 12, val_loss: 0.04256450271044191, best_val_loss: 0.04256450271044191, best_auc: 0.9878619319801841\n",
      "Epoch: 13 Batch: 0 Log-loss: 0.03560439124703407\n",
      "Epoch: 13 Batch: 40 Log-loss: 0.03237229958176613\n",
      "Epoch: 13 Batch: 80 Log-loss: 0.0354071743786335\n",
      "Epoch: 13 Batch: 120 Log-loss: 0.03446600213646889\n",
      "Epoch: 13 Batch: 160 Log-loss: 0.06640053540468216\n",
      "Epoch: 13 Batch: 200 Log-loss: 0.05300641432404518\n",
      "Epoch: 13 Batch: 240 Log-loss: 0.0348915196955204\n",
      "Epoch: 13 Batch: 280 Log-loss: 0.04787108674645424\n",
      "Epoch: 13 Batch: 320 Log-loss: 0.04782972112298012\n",
      "Epoch: 13 Batch: 360 Log-loss: 0.05335181578993797\n",
      "Epoch: 13 Batch: 400 Log-loss: 0.04796963557600975\n",
      "Epoch: 13 Batch: 440 Log-loss: 0.04598052427172661\n",
      "Epoch: 13 Batch: 480 Log-loss: 0.0394173227250576\n",
      "Epoch: 13 Batch: 520 Log-loss: 0.052510324865579605\n",
      "Epoch: 13 Batch: 560 Log-loss: 0.03728419914841652\n",
      "Epoch average log-loss: 0.04167796375363001\n",
      "In Epoch: 13, val_loss: 0.042645460188092205, best_val_loss: 0.04256450271044191, best_auc: 0.9878619319801841\n",
      "Epoch: 14 Batch: 0 Log-loss: 0.0488082617521286\n",
      "Epoch: 14 Batch: 40 Log-loss: 0.055140722543001175\n",
      "Epoch: 14 Batch: 80 Log-loss: 0.031350087374448776\n",
      "Epoch: 14 Batch: 120 Log-loss: 0.04084895923733711\n",
      "Epoch: 14 Batch: 160 Log-loss: 0.043202754110097885\n",
      "Epoch: 14 Batch: 200 Log-loss: 0.04953545704483986\n",
      "Epoch: 14 Batch: 240 Log-loss: 0.04137510806322098\n",
      "Epoch: 14 Batch: 280 Log-loss: 0.033359918743371964\n",
      "Epoch: 14 Batch: 320 Log-loss: 0.047797441482543945\n",
      "Epoch: 14 Batch: 360 Log-loss: 0.046988993883132935\n",
      "Epoch: 14 Batch: 400 Log-loss: 0.04192095622420311\n",
      "Epoch: 14 Batch: 440 Log-loss: 0.03921521082520485\n",
      "Epoch: 14 Batch: 480 Log-loss: 0.04621312394738197\n",
      "Epoch: 14 Batch: 520 Log-loss: 0.044961314648389816\n",
      "Epoch: 14 Batch: 560 Log-loss: 0.0460391491651535\n",
      "Epoch average log-loss: 0.041500999252977115\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 14, val_loss: 0.042360231506270496, best_val_loss: 0.042360231506270496, best_auc: 0.9881832481622155\n",
      "Epoch: 15 Batch: 0 Log-loss: 0.03424021229147911\n",
      "Epoch: 15 Batch: 40 Log-loss: 0.05613267794251442\n",
      "Epoch: 15 Batch: 80 Log-loss: 0.06186569109559059\n",
      "Epoch: 15 Batch: 120 Log-loss: 0.037291400134563446\n",
      "Epoch: 15 Batch: 160 Log-loss: 0.031526122242212296\n",
      "Epoch: 15 Batch: 200 Log-loss: 0.02477314881980419\n",
      "Epoch: 15 Batch: 240 Log-loss: 0.02995266765356064\n",
      "Epoch: 15 Batch: 280 Log-loss: 0.029669009149074554\n",
      "Epoch: 15 Batch: 320 Log-loss: 0.043091002851724625\n",
      "Epoch: 15 Batch: 360 Log-loss: 0.032872386276721954\n",
      "Epoch: 15 Batch: 400 Log-loss: 0.032563965767621994\n",
      "Epoch: 15 Batch: 440 Log-loss: 0.029603833332657814\n",
      "Epoch: 15 Batch: 480 Log-loss: 0.04476458951830864\n",
      "Epoch: 15 Batch: 520 Log-loss: 0.03187018260359764\n",
      "Epoch: 15 Batch: 560 Log-loss: 0.03820851445198059\n",
      "Epoch average log-loss: 0.04107685111396547\n",
      "In Epoch: 15, val_loss: 0.04242988005460787, best_val_loss: 0.042360231506270496, best_auc: 0.9881832481622155\n",
      "Epoch: 16 Batch: 0 Log-loss: 0.027580993250012398\n",
      "Epoch: 16 Batch: 40 Log-loss: 0.04668645188212395\n",
      "Epoch: 16 Batch: 80 Log-loss: 0.04972563683986664\n",
      "Epoch: 16 Batch: 120 Log-loss: 0.04280010983347893\n",
      "Epoch: 16 Batch: 160 Log-loss: 0.04692043364048004\n",
      "Epoch: 16 Batch: 200 Log-loss: 0.06076602265238762\n",
      "Epoch: 16 Batch: 240 Log-loss: 0.034732986241579056\n",
      "Epoch: 16 Batch: 280 Log-loss: 0.0482015497982502\n",
      "Epoch: 16 Batch: 320 Log-loss: 0.04181966558098793\n",
      "Epoch: 16 Batch: 360 Log-loss: 0.039490871131420135\n",
      "Epoch: 16 Batch: 400 Log-loss: 0.04917776212096214\n",
      "Epoch: 16 Batch: 440 Log-loss: 0.041496023535728455\n",
      "Epoch: 16 Batch: 480 Log-loss: 0.033066216856241226\n",
      "Epoch: 16 Batch: 520 Log-loss: 0.06374089419841766\n",
      "Epoch: 16 Batch: 560 Log-loss: 0.03601633757352829\n",
      "Epoch average log-loss: 0.041074844818961405\n",
      "In Epoch: 16, val_loss: 0.04239124883402464, best_val_loss: 0.042360231506270496, best_auc: 0.9881832481622155\n",
      "Epoch: 17 Batch: 0 Log-loss: 0.027283912524580956\n",
      "Epoch: 17 Batch: 40 Log-loss: 0.05421507731080055\n",
      "Epoch: 17 Batch: 80 Log-loss: 0.032173410058021545\n",
      "Epoch: 17 Batch: 120 Log-loss: 0.036869946867227554\n",
      "Epoch: 17 Batch: 160 Log-loss: 0.04190570488572121\n",
      "Epoch: 17 Batch: 200 Log-loss: 0.03884149342775345\n",
      "Epoch: 17 Batch: 240 Log-loss: 0.04755214974284172\n",
      "Epoch: 17 Batch: 280 Log-loss: 0.03914975747466087\n",
      "Epoch: 17 Batch: 320 Log-loss: 0.02378949522972107\n",
      "Epoch: 17 Batch: 360 Log-loss: 0.040020205080509186\n",
      "Epoch: 17 Batch: 400 Log-loss: 0.0326482318341732\n",
      "Epoch: 17 Batch: 440 Log-loss: 0.04171912744641304\n",
      "Epoch: 17 Batch: 480 Log-loss: 0.05047132447361946\n",
      "Epoch: 17 Batch: 520 Log-loss: 0.05076859891414642\n",
      "Epoch: 17 Batch: 560 Log-loss: 0.028962768614292145\n",
      "Epoch average log-loss: 0.04073081217440111\n",
      "In Epoch: 17, val_loss: 0.04240959089044281, best_val_loss: 0.042360231506270496, best_auc: 0.9881832481622155\n",
      "Epoch: 18 Batch: 0 Log-loss: 0.06097877398133278\n",
      "Epoch: 18 Batch: 40 Log-loss: 0.04587767645716667\n",
      "Epoch: 18 Batch: 80 Log-loss: 0.05606873705983162\n",
      "Epoch: 18 Batch: 120 Log-loss: 0.02311919629573822\n",
      "Epoch: 18 Batch: 160 Log-loss: 0.0635988786816597\n",
      "Epoch: 18 Batch: 200 Log-loss: 0.0430760532617569\n",
      "Epoch: 18 Batch: 240 Log-loss: 0.04743576422333717\n",
      "Epoch: 18 Batch: 280 Log-loss: 0.04313357546925545\n",
      "Epoch: 18 Batch: 320 Log-loss: 0.02689182199537754\n",
      "Epoch: 18 Batch: 360 Log-loss: 0.040440816432237625\n",
      "Epoch: 18 Batch: 400 Log-loss: 0.03870850056409836\n",
      "Epoch: 18 Batch: 440 Log-loss: 0.03878793865442276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 Batch: 480 Log-loss: 0.029355796054005623\n",
      "Epoch: 18 Batch: 520 Log-loss: 0.029866425320506096\n",
      "Epoch: 18 Batch: 560 Log-loss: 0.03735651820898056\n",
      "Epoch average log-loss: 0.04063368532806635\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 18, val_loss: 0.041949441635225536, best_val_loss: 0.041949441635225536, best_auc: 0.9882905549486795\n",
      "Epoch: 19 Batch: 0 Log-loss: 0.03200962394475937\n",
      "Epoch: 19 Batch: 40 Log-loss: 0.04702135920524597\n",
      "Epoch: 19 Batch: 80 Log-loss: 0.044384900480508804\n",
      "Epoch: 19 Batch: 120 Log-loss: 0.044657547026872635\n",
      "Epoch: 19 Batch: 160 Log-loss: 0.040187519043684006\n",
      "Epoch: 19 Batch: 200 Log-loss: 0.03203034773468971\n",
      "Epoch: 19 Batch: 240 Log-loss: 0.04374602809548378\n",
      "Epoch: 19 Batch: 280 Log-loss: 0.03994901850819588\n",
      "Epoch: 19 Batch: 320 Log-loss: 0.03671443089842796\n",
      "Epoch: 19 Batch: 360 Log-loss: 0.031438905745744705\n",
      "Epoch: 19 Batch: 400 Log-loss: 0.04317675903439522\n",
      "Epoch: 19 Batch: 440 Log-loss: 0.023264160379767418\n",
      "Epoch: 19 Batch: 480 Log-loss: 0.037901896983385086\n",
      "Epoch: 19 Batch: 520 Log-loss: 0.03703601285815239\n",
      "Epoch: 19 Batch: 560 Log-loss: 0.0520031712949276\n",
      "Epoch average log-loss: 0.040308486770040224\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 19, val_loss: 0.04186671914565856, best_val_loss: 0.04186671914565856, best_auc: 0.9881149089411728\n",
      "Epoch: 20 Batch: 0 Log-loss: 0.041211459785699844\n",
      "Epoch: 20 Batch: 40 Log-loss: 0.038223400712013245\n",
      "Epoch: 20 Batch: 80 Log-loss: 0.053175751119852066\n",
      "Epoch: 20 Batch: 120 Log-loss: 0.03914246708154678\n",
      "Epoch: 20 Batch: 160 Log-loss: 0.03563755005598068\n",
      "Epoch: 20 Batch: 200 Log-loss: 0.025558816269040108\n",
      "Epoch: 20 Batch: 240 Log-loss: 0.04293259605765343\n",
      "Epoch: 20 Batch: 280 Log-loss: 0.039699118584394455\n",
      "Epoch: 20 Batch: 320 Log-loss: 0.04112504795193672\n",
      "Epoch: 20 Batch: 360 Log-loss: 0.04867304489016533\n",
      "Epoch: 20 Batch: 400 Log-loss: 0.03190791234374046\n",
      "Epoch: 20 Batch: 440 Log-loss: 0.04143692925572395\n",
      "Epoch: 20 Batch: 480 Log-loss: 0.03436047211289406\n",
      "Epoch: 20 Batch: 520 Log-loss: 0.03887922316789627\n",
      "Epoch: 20 Batch: 560 Log-loss: 0.0421576201915741\n",
      "Epoch average log-loss: 0.040259203308128884\n",
      "In Epoch: 20, val_loss: 0.04206484191914755, best_val_loss: 0.04186671914565856, best_auc: 0.9881149089411728\n",
      "Epoch: 21 Batch: 0 Log-loss: 0.03364836052060127\n",
      "Epoch: 21 Batch: 40 Log-loss: 0.03657463192939758\n",
      "Epoch: 21 Batch: 80 Log-loss: 0.02857244201004505\n",
      "Epoch: 21 Batch: 120 Log-loss: 0.03501799330115318\n",
      "Epoch: 21 Batch: 160 Log-loss: 0.034346114844083786\n",
      "Epoch: 21 Batch: 200 Log-loss: 0.04004397988319397\n",
      "Epoch: 21 Batch: 240 Log-loss: 0.034104812890291214\n",
      "Epoch: 21 Batch: 280 Log-loss: 0.027438359335064888\n",
      "Epoch: 21 Batch: 320 Log-loss: 0.02662268467247486\n",
      "Epoch: 21 Batch: 360 Log-loss: 0.051199670881032944\n",
      "Epoch: 21 Batch: 400 Log-loss: 0.05285915359854698\n",
      "Epoch: 21 Batch: 440 Log-loss: 0.05062411352992058\n",
      "Epoch: 21 Batch: 480 Log-loss: 0.05185419321060181\n",
      "Epoch: 21 Batch: 520 Log-loss: 0.04803013429045677\n",
      "Epoch: 21 Batch: 560 Log-loss: 0.029376359656453133\n",
      "Epoch average log-loss: 0.0400922626656081\n",
      "In Epoch: 21, val_loss: 0.042033476514519004, best_val_loss: 0.04186671914565856, best_auc: 0.9881149089411728\n",
      "Epoch: 22 Batch: 0 Log-loss: 0.04887809976935387\n",
      "Epoch: 22 Batch: 40 Log-loss: 0.037470657378435135\n",
      "Epoch: 22 Batch: 80 Log-loss: 0.022388329729437828\n",
      "Epoch: 22 Batch: 120 Log-loss: 0.042974624782800674\n",
      "Epoch: 22 Batch: 160 Log-loss: 0.0364234559237957\n",
      "Epoch: 22 Batch: 200 Log-loss: 0.04056261107325554\n",
      "Epoch: 22 Batch: 240 Log-loss: 0.02560238540172577\n",
      "Epoch: 22 Batch: 280 Log-loss: 0.03576795384287834\n",
      "Epoch: 22 Batch: 320 Log-loss: 0.05203523859381676\n",
      "Epoch: 22 Batch: 360 Log-loss: 0.03265822306275368\n",
      "Epoch: 22 Batch: 400 Log-loss: 0.04200988635420799\n",
      "Epoch: 22 Batch: 440 Log-loss: 0.04272117838263512\n",
      "Epoch: 22 Batch: 480 Log-loss: 0.0476626493036747\n",
      "Epoch: 22 Batch: 520 Log-loss: 0.04634483531117439\n",
      "Epoch: 22 Batch: 560 Log-loss: 0.03300302475690842\n",
      "Epoch average log-loss: 0.03997361904808453\n",
      "In Epoch: 22, val_loss: 0.04248511802041966, best_val_loss: 0.04186671914565856, best_auc: 0.9881149089411728\n",
      "Epoch: 23 Batch: 0 Log-loss: 0.03258882462978363\n",
      "Epoch: 23 Batch: 40 Log-loss: 0.04507680609822273\n",
      "Epoch: 23 Batch: 80 Log-loss: 0.04127325490117073\n",
      "Epoch: 23 Batch: 120 Log-loss: 0.040309805423021317\n",
      "Epoch: 23 Batch: 160 Log-loss: 0.041191328316926956\n",
      "Epoch: 23 Batch: 200 Log-loss: 0.04498237371444702\n",
      "Epoch: 23 Batch: 240 Log-loss: 0.037531305104494095\n",
      "Epoch: 23 Batch: 280 Log-loss: 0.04021889343857765\n",
      "Epoch: 23 Batch: 320 Log-loss: 0.029198596253991127\n",
      "Epoch: 23 Batch: 360 Log-loss: 0.04525105282664299\n",
      "Epoch: 23 Batch: 400 Log-loss: 0.028075678274035454\n",
      "Epoch: 23 Batch: 440 Log-loss: 0.03716165944933891\n",
      "Epoch: 23 Batch: 480 Log-loss: 0.045114535838365555\n",
      "Epoch: 23 Batch: 520 Log-loss: 0.027693049982190132\n",
      "Epoch: 23 Batch: 560 Log-loss: 0.03564189374446869\n",
      "Epoch average log-loss: 0.03989988720776247\n",
      "In Epoch: 23, val_loss: 0.042275611981536954, best_val_loss: 0.04186671914565856, best_auc: 0.9881149089411728\n",
      "Epoch: 24 Batch: 0 Log-loss: 0.036852236837148666\n",
      "Epoch: 24 Batch: 40 Log-loss: 0.05801790580153465\n",
      "Epoch: 24 Batch: 80 Log-loss: 0.02648741938173771\n",
      "Epoch: 24 Batch: 120 Log-loss: 0.039753418415784836\n",
      "Epoch: 24 Batch: 160 Log-loss: 0.04723818227648735\n",
      "Epoch: 24 Batch: 200 Log-loss: 0.04745898023247719\n",
      "Epoch: 24 Batch: 240 Log-loss: 0.03447848558425903\n",
      "Epoch: 24 Batch: 280 Log-loss: 0.02201998420059681\n",
      "Epoch: 24 Batch: 320 Log-loss: 0.041612204164266586\n",
      "Epoch: 24 Batch: 360 Log-loss: 0.04034959897398949\n",
      "Epoch: 24 Batch: 400 Log-loss: 0.040306806564331055\n",
      "Epoch: 24 Batch: 440 Log-loss: 0.027857517823576927\n",
      "Epoch: 24 Batch: 480 Log-loss: 0.04325388744473457\n",
      "Epoch: 24 Batch: 520 Log-loss: 0.023969337344169617\n",
      "Epoch: 24 Batch: 560 Log-loss: 0.04578688368201256\n",
      "Epoch average log-loss: 0.03983701627003029\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 24, val_loss: 0.04173590601706444, best_val_loss: 0.04173590601706444, best_auc: 0.9888057486059932\n",
      "Epoch: 25 Batch: 0 Log-loss: 0.03304201364517212\n",
      "Epoch: 25 Batch: 40 Log-loss: 0.03430456668138504\n",
      "Epoch: 25 Batch: 80 Log-loss: 0.03690497204661369\n",
      "Epoch: 25 Batch: 120 Log-loss: 0.04279167950153351\n",
      "Epoch: 25 Batch: 160 Log-loss: 0.04434177652001381\n",
      "Epoch: 25 Batch: 200 Log-loss: 0.05574627220630646\n",
      "Epoch: 25 Batch: 240 Log-loss: 0.04019738361239433\n",
      "Epoch: 25 Batch: 280 Log-loss: 0.03626485541462898\n",
      "Epoch: 25 Batch: 320 Log-loss: 0.04321968927979469\n",
      "Epoch: 25 Batch: 360 Log-loss: 0.04598221555352211\n",
      "Epoch: 25 Batch: 400 Log-loss: 0.03176091983914375\n",
      "Epoch: 25 Batch: 440 Log-loss: 0.043894145637750626\n",
      "Epoch: 25 Batch: 480 Log-loss: 0.029333608224987984\n",
      "Epoch: 25 Batch: 520 Log-loss: 0.032164428383111954\n",
      "Epoch: 25 Batch: 560 Log-loss: 0.03836923465132713\n",
      "Epoch average log-loss: 0.03970768383837172\n",
      "In Epoch: 25, val_loss: 0.04189641681326419, best_val_loss: 0.04173590601706444, best_auc: 0.9888057486059932\n",
      "Epoch: 26 Batch: 0 Log-loss: 0.04295983910560608\n",
      "Epoch: 26 Batch: 40 Log-loss: 0.032324980944395065\n",
      "Epoch: 26 Batch: 80 Log-loss: 0.05180477723479271\n",
      "Epoch: 26 Batch: 120 Log-loss: 0.034066446125507355\n",
      "Epoch: 26 Batch: 160 Log-loss: 0.03134521469473839\n",
      "Epoch: 26 Batch: 200 Log-loss: 0.03991394117474556\n",
      "Epoch: 26 Batch: 240 Log-loss: 0.03564571216702461\n",
      "Epoch: 26 Batch: 280 Log-loss: 0.039080869406461716\n",
      "Epoch: 26 Batch: 320 Log-loss: 0.0324300080537796\n",
      "Epoch: 26 Batch: 360 Log-loss: 0.04763273894786835\n",
      "Epoch: 26 Batch: 400 Log-loss: 0.037974853068590164\n",
      "Epoch: 26 Batch: 440 Log-loss: 0.0363556332886219\n",
      "Epoch: 26 Batch: 480 Log-loss: 0.049919456243515015\n",
      "Epoch: 26 Batch: 520 Log-loss: 0.03423374518752098\n",
      "Epoch: 26 Batch: 560 Log-loss: 0.04192623123526573\n",
      "Epoch average log-loss: 0.03976291940946664\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 26, val_loss: 0.041566256228733355, best_val_loss: 0.041566256228733355, best_auc: 0.9885440142963783\n",
      "Epoch: 27 Batch: 0 Log-loss: 0.038249190896749496\n",
      "Epoch: 27 Batch: 40 Log-loss: 0.04899236559867859\n",
      "Epoch: 27 Batch: 80 Log-loss: 0.03027639351785183\n",
      "Epoch: 27 Batch: 120 Log-loss: 0.040036868304014206\n",
      "Epoch: 27 Batch: 160 Log-loss: 0.03294063359498978\n",
      "Epoch: 27 Batch: 200 Log-loss: 0.05244468152523041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 Batch: 240 Log-loss: 0.03636728972196579\n",
      "Epoch: 27 Batch: 280 Log-loss: 0.03952385112643242\n",
      "Epoch: 27 Batch: 320 Log-loss: 0.04357675835490227\n",
      "Epoch: 27 Batch: 360 Log-loss: 0.03546091541647911\n",
      "Epoch: 27 Batch: 400 Log-loss: 0.02720884419977665\n",
      "Epoch: 27 Batch: 440 Log-loss: 0.026531359180808067\n",
      "Epoch: 27 Batch: 480 Log-loss: 0.03320727497339249\n",
      "Epoch: 27 Batch: 520 Log-loss: 0.027285272255539894\n",
      "Epoch: 27 Batch: 560 Log-loss: 0.045950476080179214\n",
      "Epoch average log-loss: 0.03935169238996293\n",
      "In Epoch: 27, val_loss: 0.04190073196046846, best_val_loss: 0.041566256228733355, best_auc: 0.9885440142963783\n",
      "Epoch: 28 Batch: 0 Log-loss: 0.053408920764923096\n",
      "Epoch: 28 Batch: 40 Log-loss: 0.03792562708258629\n",
      "Epoch: 28 Batch: 80 Log-loss: 0.039173971861600876\n",
      "Epoch: 28 Batch: 120 Log-loss: 0.03182966262102127\n",
      "Epoch: 28 Batch: 160 Log-loss: 0.03742187097668648\n",
      "Epoch: 28 Batch: 200 Log-loss: 0.04261190816760063\n",
      "Epoch: 28 Batch: 240 Log-loss: 0.03193444386124611\n",
      "Epoch: 28 Batch: 280 Log-loss: 0.04803742095828056\n",
      "Epoch: 28 Batch: 320 Log-loss: 0.03555426746606827\n",
      "Epoch: 28 Batch: 360 Log-loss: 0.03413522243499756\n",
      "Epoch: 28 Batch: 400 Log-loss: 0.0237581804394722\n",
      "Epoch: 28 Batch: 440 Log-loss: 0.03606163710355759\n",
      "Epoch: 28 Batch: 480 Log-loss: 0.03221700340509415\n",
      "Epoch: 28 Batch: 520 Log-loss: 0.02905469574034214\n",
      "Epoch: 28 Batch: 560 Log-loss: 0.03433212265372276\n",
      "Epoch average log-loss: 0.0394876418369157\n",
      "In Epoch: 28, val_loss: 0.04223100272834588, best_val_loss: 0.041566256228733355, best_auc: 0.9885440142963783\n",
      "Epoch: 29 Batch: 0 Log-loss: 0.06060204282402992\n",
      "Epoch: 29 Batch: 40 Log-loss: 0.02739102579653263\n",
      "Epoch: 29 Batch: 80 Log-loss: 0.04022699594497681\n",
      "Epoch: 29 Batch: 120 Log-loss: 0.03550862893462181\n",
      "Epoch: 29 Batch: 160 Log-loss: 0.03784297779202461\n",
      "Epoch: 29 Batch: 200 Log-loss: 0.04484432563185692\n",
      "Epoch: 29 Batch: 240 Log-loss: 0.03375773876905441\n",
      "Epoch: 29 Batch: 280 Log-loss: 0.03939805552363396\n",
      "Epoch: 29 Batch: 320 Log-loss: 0.032014861702919006\n",
      "Epoch: 29 Batch: 360 Log-loss: 0.03861101344227791\n",
      "Epoch: 29 Batch: 400 Log-loss: 0.04407218098640442\n",
      "Epoch: 29 Batch: 440 Log-loss: 0.03246954083442688\n",
      "Epoch: 29 Batch: 480 Log-loss: 0.05036284402012825\n",
      "Epoch: 29 Batch: 520 Log-loss: 0.03551063314080238\n",
      "Epoch: 29 Batch: 560 Log-loss: 0.04247578978538513\n",
      "Epoch average log-loss: 0.03930723088726933\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 29, val_loss: 0.041561890492837435, best_val_loss: 0.041561890492837435, best_auc: 0.9887654390023659\n",
      "Epoch: 30 Batch: 0 Log-loss: 0.04327714815735817\n",
      "Epoch: 30 Batch: 40 Log-loss: 0.029519984498620033\n",
      "Epoch: 30 Batch: 80 Log-loss: 0.043467383831739426\n",
      "Epoch: 30 Batch: 120 Log-loss: 0.0392727255821228\n",
      "Epoch: 30 Batch: 160 Log-loss: 0.03624580055475235\n",
      "Epoch: 30 Batch: 200 Log-loss: 0.041710272431373596\n",
      "Epoch: 30 Batch: 240 Log-loss: 0.04032318294048309\n",
      "Epoch: 30 Batch: 280 Log-loss: 0.03189373388886452\n",
      "Epoch: 30 Batch: 320 Log-loss: 0.03104274533689022\n",
      "Epoch: 30 Batch: 360 Log-loss: 0.051227301359176636\n",
      "Epoch: 30 Batch: 400 Log-loss: 0.051257047802209854\n",
      "Epoch: 30 Batch: 440 Log-loss: 0.025391733273863792\n",
      "Epoch: 30 Batch: 480 Log-loss: 0.0352792851626873\n",
      "Epoch: 30 Batch: 520 Log-loss: 0.057475119829177856\n",
      "Epoch: 30 Batch: 560 Log-loss: 0.026169829070568085\n",
      "Epoch average log-loss: 0.0393086931734745\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 30, val_loss: 0.0415158343837826, best_val_loss: 0.0415158343837826, best_auc: 0.9889273606133612\n",
      "Epoch: 31 Batch: 0 Log-loss: 0.054228752851486206\n",
      "Epoch: 31 Batch: 40 Log-loss: 0.04460381343960762\n",
      "Epoch: 31 Batch: 80 Log-loss: 0.04409327730536461\n",
      "Epoch: 31 Batch: 120 Log-loss: 0.043850068002939224\n",
      "Epoch: 31 Batch: 160 Log-loss: 0.04268217459321022\n",
      "Epoch: 31 Batch: 200 Log-loss: 0.04646509513258934\n",
      "Epoch: 31 Batch: 240 Log-loss: 0.03586355596780777\n",
      "Epoch: 31 Batch: 280 Log-loss: 0.05534501001238823\n",
      "Epoch: 31 Batch: 320 Log-loss: 0.035650428384542465\n",
      "Epoch: 31 Batch: 360 Log-loss: 0.04689665138721466\n",
      "Epoch: 31 Batch: 400 Log-loss: 0.059753891080617905\n",
      "Epoch: 31 Batch: 440 Log-loss: 0.04095045104622841\n",
      "Epoch: 31 Batch: 480 Log-loss: 0.04293052479624748\n",
      "Epoch: 31 Batch: 520 Log-loss: 0.044850293546915054\n",
      "Epoch: 31 Batch: 560 Log-loss: 0.022291356697678566\n",
      "Epoch average log-loss: 0.03927119048644922\n",
      "In Epoch: 31, val_loss: 0.041640965713353635, best_val_loss: 0.0415158343837826, best_auc: 0.9889273606133612\n",
      "Epoch: 32 Batch: 0 Log-loss: 0.05113525688648224\n",
      "Epoch: 32 Batch: 40 Log-loss: 0.019271519035100937\n",
      "Epoch: 32 Batch: 80 Log-loss: 0.051221057772636414\n",
      "Epoch: 32 Batch: 120 Log-loss: 0.03223017230629921\n",
      "Epoch: 32 Batch: 160 Log-loss: 0.03973788768053055\n",
      "Epoch: 32 Batch: 200 Log-loss: 0.03236996755003929\n",
      "Epoch: 32 Batch: 240 Log-loss: 0.046652331948280334\n",
      "Epoch: 32 Batch: 280 Log-loss: 0.06073799729347229\n",
      "Epoch: 32 Batch: 320 Log-loss: 0.049926046282052994\n",
      "Epoch: 32 Batch: 360 Log-loss: 0.046259522438049316\n",
      "Epoch: 32 Batch: 400 Log-loss: 0.034383125603199005\n",
      "Epoch: 32 Batch: 440 Log-loss: 0.03859918192028999\n",
      "Epoch: 32 Batch: 480 Log-loss: 0.03422896936535835\n",
      "Epoch: 32 Batch: 520 Log-loss: 0.03066830150783062\n",
      "Epoch: 32 Batch: 560 Log-loss: 0.0532839260995388\n",
      "Epoch average log-loss: 0.03919867780059576\n",
      "In Epoch: 32, val_loss: 0.042037825572935374, best_val_loss: 0.0415158343837826, best_auc: 0.9889273606133612\n",
      "Epoch: 33 Batch: 0 Log-loss: 0.045996133238077164\n",
      "Epoch: 33 Batch: 40 Log-loss: 0.055245187133550644\n",
      "Epoch: 33 Batch: 80 Log-loss: 0.04632825031876564\n",
      "Epoch: 33 Batch: 120 Log-loss: 0.03180724009871483\n",
      "Epoch: 33 Batch: 160 Log-loss: 0.03623015061020851\n",
      "Epoch: 33 Batch: 200 Log-loss: 0.04510239139199257\n",
      "Epoch: 33 Batch: 240 Log-loss: 0.03930235654115677\n",
      "Epoch: 33 Batch: 280 Log-loss: 0.028260095044970512\n",
      "Epoch: 33 Batch: 320 Log-loss: 0.04023480415344238\n",
      "Epoch: 33 Batch: 360 Log-loss: 0.03475472703576088\n",
      "Epoch: 33 Batch: 400 Log-loss: 0.04779496788978577\n",
      "Epoch: 33 Batch: 440 Log-loss: 0.03429583087563515\n",
      "Epoch: 33 Batch: 480 Log-loss: 0.038445960730314255\n",
      "Epoch: 33 Batch: 520 Log-loss: 0.03556615114212036\n",
      "Epoch: 33 Batch: 560 Log-loss: 0.045980073511600494\n",
      "Epoch average log-loss: 0.03902416424160557\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 33, val_loss: 0.04140714119245084, best_val_loss: 0.04140714119245084, best_auc: 0.9890981337434974\n",
      "Epoch: 34 Batch: 0 Log-loss: 0.05671870708465576\n",
      "Epoch: 34 Batch: 40 Log-loss: 0.025442473590373993\n",
      "Epoch: 34 Batch: 80 Log-loss: 0.03853762894868851\n",
      "Epoch: 34 Batch: 120 Log-loss: 0.04023684933781624\n",
      "Epoch: 34 Batch: 160 Log-loss: 0.039304278790950775\n",
      "Epoch: 34 Batch: 200 Log-loss: 0.036560624837875366\n",
      "Epoch: 34 Batch: 240 Log-loss: 0.042268555611371994\n",
      "Epoch: 34 Batch: 280 Log-loss: 0.04510335996747017\n",
      "Epoch: 34 Batch: 320 Log-loss: 0.02723025344312191\n",
      "Epoch: 34 Batch: 360 Log-loss: 0.04901361092925072\n",
      "Epoch: 34 Batch: 400 Log-loss: 0.03942940756678581\n",
      "Epoch: 34 Batch: 440 Log-loss: 0.03208218887448311\n",
      "Epoch: 34 Batch: 480 Log-loss: 0.043623145669698715\n",
      "Epoch: 34 Batch: 520 Log-loss: 0.03444419056177139\n",
      "Epoch: 34 Batch: 560 Log-loss: 0.03773747384548187\n",
      "Epoch average log-loss: 0.039007178584246766\n",
      "In Epoch: 34, val_loss: 0.041566461856075716, best_val_loss: 0.04140714119245084, best_auc: 0.9890981337434974\n",
      "Epoch: 35 Batch: 0 Log-loss: 0.056323159486055374\n",
      "Epoch: 35 Batch: 40 Log-loss: 0.06841225922107697\n",
      "Epoch: 35 Batch: 80 Log-loss: 0.04314114525914192\n",
      "Epoch: 35 Batch: 120 Log-loss: 0.037084393203258514\n",
      "Epoch: 35 Batch: 160 Log-loss: 0.03591146692633629\n",
      "Epoch: 35 Batch: 200 Log-loss: 0.04465164616703987\n",
      "Epoch: 35 Batch: 240 Log-loss: 0.03460272029042244\n",
      "Epoch: 35 Batch: 280 Log-loss: 0.04227006435394287\n",
      "Epoch: 35 Batch: 320 Log-loss: 0.03590482845902443\n",
      "Epoch: 35 Batch: 360 Log-loss: 0.049366626888513565\n",
      "Epoch: 35 Batch: 400 Log-loss: 0.053080882877111435\n",
      "Epoch: 35 Batch: 440 Log-loss: 0.04669026657938957\n",
      "Epoch: 35 Batch: 480 Log-loss: 0.037500474601984024\n",
      "Epoch: 35 Batch: 520 Log-loss: 0.040641624480485916\n",
      "Epoch: 35 Batch: 560 Log-loss: 0.03836525231599808\n",
      "Epoch average log-loss: 0.03911041438945436\n",
      "In Epoch: 35, val_loss: 0.04168201510216701, best_val_loss: 0.04140714119245084, best_auc: 0.9890981337434974\n",
      "Epoch: 36 Batch: 0 Log-loss: 0.054960403591394424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 Batch: 40 Log-loss: 0.029747089371085167\n",
      "Epoch: 36 Batch: 80 Log-loss: 0.038398709148168564\n",
      "Epoch: 36 Batch: 120 Log-loss: 0.04459230229258537\n",
      "Epoch: 36 Batch: 160 Log-loss: 0.04199961945414543\n",
      "Epoch: 36 Batch: 200 Log-loss: 0.04024933651089668\n",
      "Epoch: 36 Batch: 240 Log-loss: 0.04414758086204529\n",
      "Epoch: 36 Batch: 280 Log-loss: 0.047311991453170776\n",
      "Epoch: 36 Batch: 320 Log-loss: 0.03965149447321892\n",
      "Epoch: 36 Batch: 360 Log-loss: 0.054626647382974625\n",
      "Epoch: 36 Batch: 400 Log-loss: 0.02454730123281479\n",
      "Epoch: 36 Batch: 440 Log-loss: 0.04280513897538185\n",
      "Epoch: 36 Batch: 480 Log-loss: 0.03159680590033531\n",
      "Epoch: 36 Batch: 520 Log-loss: 0.040469687432050705\n",
      "Epoch: 36 Batch: 560 Log-loss: 0.02757883444428444\n",
      "Epoch average log-loss: 0.039072232098052544\n",
      "In Epoch: 36, val_loss: 0.04171421951889098, best_val_loss: 0.04140714119245084, best_auc: 0.9890981337434974\n",
      "Epoch: 37 Batch: 0 Log-loss: 0.04768766835331917\n",
      "Epoch: 37 Batch: 40 Log-loss: 0.053777992725372314\n",
      "Epoch: 37 Batch: 80 Log-loss: 0.04163076728582382\n",
      "Epoch: 37 Batch: 120 Log-loss: 0.041502632200717926\n",
      "Epoch: 37 Batch: 160 Log-loss: 0.03446340188384056\n",
      "Epoch: 37 Batch: 200 Log-loss: 0.05530666187405586\n",
      "Epoch: 37 Batch: 240 Log-loss: 0.033500827848911285\n",
      "Epoch: 37 Batch: 280 Log-loss: 0.04587230086326599\n",
      "Epoch: 37 Batch: 320 Log-loss: 0.03395872190594673\n",
      "Epoch: 37 Batch: 360 Log-loss: 0.050020162016153336\n",
      "Epoch: 37 Batch: 400 Log-loss: 0.04308466613292694\n",
      "Epoch: 37 Batch: 440 Log-loss: 0.02791709639132023\n",
      "Epoch: 37 Batch: 480 Log-loss: 0.034760404378175735\n",
      "Epoch: 37 Batch: 520 Log-loss: 0.02629593200981617\n",
      "Epoch: 37 Batch: 560 Log-loss: 0.036555469036102295\n",
      "Epoch average log-loss: 0.039016798730673534\n",
      "In Epoch: 37, val_loss: 0.041617133500713145, best_val_loss: 0.04140714119245084, best_auc: 0.9890981337434974\n",
      "Epoch: 38 Batch: 0 Log-loss: 0.03641057759523392\n",
      "Epoch: 38 Batch: 40 Log-loss: 0.05001635476946831\n",
      "Epoch: 38 Batch: 80 Log-loss: 0.04404167830944061\n",
      "Epoch: 38 Batch: 120 Log-loss: 0.03762215003371239\n",
      "Epoch: 38 Batch: 160 Log-loss: 0.031330909579992294\n",
      "Epoch: 38 Batch: 200 Log-loss: 0.0372835136950016\n",
      "Epoch: 38 Batch: 240 Log-loss: 0.03313257917761803\n",
      "Epoch: 38 Batch: 280 Log-loss: 0.0356435663998127\n",
      "Epoch: 38 Batch: 320 Log-loss: 0.03787478432059288\n",
      "Epoch: 38 Batch: 360 Log-loss: 0.04307540878653526\n",
      "Epoch: 38 Batch: 400 Log-loss: 0.03474048897624016\n",
      "Epoch: 38 Batch: 440 Log-loss: 0.030750414356589317\n",
      "Epoch: 38 Batch: 480 Log-loss: 0.027654355391860008\n",
      "Epoch: 38 Batch: 520 Log-loss: 0.040389399975538254\n",
      "Epoch: 38 Batch: 560 Log-loss: 0.05309363082051277\n",
      "Epoch average log-loss: 0.03888798121895109\n",
      "In Epoch: 38, val_loss: 0.04174074457231677, best_val_loss: 0.04140714119245084, best_auc: 0.9890981337434974\n",
      "Epoch: 39 Batch: 0 Log-loss: 0.03752945736050606\n",
      "Epoch: 39 Batch: 40 Log-loss: 0.02971760928630829\n",
      "Epoch: 39 Batch: 80 Log-loss: 0.02982214093208313\n",
      "Epoch: 39 Batch: 120 Log-loss: 0.030283013358712196\n",
      "Epoch: 39 Batch: 160 Log-loss: 0.038473013788461685\n",
      "Epoch: 39 Batch: 200 Log-loss: 0.039133548736572266\n",
      "Epoch: 39 Batch: 240 Log-loss: 0.04458488151431084\n",
      "Epoch: 39 Batch: 280 Log-loss: 0.04355830326676369\n",
      "Epoch: 39 Batch: 320 Log-loss: 0.04535667598247528\n",
      "Epoch: 39 Batch: 360 Log-loss: 0.0389038622379303\n",
      "Epoch: 39 Batch: 400 Log-loss: 0.03427712991833687\n",
      "Epoch: 39 Batch: 440 Log-loss: 0.04005410894751549\n",
      "Epoch: 39 Batch: 480 Log-loss: 0.04206112027168274\n",
      "Epoch: 39 Batch: 520 Log-loss: 0.058548882603645325\n",
      "Epoch: 39 Batch: 560 Log-loss: 0.046114373952150345\n",
      "Epoch average log-loss: 0.038886967467676316\n",
      "In Epoch: 39, val_loss: 0.04183740306393249, best_val_loss: 0.04140714119245084, best_auc: 0.9890981337434974\n",
      "Epoch: 40 Batch: 0 Log-loss: 0.028986191377043724\n",
      "Epoch: 40 Batch: 40 Log-loss: 0.04025528207421303\n",
      "Epoch: 40 Batch: 80 Log-loss: 0.02383258193731308\n",
      "Epoch: 40 Batch: 120 Log-loss: 0.048141028732061386\n",
      "Epoch: 40 Batch: 160 Log-loss: 0.04028688743710518\n",
      "Epoch: 40 Batch: 200 Log-loss: 0.027967071160674095\n",
      "Epoch: 40 Batch: 240 Log-loss: 0.04233533516526222\n",
      "Epoch: 40 Batch: 280 Log-loss: 0.0365457683801651\n",
      "Epoch: 40 Batch: 320 Log-loss: 0.046811629086732864\n",
      "Epoch: 40 Batch: 360 Log-loss: 0.026657411828637123\n",
      "Epoch: 40 Batch: 400 Log-loss: 0.03384309634566307\n",
      "Epoch: 40 Batch: 440 Log-loss: 0.030434399843215942\n",
      "Epoch: 40 Batch: 480 Log-loss: 0.03223629668354988\n",
      "Epoch: 40 Batch: 520 Log-loss: 0.03803735226392746\n",
      "Epoch: 40 Batch: 560 Log-loss: 0.02053053118288517\n",
      "Epoch average log-loss: 0.038944501719171444\n",
      "In Epoch: 40, val_loss: 0.04165800840163582, best_val_loss: 0.04140714119245084, best_auc: 0.9890981337434974\n",
      "Epoch: 41 Batch: 0 Log-loss: 0.039301056414842606\n",
      "Epoch: 41 Batch: 40 Log-loss: 0.04985735937952995\n",
      "Epoch: 41 Batch: 80 Log-loss: 0.04793700575828552\n",
      "Epoch: 41 Batch: 120 Log-loss: 0.053250376135110855\n",
      "Epoch: 41 Batch: 160 Log-loss: 0.03740652650594711\n",
      "Epoch: 41 Batch: 200 Log-loss: 0.02615264803171158\n",
      "Epoch: 41 Batch: 240 Log-loss: 0.03572705015540123\n",
      "Epoch: 41 Batch: 280 Log-loss: 0.03114333562552929\n",
      "Epoch: 41 Batch: 320 Log-loss: 0.043567705899477005\n",
      "Epoch: 41 Batch: 360 Log-loss: 0.039847906678915024\n",
      "Epoch: 41 Batch: 400 Log-loss: 0.032133061438798904\n",
      "Epoch: 41 Batch: 440 Log-loss: 0.04110283404588699\n",
      "Epoch: 41 Batch: 480 Log-loss: 0.05483298376202583\n",
      "Epoch: 41 Batch: 520 Log-loss: 0.038247399032115936\n",
      "Epoch: 41 Batch: 560 Log-loss: 0.04466626048088074\n",
      "Epoch average log-loss: 0.03903832890625511\n",
      "In Epoch: 41, val_loss: 0.04168535491840645, best_val_loss: 0.04140714119245084, best_auc: 0.9890981337434974\n",
      "Epoch: 42 Batch: 0 Log-loss: 0.04510429501533508\n",
      "Epoch: 42 Batch: 40 Log-loss: 0.021852202713489532\n",
      "Epoch: 42 Batch: 80 Log-loss: 0.047675296664237976\n",
      "Epoch: 42 Batch: 120 Log-loss: 0.03143277391791344\n",
      "Epoch: 42 Batch: 160 Log-loss: 0.040638603270053864\n",
      "Epoch: 42 Batch: 200 Log-loss: 0.02432953380048275\n",
      "Epoch: 42 Batch: 240 Log-loss: 0.04724404588341713\n",
      "Epoch: 42 Batch: 280 Log-loss: 0.04739835858345032\n",
      "Epoch: 42 Batch: 320 Log-loss: 0.04264235496520996\n",
      "Epoch: 42 Batch: 360 Log-loss: 0.05425932630896568\n",
      "Epoch: 42 Batch: 400 Log-loss: 0.043662264943122864\n",
      "Epoch: 42 Batch: 440 Log-loss: 0.03684785217046738\n",
      "Epoch: 42 Batch: 480 Log-loss: 0.0387752428650856\n",
      "Epoch: 42 Batch: 520 Log-loss: 0.0404268354177475\n",
      "Epoch: 42 Batch: 560 Log-loss: 0.03204784914851189\n",
      "Epoch average log-loss: 0.03882857928651252\n",
      "In Epoch: 42, val_loss: 0.04179240984593785, best_val_loss: 0.04140714119245084, best_auc: 0.9890981337434974\n",
      "Epoch: 43 Batch: 0 Log-loss: 0.04413652420043945\n",
      "Epoch: 43 Batch: 40 Log-loss: 0.02436775714159012\n",
      "Epoch: 43 Batch: 80 Log-loss: 0.046291694045066833\n",
      "Epoch: 43 Batch: 120 Log-loss: 0.030909204855561256\n",
      "Epoch: 43 Batch: 160 Log-loss: 0.03149272873997688\n",
      "Epoch: 43 Batch: 200 Log-loss: 0.04185246303677559\n",
      "Epoch: 43 Batch: 240 Log-loss: 0.03998942673206329\n",
      "Epoch: 43 Batch: 280 Log-loss: 0.037246715277433395\n",
      "Epoch: 43 Batch: 320 Log-loss: 0.03272457793354988\n",
      "Epoch: 43 Batch: 360 Log-loss: 0.04727456346154213\n",
      "Epoch: 43 Batch: 400 Log-loss: 0.05338318273425102\n",
      "Epoch: 43 Batch: 440 Log-loss: 0.0609092153608799\n",
      "Epoch: 43 Batch: 480 Log-loss: 0.0413939543068409\n",
      "Epoch: 43 Batch: 520 Log-loss: 0.034780438989400864\n",
      "Epoch: 43 Batch: 560 Log-loss: 0.04967149719595909\n",
      "Epoch average log-loss: 0.03884221245867333\n",
      "In Epoch: 43, val_loss: 0.04198755872712584, best_val_loss: 0.04140714119245084, best_auc: 0.9890981337434974\n",
      "Epoch: 44 Batch: 0 Log-loss: 0.03493982180953026\n",
      "Epoch: 44 Batch: 40 Log-loss: 0.03730977326631546\n",
      "Epoch: 44 Batch: 80 Log-loss: 0.03358820080757141\n",
      "Epoch: 44 Batch: 120 Log-loss: 0.03759605810046196\n",
      "Epoch: 44 Batch: 160 Log-loss: 0.035479217767715454\n",
      "Epoch: 44 Batch: 200 Log-loss: 0.041573017835617065\n",
      "Epoch: 44 Batch: 240 Log-loss: 0.030224718153476715\n",
      "Epoch: 44 Batch: 280 Log-loss: 0.043035343289375305\n",
      "Epoch: 44 Batch: 320 Log-loss: 0.05546744540333748\n",
      "Epoch: 44 Batch: 360 Log-loss: 0.02848290093243122\n",
      "Epoch: 44 Batch: 400 Log-loss: 0.040842339396476746\n",
      "Epoch: 44 Batch: 440 Log-loss: 0.025126947090029716\n",
      "Epoch: 44 Batch: 480 Log-loss: 0.05194687843322754\n",
      "Epoch: 44 Batch: 520 Log-loss: 0.05467638000845909\n",
      "Epoch: 44 Batch: 560 Log-loss: 0.040297940373420715\n",
      "Epoch average log-loss: 0.03845388600602746\n",
      "In Epoch: 44, val_loss: 0.041571400706010876, best_val_loss: 0.04140714119245084, best_auc: 0.9890981337434974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 Batch: 0 Log-loss: 0.03174015134572983\n",
      "Epoch: 45 Batch: 40 Log-loss: 0.07204743474721909\n",
      "Epoch: 45 Batch: 80 Log-loss: 0.04195719584822655\n",
      "Epoch: 45 Batch: 120 Log-loss: 0.03763991966843605\n",
      "Epoch: 45 Batch: 160 Log-loss: 0.03925725445151329\n",
      "Epoch: 45 Batch: 200 Log-loss: 0.035029087215662\n",
      "Epoch: 45 Batch: 240 Log-loss: 0.03618140146136284\n",
      "Epoch: 45 Batch: 280 Log-loss: 0.04382982477545738\n",
      "Epoch: 45 Batch: 320 Log-loss: 0.046890854835510254\n",
      "Epoch: 45 Batch: 360 Log-loss: 0.04026982560753822\n",
      "Epoch: 45 Batch: 400 Log-loss: 0.025770707055926323\n",
      "Epoch: 45 Batch: 440 Log-loss: 0.02677927352488041\n",
      "Epoch: 45 Batch: 480 Log-loss: 0.046928659081459045\n",
      "Epoch: 45 Batch: 520 Log-loss: 0.044834885746240616\n",
      "Epoch: 45 Batch: 560 Log-loss: 0.03872470185160637\n",
      "Epoch average log-loss: 0.03878934763238898\n",
      "A pre-trained model at model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt has been loaded.\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn1.pt.\n",
      "\n",
      "Choose the torch base model.\n",
      "## Training on fold 2 ##\n",
      "Epoch: 1 Batch: 0 Log-loss: 0.6946237087249756\n",
      "Epoch: 1 Batch: 40 Log-loss: 0.10542289167642593\n",
      "Epoch: 1 Batch: 80 Log-loss: 0.15297921001911163\n",
      "Epoch: 1 Batch: 120 Log-loss: 0.09448222070932388\n",
      "Epoch: 1 Batch: 160 Log-loss: 0.0883568823337555\n",
      "Epoch: 1 Batch: 200 Log-loss: 0.06513205170631409\n",
      "Epoch: 1 Batch: 240 Log-loss: 0.0647597387433052\n",
      "Epoch: 1 Batch: 280 Log-loss: 0.05036468431353569\n",
      "Epoch: 1 Batch: 320 Log-loss: 0.04631427302956581\n",
      "Epoch: 1 Batch: 360 Log-loss: 0.0622670017182827\n",
      "Epoch: 1 Batch: 400 Log-loss: 0.04020753875374794\n",
      "Epoch: 1 Batch: 440 Log-loss: 0.05036569759249687\n",
      "Epoch: 1 Batch: 480 Log-loss: 0.04994511604309082\n",
      "Epoch: 1 Batch: 520 Log-loss: 0.03567349538207054\n",
      "Epoch: 1 Batch: 560 Log-loss: 0.049588046967983246\n",
      "Epoch average log-loss: 0.09026660504085678\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 1, val_loss: 0.05215230476950103, best_val_loss: 0.05215230476950103, best_auc: 0.9739648478099268\n",
      "Epoch: 2 Batch: 0 Log-loss: 0.03707578033208847\n",
      "Epoch: 2 Batch: 40 Log-loss: 0.04569558426737785\n",
      "Epoch: 2 Batch: 80 Log-loss: 0.03593500703573227\n",
      "Epoch: 2 Batch: 120 Log-loss: 0.05988992750644684\n",
      "Epoch: 2 Batch: 160 Log-loss: 0.04574256017804146\n",
      "Epoch: 2 Batch: 200 Log-loss: 0.06472548842430115\n",
      "Epoch: 2 Batch: 240 Log-loss: 0.04315616562962532\n",
      "Epoch: 2 Batch: 280 Log-loss: 0.038186583667993546\n",
      "Epoch: 2 Batch: 320 Log-loss: 0.05336786434054375\n",
      "Epoch: 2 Batch: 360 Log-loss: 0.06440270692110062\n",
      "Epoch: 2 Batch: 400 Log-loss: 0.037229716777801514\n",
      "Epoch: 2 Batch: 440 Log-loss: 0.04103119671344757\n",
      "Epoch: 2 Batch: 480 Log-loss: 0.05001131817698479\n",
      "Epoch: 2 Batch: 520 Log-loss: 0.05139991641044617\n",
      "Epoch: 2 Batch: 560 Log-loss: 0.0442759208381176\n",
      "Epoch average log-loss: 0.05108692462090403\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 2, val_loss: 0.04751175779880367, best_val_loss: 0.04751175779880367, best_auc: 0.9782607994161615\n",
      "Epoch: 3 Batch: 0 Log-loss: 0.04754534736275673\n",
      "Epoch: 3 Batch: 40 Log-loss: 0.04022589698433876\n",
      "Epoch: 3 Batch: 80 Log-loss: 0.04271714761853218\n",
      "Epoch: 3 Batch: 120 Log-loss: 0.033694420009851456\n",
      "Epoch: 3 Batch: 160 Log-loss: 0.04669516906142235\n",
      "Epoch: 3 Batch: 200 Log-loss: 0.04817170277237892\n",
      "Epoch: 3 Batch: 240 Log-loss: 0.0649239644408226\n",
      "Epoch: 3 Batch: 280 Log-loss: 0.043315306305885315\n",
      "Epoch: 3 Batch: 320 Log-loss: 0.05083532631397247\n",
      "Epoch: 3 Batch: 360 Log-loss: 0.032190777361392975\n",
      "Epoch: 3 Batch: 400 Log-loss: 0.05227703973650932\n",
      "Epoch: 3 Batch: 440 Log-loss: 0.033368732780218124\n",
      "Epoch: 3 Batch: 480 Log-loss: 0.0642305240035057\n",
      "Epoch: 3 Batch: 520 Log-loss: 0.051359400153160095\n",
      "Epoch: 3 Batch: 560 Log-loss: 0.04984015226364136\n",
      "Epoch average log-loss: 0.04886384738742241\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 3, val_loss: 0.04673165400660915, best_val_loss: 0.04673165400660915, best_auc: 0.983273573227352\n",
      "Epoch: 4 Batch: 0 Log-loss: 0.03993583843111992\n",
      "Epoch: 4 Batch: 40 Log-loss: 0.06263476610183716\n",
      "Epoch: 4 Batch: 80 Log-loss: 0.0459572859108448\n",
      "Epoch: 4 Batch: 120 Log-loss: 0.04747963324189186\n",
      "Epoch: 4 Batch: 160 Log-loss: 0.05114850029349327\n",
      "Epoch: 4 Batch: 200 Log-loss: 0.03828345984220505\n",
      "Epoch: 4 Batch: 240 Log-loss: 0.03826925531029701\n",
      "Epoch: 4 Batch: 280 Log-loss: 0.048767562955617905\n",
      "Epoch: 4 Batch: 320 Log-loss: 0.03760404512286186\n",
      "Epoch: 4 Batch: 360 Log-loss: 0.03634350001811981\n",
      "Epoch: 4 Batch: 400 Log-loss: 0.04312390461564064\n",
      "Epoch: 4 Batch: 440 Log-loss: 0.04295085370540619\n",
      "Epoch: 4 Batch: 480 Log-loss: 0.043542083352804184\n",
      "Epoch: 4 Batch: 520 Log-loss: 0.04381506145000458\n",
      "Epoch: 4 Batch: 560 Log-loss: 0.045248959213495255\n",
      "Epoch average log-loss: 0.04719407357541578\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 4, val_loss: 0.04378946108648146, best_val_loss: 0.04378946108648146, best_auc: 0.9858229478317827\n",
      "Epoch: 5 Batch: 0 Log-loss: 0.040091823786497116\n",
      "Epoch: 5 Batch: 40 Log-loss: 0.06373976916074753\n",
      "Epoch: 5 Batch: 80 Log-loss: 0.0474703274667263\n",
      "Epoch: 5 Batch: 120 Log-loss: 0.05213497579097748\n",
      "Epoch: 5 Batch: 160 Log-loss: 0.03945508971810341\n",
      "Epoch: 5 Batch: 200 Log-loss: 0.058673497289419174\n",
      "Epoch: 5 Batch: 240 Log-loss: 0.049626097083091736\n",
      "Epoch: 5 Batch: 280 Log-loss: 0.03898071125149727\n",
      "Epoch: 5 Batch: 320 Log-loss: 0.060142602771520615\n",
      "Epoch: 5 Batch: 360 Log-loss: 0.0372767336666584\n",
      "Epoch: 5 Batch: 400 Log-loss: 0.06735692173242569\n",
      "Epoch: 5 Batch: 440 Log-loss: 0.04235813394188881\n",
      "Epoch: 5 Batch: 480 Log-loss: 0.0290058720856905\n",
      "Epoch: 5 Batch: 520 Log-loss: 0.049690231680870056\n",
      "Epoch: 5 Batch: 560 Log-loss: 0.05505591258406639\n",
      "Epoch average log-loss: 0.045549813485039135\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 5, val_loss: 0.04314082688583295, best_val_loss: 0.04314082688583295, best_auc: 0.9873866971204494\n",
      "Epoch: 6 Batch: 0 Log-loss: 0.05232984945178032\n",
      "Epoch: 6 Batch: 40 Log-loss: 0.03735683858394623\n",
      "Epoch: 6 Batch: 80 Log-loss: 0.05033202841877937\n",
      "Epoch: 6 Batch: 120 Log-loss: 0.03613153472542763\n",
      "Epoch: 6 Batch: 160 Log-loss: 0.057153549045324326\n",
      "Epoch: 6 Batch: 200 Log-loss: 0.04843029007315636\n",
      "Epoch: 6 Batch: 240 Log-loss: 0.03180413320660591\n",
      "Epoch: 6 Batch: 280 Log-loss: 0.035503219813108444\n",
      "Epoch: 6 Batch: 320 Log-loss: 0.03883448615670204\n",
      "Epoch: 6 Batch: 360 Log-loss: 0.029315272346138954\n",
      "Epoch: 6 Batch: 400 Log-loss: 0.029753955081105232\n",
      "Epoch: 6 Batch: 440 Log-loss: 0.027825815603137016\n",
      "Epoch: 6 Batch: 480 Log-loss: 0.026397503912448883\n",
      "Epoch: 6 Batch: 520 Log-loss: 0.04678216576576233\n",
      "Epoch: 6 Batch: 560 Log-loss: 0.04102927818894386\n",
      "Epoch average log-loss: 0.0446460768148037\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 6, val_loss: 0.043048287701177586, best_val_loss: 0.043048287701177586, best_auc: 0.9885017122234802\n",
      "Epoch: 7 Batch: 0 Log-loss: 0.05363917723298073\n",
      "Epoch: 7 Batch: 40 Log-loss: 0.045425351709127426\n",
      "Epoch: 7 Batch: 80 Log-loss: 0.042833223938941956\n",
      "Epoch: 7 Batch: 120 Log-loss: 0.04682591184973717\n",
      "Epoch: 7 Batch: 160 Log-loss: 0.045789554715156555\n",
      "Epoch: 7 Batch: 200 Log-loss: 0.040547218173742294\n",
      "Epoch: 7 Batch: 240 Log-loss: 0.044987525790929794\n",
      "Epoch: 7 Batch: 280 Log-loss: 0.051064953207969666\n",
      "Epoch: 7 Batch: 320 Log-loss: 0.03770273178815842\n",
      "Epoch: 7 Batch: 360 Log-loss: 0.05058635398745537\n",
      "Epoch: 7 Batch: 400 Log-loss: 0.044837143272161484\n",
      "Epoch: 7 Batch: 440 Log-loss: 0.03829282894730568\n",
      "Epoch: 7 Batch: 480 Log-loss: 0.03412371873855591\n",
      "Epoch: 7 Batch: 520 Log-loss: 0.033201757818460464\n",
      "Epoch: 7 Batch: 560 Log-loss: 0.06121794506907463\n",
      "Epoch average log-loss: 0.044035996350326706\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 7, val_loss: 0.041542296041397586, best_val_loss: 0.041542296041397586, best_auc: 0.9891236366433569\n",
      "Epoch: 8 Batch: 0 Log-loss: 0.05173826590180397\n",
      "Epoch: 8 Batch: 40 Log-loss: 0.04171426594257355\n",
      "Epoch: 8 Batch: 80 Log-loss: 0.04909966513514519\n",
      "Epoch: 8 Batch: 120 Log-loss: 0.03213559463620186\n",
      "Epoch: 8 Batch: 160 Log-loss: 0.0434097982943058\n",
      "Epoch: 8 Batch: 200 Log-loss: 0.04413939639925957\n",
      "Epoch: 8 Batch: 240 Log-loss: 0.03690995275974274\n",
      "Epoch: 8 Batch: 280 Log-loss: 0.03557784482836723\n",
      "Epoch: 8 Batch: 320 Log-loss: 0.04156048223376274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Batch: 360 Log-loss: 0.04403582215309143\n",
      "Epoch: 8 Batch: 400 Log-loss: 0.041644614189863205\n",
      "Epoch: 8 Batch: 440 Log-loss: 0.0430566668510437\n",
      "Epoch: 8 Batch: 480 Log-loss: 0.06036766991019249\n",
      "Epoch: 8 Batch: 520 Log-loss: 0.041684720665216446\n",
      "Epoch: 8 Batch: 560 Log-loss: 0.030884424224495888\n",
      "Epoch average log-loss: 0.04345810001915587\n",
      "In Epoch: 8, val_loss: 0.04188464199946818, best_val_loss: 0.041542296041397586, best_auc: 0.9891236366433569\n",
      "Epoch: 9 Batch: 0 Log-loss: 0.06072479486465454\n",
      "Epoch: 9 Batch: 40 Log-loss: 0.04781827703118324\n",
      "Epoch: 9 Batch: 80 Log-loss: 0.02761070430278778\n",
      "Epoch: 9 Batch: 120 Log-loss: 0.05236664041876793\n",
      "Epoch: 9 Batch: 160 Log-loss: 0.05475163832306862\n",
      "Epoch: 9 Batch: 200 Log-loss: 0.0396842435002327\n",
      "Epoch: 9 Batch: 240 Log-loss: 0.03905150294303894\n",
      "Epoch: 9 Batch: 280 Log-loss: 0.04884999617934227\n",
      "Epoch: 9 Batch: 320 Log-loss: 0.04391162097454071\n",
      "Epoch: 9 Batch: 360 Log-loss: 0.03227635845541954\n",
      "Epoch: 9 Batch: 400 Log-loss: 0.04545612633228302\n",
      "Epoch: 9 Batch: 440 Log-loss: 0.04797300323843956\n",
      "Epoch: 9 Batch: 480 Log-loss: 0.0500635989010334\n",
      "Epoch: 9 Batch: 520 Log-loss: 0.04827997088432312\n",
      "Epoch: 9 Batch: 560 Log-loss: 0.03816080093383789\n",
      "Epoch average log-loss: 0.043127106703884366\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 9, val_loss: 0.04143617366403059, best_val_loss: 0.04143617366403059, best_auc: 0.9890775189562517\n",
      "Epoch: 10 Batch: 0 Log-loss: 0.0529562383890152\n",
      "Epoch: 10 Batch: 40 Log-loss: 0.03508754074573517\n",
      "Epoch: 10 Batch: 80 Log-loss: 0.03527512028813362\n",
      "Epoch: 10 Batch: 120 Log-loss: 0.034611113369464874\n",
      "Epoch: 10 Batch: 160 Log-loss: 0.03436017408967018\n",
      "Epoch: 10 Batch: 200 Log-loss: 0.031835585832595825\n",
      "Epoch: 10 Batch: 240 Log-loss: 0.0522640198469162\n",
      "Epoch: 10 Batch: 280 Log-loss: 0.0407746359705925\n",
      "Epoch: 10 Batch: 320 Log-loss: 0.03410658612847328\n",
      "Epoch: 10 Batch: 360 Log-loss: 0.05262526869773865\n",
      "Epoch: 10 Batch: 400 Log-loss: 0.0350925587117672\n",
      "Epoch: 10 Batch: 440 Log-loss: 0.0329621322453022\n",
      "Epoch: 10 Batch: 480 Log-loss: 0.05557556822896004\n",
      "Epoch: 10 Batch: 520 Log-loss: 0.038747698068618774\n",
      "Epoch: 10 Batch: 560 Log-loss: 0.05442029982805252\n",
      "Epoch average log-loss: 0.042500529807460094\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 10, val_loss: 0.041041599718212855, best_val_loss: 0.041041599718212855, best_auc: 0.9895495045996826\n",
      "Epoch: 11 Batch: 0 Log-loss: 0.04287712648510933\n",
      "Epoch: 11 Batch: 40 Log-loss: 0.04652098938822746\n",
      "Epoch: 11 Batch: 80 Log-loss: 0.04974915087223053\n",
      "Epoch: 11 Batch: 120 Log-loss: 0.04652496799826622\n",
      "Epoch: 11 Batch: 160 Log-loss: 0.053918275982141495\n",
      "Epoch: 11 Batch: 200 Log-loss: 0.05239954963326454\n",
      "Epoch: 11 Batch: 240 Log-loss: 0.04855688288807869\n",
      "Epoch: 11 Batch: 280 Log-loss: 0.040459517389535904\n",
      "Epoch: 11 Batch: 320 Log-loss: 0.03976260870695114\n",
      "Epoch: 11 Batch: 360 Log-loss: 0.03757169842720032\n",
      "Epoch: 11 Batch: 400 Log-loss: 0.03822532668709755\n",
      "Epoch: 11 Batch: 440 Log-loss: 0.03575750067830086\n",
      "Epoch: 11 Batch: 480 Log-loss: 0.04190679267048836\n",
      "Epoch: 11 Batch: 520 Log-loss: 0.02158576250076294\n",
      "Epoch: 11 Batch: 560 Log-loss: 0.042280908674001694\n",
      "Epoch average log-loss: 0.04227316611047302\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 11, val_loss: 0.04071597119638095, best_val_loss: 0.04071597119638095, best_auc: 0.989740712471508\n",
      "Epoch: 12 Batch: 0 Log-loss: 0.05800750479102135\n",
      "Epoch: 12 Batch: 40 Log-loss: 0.04752172902226448\n",
      "Epoch: 12 Batch: 80 Log-loss: 0.041753243654966354\n",
      "Epoch: 12 Batch: 120 Log-loss: 0.052103325724601746\n",
      "Epoch: 12 Batch: 160 Log-loss: 0.05802925303578377\n",
      "Epoch: 12 Batch: 200 Log-loss: 0.04224519431591034\n",
      "Epoch: 12 Batch: 240 Log-loss: 0.031177891418337822\n",
      "Epoch: 12 Batch: 280 Log-loss: 0.03739563375711441\n",
      "Epoch: 12 Batch: 320 Log-loss: 0.046002164483070374\n",
      "Epoch: 12 Batch: 360 Log-loss: 0.05232800170779228\n",
      "Epoch: 12 Batch: 400 Log-loss: 0.03055931068956852\n",
      "Epoch: 12 Batch: 440 Log-loss: 0.04083544760942459\n",
      "Epoch: 12 Batch: 480 Log-loss: 0.04152979701757431\n",
      "Epoch: 12 Batch: 520 Log-loss: 0.033710550516843796\n",
      "Epoch: 12 Batch: 560 Log-loss: 0.03908518701791763\n",
      "Epoch average log-loss: 0.0420175092860258\n",
      "In Epoch: 12, val_loss: 0.0412448445788166, best_val_loss: 0.04071597119638095, best_auc: 0.989740712471508\n",
      "Epoch: 13 Batch: 0 Log-loss: 0.0394405834376812\n",
      "Epoch: 13 Batch: 40 Log-loss: 0.03384597972035408\n",
      "Epoch: 13 Batch: 80 Log-loss: 0.038173843175172806\n",
      "Epoch: 13 Batch: 120 Log-loss: 0.047727569937705994\n",
      "Epoch: 13 Batch: 160 Log-loss: 0.04455016553401947\n",
      "Epoch: 13 Batch: 200 Log-loss: 0.048772480338811874\n",
      "Epoch: 13 Batch: 240 Log-loss: 0.04692612588405609\n",
      "Epoch: 13 Batch: 280 Log-loss: 0.04895779862999916\n",
      "Epoch: 13 Batch: 320 Log-loss: 0.053976744413375854\n",
      "Epoch: 13 Batch: 360 Log-loss: 0.03698895499110222\n",
      "Epoch: 13 Batch: 400 Log-loss: 0.04099448397755623\n",
      "Epoch: 13 Batch: 440 Log-loss: 0.032818540930747986\n",
      "Epoch: 13 Batch: 480 Log-loss: 0.03232647106051445\n",
      "Epoch: 13 Batch: 520 Log-loss: 0.04163946583867073\n",
      "Epoch: 13 Batch: 560 Log-loss: 0.049326714128255844\n",
      "Epoch average log-loss: 0.0417995284544304\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 13, val_loss: 0.04043081126792159, best_val_loss: 0.04043081126792159, best_auc: 0.9893814577197895\n",
      "Epoch: 14 Batch: 0 Log-loss: 0.05850758031010628\n",
      "Epoch: 14 Batch: 40 Log-loss: 0.035542890429496765\n",
      "Epoch: 14 Batch: 80 Log-loss: 0.05478769168257713\n",
      "Epoch: 14 Batch: 120 Log-loss: 0.0374392569065094\n",
      "Epoch: 14 Batch: 160 Log-loss: 0.04242631793022156\n",
      "Epoch: 14 Batch: 200 Log-loss: 0.03267968073487282\n",
      "Epoch: 14 Batch: 240 Log-loss: 0.03470412269234657\n",
      "Epoch: 14 Batch: 280 Log-loss: 0.0558377243578434\n",
      "Epoch: 14 Batch: 320 Log-loss: 0.05064867064356804\n",
      "Epoch: 14 Batch: 360 Log-loss: 0.035671915858983994\n",
      "Epoch: 14 Batch: 400 Log-loss: 0.03542719781398773\n",
      "Epoch: 14 Batch: 440 Log-loss: 0.026795750483870506\n",
      "Epoch: 14 Batch: 480 Log-loss: 0.03842023387551308\n",
      "Epoch: 14 Batch: 520 Log-loss: 0.05449885129928589\n",
      "Epoch: 14 Batch: 560 Log-loss: 0.0313599593937397\n",
      "Epoch average log-loss: 0.04147575639321336\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 14, val_loss: 0.04040052411735342, best_val_loss: 0.04040052411735342, best_auc: 0.9895947568488105\n",
      "Epoch: 15 Batch: 0 Log-loss: 0.04635168984532356\n",
      "Epoch: 15 Batch: 40 Log-loss: 0.05648782476782799\n",
      "Epoch: 15 Batch: 80 Log-loss: 0.029799185693264008\n",
      "Epoch: 15 Batch: 120 Log-loss: 0.04662508890032768\n",
      "Epoch: 15 Batch: 160 Log-loss: 0.03211233392357826\n",
      "Epoch: 15 Batch: 200 Log-loss: 0.044288069009780884\n",
      "Epoch: 15 Batch: 240 Log-loss: 0.043976783752441406\n",
      "Epoch: 15 Batch: 280 Log-loss: 0.034506868571043015\n",
      "Epoch: 15 Batch: 320 Log-loss: 0.025574572384357452\n",
      "Epoch: 15 Batch: 360 Log-loss: 0.05045019090175629\n",
      "Epoch: 15 Batch: 400 Log-loss: 0.03928699716925621\n",
      "Epoch: 15 Batch: 440 Log-loss: 0.05257776752114296\n",
      "Epoch: 15 Batch: 480 Log-loss: 0.030149152502417564\n",
      "Epoch: 15 Batch: 520 Log-loss: 0.046524789184331894\n",
      "Epoch: 15 Batch: 560 Log-loss: 0.03210165351629257\n",
      "Epoch average log-loss: 0.0414496088134391\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 15, val_loss: 0.04019114703902692, best_val_loss: 0.04019114703902692, best_auc: 0.9895064230716057\n",
      "Epoch: 16 Batch: 0 Log-loss: 0.041885096579790115\n",
      "Epoch: 16 Batch: 40 Log-loss: 0.042306482791900635\n",
      "Epoch: 16 Batch: 80 Log-loss: 0.03948131576180458\n",
      "Epoch: 16 Batch: 120 Log-loss: 0.035617709159851074\n",
      "Epoch: 16 Batch: 160 Log-loss: 0.03720371052622795\n",
      "Epoch: 16 Batch: 200 Log-loss: 0.04679986461997032\n",
      "Epoch: 16 Batch: 240 Log-loss: 0.0284184068441391\n",
      "Epoch: 16 Batch: 280 Log-loss: 0.04106464236974716\n",
      "Epoch: 16 Batch: 320 Log-loss: 0.05461670085787773\n",
      "Epoch: 16 Batch: 360 Log-loss: 0.04359162226319313\n",
      "Epoch: 16 Batch: 400 Log-loss: 0.05005943775177002\n",
      "Epoch: 16 Batch: 440 Log-loss: 0.040819086134433746\n",
      "Epoch: 16 Batch: 480 Log-loss: 0.03717401623725891\n",
      "Epoch: 16 Batch: 520 Log-loss: 0.0517134927213192\n",
      "Epoch: 16 Batch: 560 Log-loss: 0.04042569547891617\n",
      "Epoch average log-loss: 0.04121636622585356\n",
      "In Epoch: 16, val_loss: 0.04051180681619958, best_val_loss: 0.04019114703902692, best_auc: 0.9895064230716057\n",
      "Epoch: 17 Batch: 0 Log-loss: 0.026926493272185326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 Batch: 40 Log-loss: 0.03628319874405861\n",
      "Epoch: 17 Batch: 80 Log-loss: 0.04652724042534828\n",
      "Epoch: 17 Batch: 120 Log-loss: 0.038793306797742844\n",
      "Epoch: 17 Batch: 160 Log-loss: 0.049183014780282974\n",
      "Epoch: 17 Batch: 200 Log-loss: 0.05843858793377876\n",
      "Epoch: 17 Batch: 240 Log-loss: 0.04956461116671562\n",
      "Epoch: 17 Batch: 280 Log-loss: 0.04241880774497986\n",
      "Epoch: 17 Batch: 320 Log-loss: 0.043617114424705505\n",
      "Epoch: 17 Batch: 360 Log-loss: 0.03677980229258537\n",
      "Epoch: 17 Batch: 400 Log-loss: 0.02205084078013897\n",
      "Epoch: 17 Batch: 440 Log-loss: 0.03506985679268837\n",
      "Epoch: 17 Batch: 480 Log-loss: 0.03242848441004753\n",
      "Epoch: 17 Batch: 520 Log-loss: 0.038468364626169205\n",
      "Epoch: 17 Batch: 560 Log-loss: 0.03726230561733246\n",
      "Epoch average log-loss: 0.041010015328148644\n",
      "In Epoch: 17, val_loss: 0.0405329259181784, best_val_loss: 0.04019114703902692, best_auc: 0.9895064230716057\n",
      "Epoch: 18 Batch: 0 Log-loss: 0.044072508811950684\n",
      "Epoch: 18 Batch: 40 Log-loss: 0.03758889436721802\n",
      "Epoch: 18 Batch: 80 Log-loss: 0.03776320442557335\n",
      "Epoch: 18 Batch: 120 Log-loss: 0.04581062123179436\n",
      "Epoch: 18 Batch: 160 Log-loss: 0.03758452832698822\n",
      "Epoch: 18 Batch: 200 Log-loss: 0.03785942122340202\n",
      "Epoch: 18 Batch: 240 Log-loss: 0.029177524149417877\n",
      "Epoch: 18 Batch: 280 Log-loss: 0.05054450035095215\n",
      "Epoch: 18 Batch: 320 Log-loss: 0.028369508683681488\n",
      "Epoch: 18 Batch: 360 Log-loss: 0.049538567662239075\n",
      "Epoch: 18 Batch: 400 Log-loss: 0.04087099805474281\n",
      "Epoch: 18 Batch: 440 Log-loss: 0.02505749650299549\n",
      "Epoch: 18 Batch: 480 Log-loss: 0.038596946746110916\n",
      "Epoch: 18 Batch: 520 Log-loss: 0.03168708458542824\n",
      "Epoch: 18 Batch: 560 Log-loss: 0.056449685245752335\n",
      "Epoch average log-loss: 0.04080200823622623\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 18, val_loss: 0.04000380089726515, best_val_loss: 0.04000380089726515, best_auc: 0.9899750191480434\n",
      "Epoch: 19 Batch: 0 Log-loss: 0.03566885367035866\n",
      "Epoch: 19 Batch: 40 Log-loss: 0.04187849536538124\n",
      "Epoch: 19 Batch: 80 Log-loss: 0.03956349566578865\n",
      "Epoch: 19 Batch: 120 Log-loss: 0.045189738273620605\n",
      "Epoch: 19 Batch: 160 Log-loss: 0.044716645032167435\n",
      "Epoch: 19 Batch: 200 Log-loss: 0.03995971009135246\n",
      "Epoch: 19 Batch: 240 Log-loss: 0.05081765353679657\n",
      "Epoch: 19 Batch: 280 Log-loss: 0.03945222496986389\n",
      "Epoch: 19 Batch: 320 Log-loss: 0.036255475133657455\n",
      "Epoch: 19 Batch: 360 Log-loss: 0.0356610044836998\n",
      "Epoch: 19 Batch: 400 Log-loss: 0.04510684311389923\n",
      "Epoch: 19 Batch: 440 Log-loss: 0.027725106105208397\n",
      "Epoch: 19 Batch: 480 Log-loss: 0.03630799427628517\n",
      "Epoch: 19 Batch: 520 Log-loss: 0.03188742324709892\n",
      "Epoch: 19 Batch: 560 Log-loss: 0.03864765539765358\n",
      "Epoch average log-loss: 0.040642405870104474\n",
      "In Epoch: 19, val_loss: 0.040091233596999405, best_val_loss: 0.04000380089726515, best_auc: 0.9899750191480434\n",
      "Epoch: 20 Batch: 0 Log-loss: 0.04550034925341606\n",
      "Epoch: 20 Batch: 40 Log-loss: 0.042096611112356186\n",
      "Epoch: 20 Batch: 80 Log-loss: 0.03475454822182655\n",
      "Epoch: 20 Batch: 120 Log-loss: 0.026560457423329353\n",
      "Epoch: 20 Batch: 160 Log-loss: 0.03461531177163124\n",
      "Epoch: 20 Batch: 200 Log-loss: 0.04662725701928139\n",
      "Epoch: 20 Batch: 240 Log-loss: 0.029048943892121315\n",
      "Epoch: 20 Batch: 280 Log-loss: 0.025943851098418236\n",
      "Epoch: 20 Batch: 320 Log-loss: 0.03611388057470322\n",
      "Epoch: 20 Batch: 360 Log-loss: 0.038596976548433304\n",
      "Epoch: 20 Batch: 400 Log-loss: 0.03574944660067558\n",
      "Epoch: 20 Batch: 440 Log-loss: 0.03766627237200737\n",
      "Epoch: 20 Batch: 480 Log-loss: 0.03632152080535889\n",
      "Epoch: 20 Batch: 520 Log-loss: 0.042186614125967026\n",
      "Epoch: 20 Batch: 560 Log-loss: 0.054512105882167816\n",
      "Epoch average log-loss: 0.04044481165307973\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 20, val_loss: 0.03993927199705321, best_val_loss: 0.03993927199705321, best_auc: 0.9896101726857118\n",
      "Epoch: 21 Batch: 0 Log-loss: 0.04418036341667175\n",
      "Epoch: 21 Batch: 40 Log-loss: 0.04394448176026344\n",
      "Epoch: 21 Batch: 80 Log-loss: 0.034098848700523376\n",
      "Epoch: 21 Batch: 120 Log-loss: 0.03103780932724476\n",
      "Epoch: 21 Batch: 160 Log-loss: 0.03687344864010811\n",
      "Epoch: 21 Batch: 200 Log-loss: 0.05621680989861488\n",
      "Epoch: 21 Batch: 240 Log-loss: 0.03358618542551994\n",
      "Epoch: 21 Batch: 280 Log-loss: 0.04129626229405403\n",
      "Epoch: 21 Batch: 320 Log-loss: 0.033608708530664444\n",
      "Epoch: 21 Batch: 360 Log-loss: 0.04567704722285271\n",
      "Epoch: 21 Batch: 400 Log-loss: 0.03197820112109184\n",
      "Epoch: 21 Batch: 440 Log-loss: 0.030338497832417488\n",
      "Epoch: 21 Batch: 480 Log-loss: 0.028357697650790215\n",
      "Epoch: 21 Batch: 520 Log-loss: 0.04795050993561745\n",
      "Epoch: 21 Batch: 560 Log-loss: 0.03062092512845993\n",
      "Epoch average log-loss: 0.0401082468318886\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 21, val_loss: 0.039902863592985345, best_val_loss: 0.039902863592985345, best_auc: 0.9901654712539857\n",
      "Epoch: 22 Batch: 0 Log-loss: 0.04864373430609703\n",
      "Epoch: 22 Batch: 40 Log-loss: 0.04337308928370476\n",
      "Epoch: 22 Batch: 80 Log-loss: 0.034678686410188675\n",
      "Epoch: 22 Batch: 120 Log-loss: 0.020701322704553604\n",
      "Epoch: 22 Batch: 160 Log-loss: 0.03615264594554901\n",
      "Epoch: 22 Batch: 200 Log-loss: 0.027547379955649376\n",
      "Epoch: 22 Batch: 240 Log-loss: 0.04357808828353882\n",
      "Epoch: 22 Batch: 280 Log-loss: 0.058610573410987854\n",
      "Epoch: 22 Batch: 320 Log-loss: 0.04521169885993004\n",
      "Epoch: 22 Batch: 360 Log-loss: 0.04612189903855324\n",
      "Epoch: 22 Batch: 400 Log-loss: 0.03710972145199776\n",
      "Epoch: 22 Batch: 440 Log-loss: 0.05394933745265007\n",
      "Epoch: 22 Batch: 480 Log-loss: 0.03288711979985237\n",
      "Epoch: 22 Batch: 520 Log-loss: 0.034555599093437195\n",
      "Epoch: 22 Batch: 560 Log-loss: 0.033739760518074036\n",
      "Epoch average log-loss: 0.04012256413698197\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 22, val_loss: 0.03970927165875476, best_val_loss: 0.03970927165875476, best_auc: 0.9901587325131563\n",
      "Epoch: 23 Batch: 0 Log-loss: 0.030039465054869652\n",
      "Epoch: 23 Batch: 40 Log-loss: 0.030154719948768616\n",
      "Epoch: 23 Batch: 80 Log-loss: 0.03659119829535484\n",
      "Epoch: 23 Batch: 120 Log-loss: 0.05342916026711464\n",
      "Epoch: 23 Batch: 160 Log-loss: 0.04091991111636162\n",
      "Epoch: 23 Batch: 200 Log-loss: 0.03191780298948288\n",
      "Epoch: 23 Batch: 240 Log-loss: 0.03516073152422905\n",
      "Epoch: 23 Batch: 280 Log-loss: 0.03396087884902954\n",
      "Epoch: 23 Batch: 320 Log-loss: 0.04812799394130707\n",
      "Epoch: 23 Batch: 360 Log-loss: 0.0355144627392292\n",
      "Epoch: 23 Batch: 400 Log-loss: 0.04188239201903343\n",
      "Epoch: 23 Batch: 440 Log-loss: 0.039461515843868256\n",
      "Epoch: 23 Batch: 480 Log-loss: 0.042995382100343704\n",
      "Epoch: 23 Batch: 520 Log-loss: 0.03538220003247261\n",
      "Epoch: 23 Batch: 560 Log-loss: 0.05448811873793602\n",
      "Epoch average log-loss: 0.04011146268208644\n",
      "In Epoch: 23, val_loss: 0.03991499339025386, best_val_loss: 0.03970927165875476, best_auc: 0.9901587325131563\n",
      "Epoch: 24 Batch: 0 Log-loss: 0.04917145147919655\n",
      "Epoch: 24 Batch: 40 Log-loss: 0.04157315194606781\n",
      "Epoch: 24 Batch: 80 Log-loss: 0.036680031567811966\n",
      "Epoch: 24 Batch: 120 Log-loss: 0.04860996827483177\n",
      "Epoch: 24 Batch: 160 Log-loss: 0.06512576341629028\n",
      "Epoch: 24 Batch: 200 Log-loss: 0.026757478713989258\n",
      "Epoch: 24 Batch: 240 Log-loss: 0.030204331502318382\n",
      "Epoch: 24 Batch: 280 Log-loss: 0.02862510271370411\n",
      "Epoch: 24 Batch: 320 Log-loss: 0.037889301776885986\n",
      "Epoch: 24 Batch: 360 Log-loss: 0.048683565109968185\n",
      "Epoch: 24 Batch: 400 Log-loss: 0.03941822424530983\n",
      "Epoch: 24 Batch: 440 Log-loss: 0.02950502373278141\n",
      "Epoch: 24 Batch: 480 Log-loss: 0.047650739550590515\n",
      "Epoch: 24 Batch: 520 Log-loss: 0.046455349773168564\n",
      "Epoch: 24 Batch: 560 Log-loss: 0.04175331071019173\n",
      "Epoch average log-loss: 0.03997987472186131\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 24, val_loss: 0.039671242604912775, best_val_loss: 0.039671242604912775, best_auc: 0.9900922645566043\n",
      "Epoch: 25 Batch: 0 Log-loss: 0.05571097135543823\n",
      "Epoch: 25 Batch: 40 Log-loss: 0.04356713965535164\n",
      "Epoch: 25 Batch: 80 Log-loss: 0.044679779559373856\n",
      "Epoch: 25 Batch: 120 Log-loss: 0.040426451712846756\n",
      "Epoch: 25 Batch: 160 Log-loss: 0.05224233493208885\n",
      "Epoch: 25 Batch: 200 Log-loss: 0.038846373558044434\n",
      "Epoch: 25 Batch: 240 Log-loss: 0.0398041270673275\n",
      "Epoch: 25 Batch: 280 Log-loss: 0.04668338596820831\n",
      "Epoch: 25 Batch: 320 Log-loss: 0.04291810467839241\n",
      "Epoch: 25 Batch: 360 Log-loss: 0.03809482604265213\n",
      "Epoch: 25 Batch: 400 Log-loss: 0.06317073851823807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 Batch: 440 Log-loss: 0.04173193499445915\n",
      "Epoch: 25 Batch: 480 Log-loss: 0.022600390017032623\n",
      "Epoch: 25 Batch: 520 Log-loss: 0.04010700806975365\n",
      "Epoch: 25 Batch: 560 Log-loss: 0.034383464604616165\n",
      "Epoch average log-loss: 0.039839775690675844\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 25, val_loss: 0.039670934016694186, best_val_loss: 0.039670934016694186, best_auc: 0.9905380881008642\n",
      "Epoch: 26 Batch: 0 Log-loss: 0.03860387206077576\n",
      "Epoch: 26 Batch: 40 Log-loss: 0.03736722096800804\n",
      "Epoch: 26 Batch: 80 Log-loss: 0.041188325732946396\n",
      "Epoch: 26 Batch: 120 Log-loss: 0.02929905615746975\n",
      "Epoch: 26 Batch: 160 Log-loss: 0.03927440196275711\n",
      "Epoch: 26 Batch: 200 Log-loss: 0.043954864144325256\n",
      "Epoch: 26 Batch: 240 Log-loss: 0.04968782141804695\n",
      "Epoch: 26 Batch: 280 Log-loss: 0.04779147729277611\n",
      "Epoch: 26 Batch: 320 Log-loss: 0.04404182359576225\n",
      "Epoch: 26 Batch: 360 Log-loss: 0.03052383102476597\n",
      "Epoch: 26 Batch: 400 Log-loss: 0.03867020830512047\n",
      "Epoch: 26 Batch: 440 Log-loss: 0.03180961683392525\n",
      "Epoch: 26 Batch: 480 Log-loss: 0.03347311168909073\n",
      "Epoch: 26 Batch: 520 Log-loss: 0.037900570780038834\n",
      "Epoch: 26 Batch: 560 Log-loss: 0.03612082451581955\n",
      "Epoch average log-loss: 0.03993874163632946\n",
      "In Epoch: 26, val_loss: 0.03968198446058943, best_val_loss: 0.039670934016694186, best_auc: 0.9905380881008642\n",
      "Epoch: 27 Batch: 0 Log-loss: 0.02696610800921917\n",
      "Epoch: 27 Batch: 40 Log-loss: 0.029634468257427216\n",
      "Epoch: 27 Batch: 80 Log-loss: 0.043884728103876114\n",
      "Epoch: 27 Batch: 120 Log-loss: 0.041575636714696884\n",
      "Epoch: 27 Batch: 160 Log-loss: 0.048102546483278275\n",
      "Epoch: 27 Batch: 200 Log-loss: 0.03718743473291397\n",
      "Epoch: 27 Batch: 240 Log-loss: 0.028836332261562347\n",
      "Epoch: 27 Batch: 280 Log-loss: 0.04079652205109596\n",
      "Epoch: 27 Batch: 320 Log-loss: 0.033051297068595886\n",
      "Epoch: 27 Batch: 360 Log-loss: 0.039837826043367386\n",
      "Epoch: 27 Batch: 400 Log-loss: 0.033770833164453506\n",
      "Epoch: 27 Batch: 440 Log-loss: 0.030185021460056305\n",
      "Epoch: 27 Batch: 480 Log-loss: 0.044520094990730286\n",
      "Epoch: 27 Batch: 520 Log-loss: 0.03741858899593353\n",
      "Epoch: 27 Batch: 560 Log-loss: 0.0564989410340786\n",
      "Epoch average log-loss: 0.039742474420927466\n",
      "In Epoch: 27, val_loss: 0.04014687060797423, best_val_loss: 0.039670934016694186, best_auc: 0.9905380881008642\n",
      "Epoch: 28 Batch: 0 Log-loss: 0.04592892900109291\n",
      "Epoch: 28 Batch: 40 Log-loss: 0.04071267694234848\n",
      "Epoch: 28 Batch: 80 Log-loss: 0.061746995896101\n",
      "Epoch: 28 Batch: 120 Log-loss: 0.033435020595788956\n",
      "Epoch: 28 Batch: 160 Log-loss: 0.038281895220279694\n",
      "Epoch: 28 Batch: 200 Log-loss: 0.02529146522283554\n",
      "Epoch: 28 Batch: 240 Log-loss: 0.03820965439081192\n",
      "Epoch: 28 Batch: 280 Log-loss: 0.043678607791662216\n",
      "Epoch: 28 Batch: 320 Log-loss: 0.028624078258872032\n",
      "Epoch: 28 Batch: 360 Log-loss: 0.027668075636029243\n",
      "Epoch: 28 Batch: 400 Log-loss: 0.029746584594249725\n",
      "Epoch: 28 Batch: 440 Log-loss: 0.05383488908410072\n",
      "Epoch: 28 Batch: 480 Log-loss: 0.04809556528925896\n",
      "Epoch: 28 Batch: 520 Log-loss: 0.03672271594405174\n",
      "Epoch: 28 Batch: 560 Log-loss: 0.033570438623428345\n",
      "Epoch average log-loss: 0.03960089440058385\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 28, val_loss: 0.03939359103841479, best_val_loss: 0.03939359103841479, best_auc: 0.9901002395479681\n",
      "Epoch: 29 Batch: 0 Log-loss: 0.05517149344086647\n",
      "Epoch: 29 Batch: 40 Log-loss: 0.03096209280192852\n",
      "Epoch: 29 Batch: 80 Log-loss: 0.04145451635122299\n",
      "Epoch: 29 Batch: 120 Log-loss: 0.035101693123579025\n",
      "Epoch: 29 Batch: 160 Log-loss: 0.023029888048768044\n",
      "Epoch: 29 Batch: 200 Log-loss: 0.035092901438474655\n",
      "Epoch: 29 Batch: 240 Log-loss: 0.036847494542598724\n",
      "Epoch: 29 Batch: 280 Log-loss: 0.04389941319823265\n",
      "Epoch: 29 Batch: 320 Log-loss: 0.0415889248251915\n",
      "Epoch: 29 Batch: 360 Log-loss: 0.04120957851409912\n",
      "Epoch: 29 Batch: 400 Log-loss: 0.04888419434428215\n",
      "Epoch: 29 Batch: 440 Log-loss: 0.035962242633104324\n",
      "Epoch: 29 Batch: 480 Log-loss: 0.05160136520862579\n",
      "Epoch: 29 Batch: 520 Log-loss: 0.042802583426237106\n",
      "Epoch: 29 Batch: 560 Log-loss: 0.03980594873428345\n",
      "Epoch average log-loss: 0.03960398809452142\n",
      "In Epoch: 29, val_loss: 0.03996097884207055, best_val_loss: 0.03939359103841479, best_auc: 0.9901002395479681\n",
      "Epoch: 30 Batch: 0 Log-loss: 0.03930024430155754\n",
      "Epoch: 30 Batch: 40 Log-loss: 0.04100558161735535\n",
      "Epoch: 30 Batch: 80 Log-loss: 0.03996748849749565\n",
      "Epoch: 30 Batch: 120 Log-loss: 0.045468781143426895\n",
      "Epoch: 30 Batch: 160 Log-loss: 0.046565428376197815\n",
      "Epoch: 30 Batch: 200 Log-loss: 0.026929249987006187\n",
      "Epoch: 30 Batch: 240 Log-loss: 0.057571958750486374\n",
      "Epoch: 30 Batch: 280 Log-loss: 0.031175995245575905\n",
      "Epoch: 30 Batch: 320 Log-loss: 0.03968914970755577\n",
      "Epoch: 30 Batch: 360 Log-loss: 0.048203807324171066\n",
      "Epoch: 30 Batch: 400 Log-loss: 0.04248054698109627\n",
      "Epoch: 30 Batch: 440 Log-loss: 0.041679125279188156\n",
      "Epoch: 30 Batch: 480 Log-loss: 0.04241032898426056\n",
      "Epoch: 30 Batch: 520 Log-loss: 0.03337807208299637\n",
      "Epoch: 30 Batch: 560 Log-loss: 0.02956688590347767\n",
      "Epoch average log-loss: 0.03953352970004614\n",
      "In Epoch: 30, val_loss: 0.03974420116908293, best_val_loss: 0.03939359103841479, best_auc: 0.9901002395479681\n",
      "Epoch: 31 Batch: 0 Log-loss: 0.03714120015501976\n",
      "Epoch: 31 Batch: 40 Log-loss: 0.0516330748796463\n",
      "Epoch: 31 Batch: 80 Log-loss: 0.04067167267203331\n",
      "Epoch: 31 Batch: 120 Log-loss: 0.043622177094221115\n",
      "Epoch: 31 Batch: 160 Log-loss: 0.03539580479264259\n",
      "Epoch: 31 Batch: 200 Log-loss: 0.03740428015589714\n",
      "Epoch: 31 Batch: 240 Log-loss: 0.049199413508176804\n",
      "Epoch: 31 Batch: 280 Log-loss: 0.04925532639026642\n",
      "Epoch: 31 Batch: 320 Log-loss: 0.030127016827464104\n",
      "Epoch: 31 Batch: 360 Log-loss: 0.04286341369152069\n",
      "Epoch: 31 Batch: 400 Log-loss: 0.04421278461813927\n",
      "Epoch: 31 Batch: 440 Log-loss: 0.04234302043914795\n",
      "Epoch: 31 Batch: 480 Log-loss: 0.04267168417572975\n",
      "Epoch: 31 Batch: 520 Log-loss: 0.05700882896780968\n",
      "Epoch: 31 Batch: 560 Log-loss: 0.04129349812865257\n",
      "Epoch average log-loss: 0.039420798434210674\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 31, val_loss: 0.039314097107352335, best_val_loss: 0.039314097107352335, best_auc: 0.9902619699721719\n",
      "Epoch: 32 Batch: 0 Log-loss: 0.038452405482530594\n",
      "Epoch: 32 Batch: 40 Log-loss: 0.026767348870635033\n",
      "Epoch: 32 Batch: 80 Log-loss: 0.03136846795678139\n",
      "Epoch: 32 Batch: 120 Log-loss: 0.03362179547548294\n",
      "Epoch: 32 Batch: 160 Log-loss: 0.06430340558290482\n",
      "Epoch: 32 Batch: 200 Log-loss: 0.046182479709386826\n",
      "Epoch: 32 Batch: 240 Log-loss: 0.03723469004034996\n",
      "Epoch: 32 Batch: 280 Log-loss: 0.04259766638278961\n",
      "Epoch: 32 Batch: 320 Log-loss: 0.051764313131570816\n",
      "Epoch: 32 Batch: 360 Log-loss: 0.04026389122009277\n",
      "Epoch: 32 Batch: 400 Log-loss: 0.05042951926589012\n",
      "Epoch: 32 Batch: 440 Log-loss: 0.051550280302762985\n",
      "Epoch: 32 Batch: 480 Log-loss: 0.039916928857564926\n",
      "Epoch: 32 Batch: 520 Log-loss: 0.039599720388650894\n",
      "Epoch: 32 Batch: 560 Log-loss: 0.06249593570828438\n",
      "Epoch average log-loss: 0.03942982912994921\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 32, val_loss: 0.03920818030979369, best_val_loss: 0.03920818030979369, best_auc: 0.9903700016365362\n",
      "Epoch: 33 Batch: 0 Log-loss: 0.047099530696868896\n",
      "Epoch: 33 Batch: 40 Log-loss: 0.04378368332982063\n",
      "Epoch: 33 Batch: 80 Log-loss: 0.041705112904310226\n",
      "Epoch: 33 Batch: 120 Log-loss: 0.04789561405777931\n",
      "Epoch: 33 Batch: 160 Log-loss: 0.04755302146077156\n",
      "Epoch: 33 Batch: 200 Log-loss: 0.027813466265797615\n",
      "Epoch: 33 Batch: 240 Log-loss: 0.02120407484471798\n",
      "Epoch: 33 Batch: 280 Log-loss: 0.04299123212695122\n",
      "Epoch: 33 Batch: 320 Log-loss: 0.030265450477600098\n",
      "Epoch: 33 Batch: 360 Log-loss: 0.04534827545285225\n",
      "Epoch: 33 Batch: 400 Log-loss: 0.03381993994116783\n",
      "Epoch: 33 Batch: 440 Log-loss: 0.04687829315662384\n",
      "Epoch: 33 Batch: 480 Log-loss: 0.03532629832625389\n",
      "Epoch: 33 Batch: 520 Log-loss: 0.02975996397435665\n",
      "Epoch: 33 Batch: 560 Log-loss: 0.034287229180336\n",
      "Epoch average log-loss: 0.03959810904759382\n",
      "In Epoch: 33, val_loss: 0.0393362059420685, best_val_loss: 0.03920818030979369, best_auc: 0.9903700016365362\n",
      "Epoch: 34 Batch: 0 Log-loss: 0.042276591062545776\n",
      "Epoch: 34 Batch: 40 Log-loss: 0.04229545220732689\n",
      "Epoch: 34 Batch: 80 Log-loss: 0.05782413110136986\n",
      "Epoch: 34 Batch: 120 Log-loss: 0.035321194678545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 Batch: 160 Log-loss: 0.03244984522461891\n",
      "Epoch: 34 Batch: 200 Log-loss: 0.05142946541309357\n",
      "Epoch: 34 Batch: 240 Log-loss: 0.035950917750597\n",
      "Epoch: 34 Batch: 280 Log-loss: 0.025924669578671455\n",
      "Epoch: 34 Batch: 320 Log-loss: 0.04563179612159729\n",
      "Epoch: 34 Batch: 360 Log-loss: 0.040022678673267365\n",
      "Epoch: 34 Batch: 400 Log-loss: 0.04930157959461212\n",
      "Epoch: 34 Batch: 440 Log-loss: 0.040117260068655014\n",
      "Epoch: 34 Batch: 480 Log-loss: 0.025376876816153526\n",
      "Epoch: 34 Batch: 520 Log-loss: 0.03224528953433037\n",
      "Epoch: 34 Batch: 560 Log-loss: 0.04670834541320801\n",
      "Epoch average log-loss: 0.0393310227630926\n",
      "In Epoch: 34, val_loss: 0.039422999766547666, best_val_loss: 0.03920818030979369, best_auc: 0.9903700016365362\n",
      "Epoch: 35 Batch: 0 Log-loss: 0.040138646960258484\n",
      "Epoch: 35 Batch: 40 Log-loss: 0.03879069164395332\n",
      "Epoch: 35 Batch: 80 Log-loss: 0.03312084451317787\n",
      "Epoch: 35 Batch: 120 Log-loss: 0.027538934722542763\n",
      "Epoch: 35 Batch: 160 Log-loss: 0.04145190492272377\n",
      "Epoch: 35 Batch: 200 Log-loss: 0.03965877369046211\n",
      "Epoch: 35 Batch: 240 Log-loss: 0.029674315825104713\n",
      "Epoch: 35 Batch: 280 Log-loss: 0.03982337564229965\n",
      "Epoch: 35 Batch: 320 Log-loss: 0.03825228288769722\n",
      "Epoch: 35 Batch: 360 Log-loss: 0.03636396303772926\n",
      "Epoch: 35 Batch: 400 Log-loss: 0.03177155181765556\n",
      "Epoch: 35 Batch: 440 Log-loss: 0.04566746950149536\n",
      "Epoch: 35 Batch: 480 Log-loss: 0.03975779190659523\n",
      "Epoch: 35 Batch: 520 Log-loss: 0.03746689110994339\n",
      "Epoch: 35 Batch: 560 Log-loss: 0.035273972898721695\n",
      "Epoch average log-loss: 0.03925103221554309\n",
      "In Epoch: 35, val_loss: 0.03960068625726965, best_val_loss: 0.03920818030979369, best_auc: 0.9903700016365362\n",
      "Epoch: 36 Batch: 0 Log-loss: 0.034413039684295654\n",
      "Epoch: 36 Batch: 40 Log-loss: 0.043478626757860184\n",
      "Epoch: 36 Batch: 80 Log-loss: 0.030858097597956657\n",
      "Epoch: 36 Batch: 120 Log-loss: 0.045990120619535446\n",
      "Epoch: 36 Batch: 160 Log-loss: 0.038185108453035355\n",
      "Epoch: 36 Batch: 200 Log-loss: 0.04787145182490349\n",
      "Epoch: 36 Batch: 240 Log-loss: 0.040492571890354156\n",
      "Epoch: 36 Batch: 280 Log-loss: 0.036348599940538406\n",
      "Epoch: 36 Batch: 320 Log-loss: 0.03131696581840515\n",
      "Epoch: 36 Batch: 360 Log-loss: 0.044600505381822586\n",
      "Epoch: 36 Batch: 400 Log-loss: 0.0359039343893528\n",
      "Epoch: 36 Batch: 440 Log-loss: 0.050034184008836746\n",
      "Epoch: 36 Batch: 480 Log-loss: 0.03028532862663269\n",
      "Epoch: 36 Batch: 520 Log-loss: 0.039622027426958084\n",
      "Epoch: 36 Batch: 560 Log-loss: 0.0429118350148201\n",
      "Epoch average log-loss: 0.039462557963893884\n",
      "In Epoch: 36, val_loss: 0.03935692358687635, best_val_loss: 0.03920818030979369, best_auc: 0.9903700016365362\n",
      "Epoch: 37 Batch: 0 Log-loss: 0.03644772246479988\n",
      "Epoch: 37 Batch: 40 Log-loss: 0.03006930835545063\n",
      "Epoch: 37 Batch: 80 Log-loss: 0.051837921142578125\n",
      "Epoch: 37 Batch: 120 Log-loss: 0.036832209676504135\n",
      "Epoch: 37 Batch: 160 Log-loss: 0.035896290093660355\n",
      "Epoch: 37 Batch: 200 Log-loss: 0.027784598991274834\n",
      "Epoch: 37 Batch: 240 Log-loss: 0.029656121507287025\n",
      "Epoch: 37 Batch: 280 Log-loss: 0.04318193718791008\n",
      "Epoch: 37 Batch: 320 Log-loss: 0.03227473795413971\n",
      "Epoch: 37 Batch: 360 Log-loss: 0.043298397213220596\n",
      "Epoch: 37 Batch: 400 Log-loss: 0.03451107069849968\n",
      "Epoch: 37 Batch: 440 Log-loss: 0.05472883954644203\n",
      "Epoch: 37 Batch: 480 Log-loss: 0.031168177723884583\n",
      "Epoch: 37 Batch: 520 Log-loss: 0.04645495116710663\n",
      "Epoch: 37 Batch: 560 Log-loss: 0.02536001242697239\n",
      "Epoch average log-loss: 0.039338525800433545\n",
      "In Epoch: 37, val_loss: 0.03930316571284604, best_val_loss: 0.03920818030979369, best_auc: 0.9903700016365362\n",
      "Epoch: 38 Batch: 0 Log-loss: 0.04871993884444237\n",
      "Epoch: 38 Batch: 40 Log-loss: 0.03865140303969383\n",
      "Epoch: 38 Batch: 80 Log-loss: 0.050528015941381454\n",
      "Epoch: 38 Batch: 120 Log-loss: 0.04307609796524048\n",
      "Epoch: 38 Batch: 160 Log-loss: 0.02821686863899231\n",
      "Epoch: 38 Batch: 200 Log-loss: 0.03293613716959953\n",
      "Epoch: 38 Batch: 240 Log-loss: 0.04346360266208649\n",
      "Epoch: 38 Batch: 280 Log-loss: 0.03126334026455879\n",
      "Epoch: 38 Batch: 320 Log-loss: 0.04875178262591362\n",
      "Epoch: 38 Batch: 360 Log-loss: 0.030793437734246254\n",
      "Epoch: 38 Batch: 400 Log-loss: 0.03636426106095314\n",
      "Epoch: 38 Batch: 440 Log-loss: 0.03389137610793114\n",
      "Epoch: 38 Batch: 480 Log-loss: 0.04448501393198967\n",
      "Epoch: 38 Batch: 520 Log-loss: 0.049231406301259995\n",
      "Epoch: 38 Batch: 560 Log-loss: 0.03717246651649475\n",
      "Epoch average log-loss: 0.03917936424259096\n",
      "In Epoch: 38, val_loss: 0.039553415891554665, best_val_loss: 0.03920818030979369, best_auc: 0.9903700016365362\n",
      "Epoch: 39 Batch: 0 Log-loss: 0.04527568817138672\n",
      "Epoch: 39 Batch: 40 Log-loss: 0.032506123185157776\n",
      "Epoch: 39 Batch: 80 Log-loss: 0.03984858840703964\n",
      "Epoch: 39 Batch: 120 Log-loss: 0.03767002373933792\n",
      "Epoch: 39 Batch: 160 Log-loss: 0.025487257167696953\n",
      "Epoch: 39 Batch: 200 Log-loss: 0.03682301566004753\n",
      "Epoch: 39 Batch: 240 Log-loss: 0.03731616213917732\n",
      "Epoch: 39 Batch: 280 Log-loss: 0.04137398302555084\n",
      "Epoch: 39 Batch: 320 Log-loss: 0.06341718882322311\n",
      "Epoch: 39 Batch: 360 Log-loss: 0.04113566875457764\n",
      "Epoch: 39 Batch: 400 Log-loss: 0.04082704335451126\n",
      "Epoch: 39 Batch: 440 Log-loss: 0.03349726274609566\n",
      "Epoch: 39 Batch: 480 Log-loss: 0.04245106875896454\n",
      "Epoch: 39 Batch: 520 Log-loss: 0.046071842312812805\n",
      "Epoch: 39 Batch: 560 Log-loss: 0.027840349823236465\n",
      "Epoch average log-loss: 0.038952827626573185\n",
      "In Epoch: 39, val_loss: 0.0397491058747876, best_val_loss: 0.03920818030979369, best_auc: 0.9903700016365362\n",
      "Epoch: 40 Batch: 0 Log-loss: 0.041555438190698624\n",
      "Epoch: 40 Batch: 40 Log-loss: 0.044186875224113464\n",
      "Epoch: 40 Batch: 80 Log-loss: 0.03280242532491684\n",
      "Epoch: 40 Batch: 120 Log-loss: 0.014010065235197544\n",
      "Epoch: 40 Batch: 160 Log-loss: 0.04579287767410278\n",
      "Epoch: 40 Batch: 200 Log-loss: 0.03690681979060173\n",
      "Epoch: 40 Batch: 240 Log-loss: 0.031742945313453674\n",
      "Epoch: 40 Batch: 280 Log-loss: 0.031207149848341942\n",
      "Epoch: 40 Batch: 320 Log-loss: 0.047189366072416306\n",
      "Epoch: 40 Batch: 360 Log-loss: 0.034204188734292984\n",
      "Epoch: 40 Batch: 400 Log-loss: 0.03757164254784584\n",
      "Epoch: 40 Batch: 440 Log-loss: 0.023607129231095314\n",
      "Epoch: 40 Batch: 480 Log-loss: 0.02062860131263733\n",
      "Epoch: 40 Batch: 520 Log-loss: 0.047733988612890244\n",
      "Epoch: 40 Batch: 560 Log-loss: 0.04284060373902321\n",
      "Epoch average log-loss: 0.03911254310847393\n",
      "In Epoch: 40, val_loss: 0.03948822554082546, best_val_loss: 0.03920818030979369, best_auc: 0.9903700016365362\n",
      "Epoch: 41 Batch: 0 Log-loss: 0.033652473241090775\n",
      "Epoch: 41 Batch: 40 Log-loss: 0.04259708151221275\n",
      "Epoch: 41 Batch: 80 Log-loss: 0.04038362577557564\n",
      "Epoch: 41 Batch: 120 Log-loss: 0.03306298702955246\n",
      "Epoch: 41 Batch: 160 Log-loss: 0.02759886346757412\n",
      "Epoch: 41 Batch: 200 Log-loss: 0.030536361038684845\n",
      "Epoch: 41 Batch: 240 Log-loss: 0.043752286583185196\n",
      "Epoch: 41 Batch: 280 Log-loss: 0.05948257818818092\n",
      "Epoch: 41 Batch: 320 Log-loss: 0.03921284154057503\n",
      "Epoch: 41 Batch: 360 Log-loss: 0.03850683569908142\n",
      "Epoch: 41 Batch: 400 Log-loss: 0.03649425506591797\n",
      "Epoch: 41 Batch: 440 Log-loss: 0.029979480430483818\n",
      "Epoch: 41 Batch: 480 Log-loss: 0.029293859377503395\n",
      "Epoch: 41 Batch: 520 Log-loss: 0.05534279718995094\n",
      "Epoch: 41 Batch: 560 Log-loss: 0.05971820279955864\n",
      "Epoch average log-loss: 0.039139135966875724\n",
      "In Epoch: 41, val_loss: 0.039568680213661715, best_val_loss: 0.03920818030979369, best_auc: 0.9903700016365362\n",
      "Epoch: 42 Batch: 0 Log-loss: 0.03970709815621376\n",
      "Epoch: 42 Batch: 40 Log-loss: 0.03999932482838631\n",
      "Epoch: 42 Batch: 80 Log-loss: 0.03532267361879349\n",
      "Epoch: 42 Batch: 120 Log-loss: 0.0495183952152729\n",
      "Epoch: 42 Batch: 160 Log-loss: 0.044174131006002426\n",
      "Epoch: 42 Batch: 200 Log-loss: 0.04921536520123482\n",
      "Epoch: 42 Batch: 240 Log-loss: 0.056801993399858475\n",
      "Epoch: 42 Batch: 280 Log-loss: 0.04276813194155693\n",
      "Epoch: 42 Batch: 320 Log-loss: 0.038030970841646194\n",
      "Epoch: 42 Batch: 360 Log-loss: 0.04657121002674103\n",
      "Epoch: 42 Batch: 400 Log-loss: 0.04121207445859909\n",
      "Epoch: 42 Batch: 440 Log-loss: 0.030402271077036858\n",
      "Epoch: 42 Batch: 480 Log-loss: 0.05237177386879921\n",
      "Epoch: 42 Batch: 520 Log-loss: 0.03397754207253456\n",
      "Epoch: 42 Batch: 560 Log-loss: 0.04116498678922653\n",
      "Epoch average log-loss: 0.03903187553265265\n",
      "In Epoch: 42, val_loss: 0.03935132785953891, best_val_loss: 0.03920818030979369, best_auc: 0.9903700016365362\n",
      "Epoch: 43 Batch: 0 Log-loss: 0.0362105555832386\n",
      "Epoch: 43 Batch: 40 Log-loss: 0.048941727727651596\n",
      "Epoch: 43 Batch: 80 Log-loss: 0.03439510986208916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 Batch: 120 Log-loss: 0.04365473613142967\n",
      "Epoch: 43 Batch: 160 Log-loss: 0.04615509882569313\n",
      "Epoch: 43 Batch: 200 Log-loss: 0.04120107367634773\n",
      "Epoch: 43 Batch: 240 Log-loss: 0.030953818932175636\n",
      "Epoch: 43 Batch: 280 Log-loss: 0.06346815079450607\n",
      "Epoch: 43 Batch: 320 Log-loss: 0.048835840076208115\n",
      "Epoch: 43 Batch: 360 Log-loss: 0.03193243220448494\n",
      "Epoch: 43 Batch: 400 Log-loss: 0.04063098505139351\n",
      "Epoch: 43 Batch: 440 Log-loss: 0.029473720118403435\n",
      "Epoch: 43 Batch: 480 Log-loss: 0.049528565257787704\n",
      "Epoch: 43 Batch: 520 Log-loss: 0.0453464575111866\n",
      "Epoch: 43 Batch: 560 Log-loss: 0.032125066965818405\n",
      "Epoch average log-loss: 0.039208267106940704\n",
      "In Epoch: 43, val_loss: 0.03941661095882591, best_val_loss: 0.03920818030979369, best_auc: 0.9903700016365362\n",
      "Epoch: 44 Batch: 0 Log-loss: 0.03996959701180458\n",
      "Epoch: 44 Batch: 40 Log-loss: 0.04313095286488533\n",
      "Epoch: 44 Batch: 80 Log-loss: 0.03195556253194809\n",
      "Epoch: 44 Batch: 120 Log-loss: 0.03052748180925846\n",
      "Epoch: 44 Batch: 160 Log-loss: 0.045339811593294144\n",
      "Epoch: 44 Batch: 200 Log-loss: 0.027499467134475708\n",
      "Epoch: 44 Batch: 240 Log-loss: 0.037184376269578934\n",
      "Epoch: 44 Batch: 280 Log-loss: 0.04177095368504524\n",
      "Epoch: 44 Batch: 320 Log-loss: 0.053108204156160355\n",
      "Epoch: 44 Batch: 360 Log-loss: 0.03701931983232498\n",
      "Epoch: 44 Batch: 400 Log-loss: 0.03532565012574196\n",
      "Epoch: 44 Batch: 440 Log-loss: 0.046047333627939224\n",
      "Epoch: 44 Batch: 480 Log-loss: 0.026418626308441162\n",
      "Epoch: 44 Batch: 520 Log-loss: 0.04713396728038788\n",
      "Epoch: 44 Batch: 560 Log-loss: 0.044013261795043945\n",
      "Epoch average log-loss: 0.03890702994207718\n",
      "A pre-trained model at model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt has been loaded.\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn2.pt.\n",
      "\n",
      "Choose the torch base model.\n",
      "## Training on fold 3 ##\n",
      "Epoch: 1 Batch: 0 Log-loss: 0.6983099579811096\n",
      "Epoch: 1 Batch: 40 Log-loss: 0.17204342782497406\n",
      "Epoch: 1 Batch: 80 Log-loss: 0.1454945057630539\n",
      "Epoch: 1 Batch: 120 Log-loss: 0.08052521198987961\n",
      "Epoch: 1 Batch: 160 Log-loss: 0.07817576080560684\n",
      "Epoch: 1 Batch: 200 Log-loss: 0.050840090960264206\n",
      "Epoch: 1 Batch: 240 Log-loss: 0.05482248589396477\n",
      "Epoch: 1 Batch: 280 Log-loss: 0.052172038704156876\n",
      "Epoch: 1 Batch: 320 Log-loss: 0.05744989588856697\n",
      "Epoch: 1 Batch: 360 Log-loss: 0.048596352338790894\n",
      "Epoch: 1 Batch: 400 Log-loss: 0.046108171343803406\n",
      "Epoch: 1 Batch: 440 Log-loss: 0.04518967866897583\n",
      "Epoch: 1 Batch: 480 Log-loss: 0.04061942175030708\n",
      "Epoch: 1 Batch: 520 Log-loss: 0.05551822856068611\n",
      "Epoch: 1 Batch: 560 Log-loss: 0.05840064585208893\n",
      "Epoch average log-loss: 0.08708709195987986\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 1, val_loss: 0.05305690045101791, best_val_loss: 0.05305690045101791, best_auc: 0.9682826602966211\n",
      "Epoch: 2 Batch: 0 Log-loss: 0.055501025170087814\n",
      "Epoch: 2 Batch: 40 Log-loss: 0.05299539491534233\n",
      "Epoch: 2 Batch: 80 Log-loss: 0.058472152799367905\n",
      "Epoch: 2 Batch: 120 Log-loss: 0.055731307715177536\n",
      "Epoch: 2 Batch: 160 Log-loss: 0.02478167600929737\n",
      "Epoch: 2 Batch: 200 Log-loss: 0.031327132135629654\n",
      "Epoch: 2 Batch: 240 Log-loss: 0.048457760363817215\n",
      "Epoch: 2 Batch: 280 Log-loss: 0.053926512598991394\n",
      "Epoch: 2 Batch: 320 Log-loss: 0.055483683943748474\n",
      "Epoch: 2 Batch: 360 Log-loss: 0.07092676311731339\n",
      "Epoch: 2 Batch: 400 Log-loss: 0.04595431312918663\n",
      "Epoch: 2 Batch: 440 Log-loss: 0.046154607087373734\n",
      "Epoch: 2 Batch: 480 Log-loss: 0.04984685778617859\n",
      "Epoch: 2 Batch: 520 Log-loss: 0.06965436786413193\n",
      "Epoch: 2 Batch: 560 Log-loss: 0.03975877910852432\n",
      "Epoch average log-loss: 0.05186568841870342\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 2, val_loss: 0.049145947810306546, best_val_loss: 0.049145947810306546, best_auc: 0.9742659685872535\n",
      "Epoch: 3 Batch: 0 Log-loss: 0.04115058481693268\n",
      "Epoch: 3 Batch: 40 Log-loss: 0.04413663223385811\n",
      "Epoch: 3 Batch: 80 Log-loss: 0.031809549778699875\n",
      "Epoch: 3 Batch: 120 Log-loss: 0.0652673989534378\n",
      "Epoch: 3 Batch: 160 Log-loss: 0.05076400935649872\n",
      "Epoch: 3 Batch: 200 Log-loss: 0.02913006581366062\n",
      "Epoch: 3 Batch: 240 Log-loss: 0.04935368895530701\n",
      "Epoch: 3 Batch: 280 Log-loss: 0.04993794858455658\n",
      "Epoch: 3 Batch: 320 Log-loss: 0.06069747731089592\n",
      "Epoch: 3 Batch: 360 Log-loss: 0.0432400144636631\n",
      "Epoch: 3 Batch: 400 Log-loss: 0.03820676729083061\n",
      "Epoch: 3 Batch: 440 Log-loss: 0.04896165058016777\n",
      "Epoch: 3 Batch: 480 Log-loss: 0.05133722722530365\n",
      "Epoch: 3 Batch: 520 Log-loss: 0.06113222613930702\n",
      "Epoch: 3 Batch: 560 Log-loss: 0.06341295689344406\n",
      "Epoch average log-loss: 0.04876218110189906\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 3, val_loss: 0.04702130942410279, best_val_loss: 0.04702130942410279, best_auc: 0.9789428637916774\n",
      "Epoch: 4 Batch: 0 Log-loss: 0.045047204941511154\n",
      "Epoch: 4 Batch: 40 Log-loss: 0.047845084220170975\n",
      "Epoch: 4 Batch: 80 Log-loss: 0.06370378285646439\n",
      "Epoch: 4 Batch: 120 Log-loss: 0.02934049256145954\n",
      "Epoch: 4 Batch: 160 Log-loss: 0.0390402190387249\n",
      "Epoch: 4 Batch: 200 Log-loss: 0.05328211188316345\n",
      "Epoch: 4 Batch: 240 Log-loss: 0.04303598403930664\n",
      "Epoch: 4 Batch: 280 Log-loss: 0.04027305170893669\n",
      "Epoch: 4 Batch: 320 Log-loss: 0.05006645992398262\n",
      "Epoch: 4 Batch: 360 Log-loss: 0.06566040217876434\n",
      "Epoch: 4 Batch: 400 Log-loss: 0.03468763455748558\n",
      "Epoch: 4 Batch: 440 Log-loss: 0.05421597883105278\n",
      "Epoch: 4 Batch: 480 Log-loss: 0.05071060732007027\n",
      "Epoch: 4 Batch: 520 Log-loss: 0.08389678597450256\n",
      "Epoch: 4 Batch: 560 Log-loss: 0.048302922397851944\n",
      "Epoch average log-loss: 0.04664345140502389\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 4, val_loss: 0.046464546270857394, best_val_loss: 0.046464546270857394, best_auc: 0.9795230499124227\n",
      "Epoch: 5 Batch: 0 Log-loss: 0.06390628963708878\n",
      "Epoch: 5 Batch: 40 Log-loss: 0.053778667002916336\n",
      "Epoch: 5 Batch: 80 Log-loss: 0.04071157053112984\n",
      "Epoch: 5 Batch: 120 Log-loss: 0.049647729843854904\n",
      "Epoch: 5 Batch: 160 Log-loss: 0.04130474478006363\n",
      "Epoch: 5 Batch: 200 Log-loss: 0.07099715620279312\n",
      "Epoch: 5 Batch: 240 Log-loss: 0.03639756515622139\n",
      "Epoch: 5 Batch: 280 Log-loss: 0.036911774426698685\n",
      "Epoch: 5 Batch: 320 Log-loss: 0.06838641315698624\n",
      "Epoch: 5 Batch: 360 Log-loss: 0.04864529147744179\n",
      "Epoch: 5 Batch: 400 Log-loss: 0.061478063464164734\n",
      "Epoch: 5 Batch: 440 Log-loss: 0.059510279446840286\n",
      "Epoch: 5 Batch: 480 Log-loss: 0.03357831761240959\n",
      "Epoch: 5 Batch: 520 Log-loss: 0.037929732352495193\n",
      "Epoch: 5 Batch: 560 Log-loss: 0.04077021777629852\n",
      "Epoch average log-loss: 0.04558239057660103\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 5, val_loss: 0.0453462917515737, best_val_loss: 0.0453462917515737, best_auc: 0.9825678652974306\n",
      "Epoch: 6 Batch: 0 Log-loss: 0.047252852469682693\n",
      "Epoch: 6 Batch: 40 Log-loss: 0.040510836988687515\n",
      "Epoch: 6 Batch: 80 Log-loss: 0.06448280066251755\n",
      "Epoch: 6 Batch: 120 Log-loss: 0.04004599526524544\n",
      "Epoch: 6 Batch: 160 Log-loss: 0.0483558364212513\n",
      "Epoch: 6 Batch: 200 Log-loss: 0.052329014986753464\n",
      "Epoch: 6 Batch: 240 Log-loss: 0.04367924854159355\n",
      "Epoch: 6 Batch: 280 Log-loss: 0.04609294608235359\n",
      "Epoch: 6 Batch: 320 Log-loss: 0.05419555678963661\n",
      "Epoch: 6 Batch: 360 Log-loss: 0.04183993116021156\n",
      "Epoch: 6 Batch: 400 Log-loss: 0.044183339923620224\n",
      "Epoch: 6 Batch: 440 Log-loss: 0.05748744681477547\n",
      "Epoch: 6 Batch: 480 Log-loss: 0.047147732228040695\n",
      "Epoch: 6 Batch: 520 Log-loss: 0.04114921763539314\n",
      "Epoch: 6 Batch: 560 Log-loss: 0.05082189664244652\n",
      "Epoch average log-loss: 0.04461311647069773\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 6, val_loss: 0.04485352604127909, best_val_loss: 0.04485352604127909, best_auc: 0.9828228842134784\n",
      "Epoch: 7 Batch: 0 Log-loss: 0.04518246650695801\n",
      "Epoch: 7 Batch: 40 Log-loss: 0.0326644703745842\n",
      "Epoch: 7 Batch: 80 Log-loss: 0.04685254395008087\n",
      "Epoch: 7 Batch: 120 Log-loss: 0.05262336507439613\n",
      "Epoch: 7 Batch: 160 Log-loss: 0.04149935021996498\n",
      "Epoch: 7 Batch: 200 Log-loss: 0.043841633945703506\n",
      "Epoch: 7 Batch: 240 Log-loss: 0.05765293538570404\n",
      "Epoch: 7 Batch: 280 Log-loss: 0.07507985085248947\n",
      "Epoch: 7 Batch: 320 Log-loss: 0.05133362114429474\n",
      "Epoch: 7 Batch: 360 Log-loss: 0.049361731857061386\n",
      "Epoch: 7 Batch: 400 Log-loss: 0.045540764927864075\n",
      "Epoch: 7 Batch: 440 Log-loss: 0.04707251489162445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Batch: 480 Log-loss: 0.04782577231526375\n",
      "Epoch: 7 Batch: 520 Log-loss: 0.04095962643623352\n",
      "Epoch: 7 Batch: 560 Log-loss: 0.04207990691065788\n",
      "Epoch average log-loss: 0.043992608263423404\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 7, val_loss: 0.04415017307607541, best_val_loss: 0.04415017307607541, best_auc: 0.984707366748618\n",
      "Epoch: 8 Batch: 0 Log-loss: 0.06348206847906113\n",
      "Epoch: 8 Batch: 40 Log-loss: 0.044476237148046494\n",
      "Epoch: 8 Batch: 80 Log-loss: 0.03903960809111595\n",
      "Epoch: 8 Batch: 120 Log-loss: 0.02411440946161747\n",
      "Epoch: 8 Batch: 160 Log-loss: 0.04496742784976959\n",
      "Epoch: 8 Batch: 200 Log-loss: 0.04574362561106682\n",
      "Epoch: 8 Batch: 240 Log-loss: 0.03364004194736481\n",
      "Epoch: 8 Batch: 280 Log-loss: 0.051981013268232346\n",
      "Epoch: 8 Batch: 320 Log-loss: 0.04269278049468994\n",
      "Epoch: 8 Batch: 360 Log-loss: 0.041359927505254745\n",
      "Epoch: 8 Batch: 400 Log-loss: 0.06058087572455406\n",
      "Epoch: 8 Batch: 440 Log-loss: 0.03174297884106636\n",
      "Epoch: 8 Batch: 480 Log-loss: 0.036446329206228256\n",
      "Epoch: 8 Batch: 520 Log-loss: 0.03252490237355232\n",
      "Epoch: 8 Batch: 560 Log-loss: 0.052997659891843796\n",
      "Epoch average log-loss: 0.04352977511339954\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 8, val_loss: 0.04355558036012038, best_val_loss: 0.04355558036012038, best_auc: 0.9857270397224687\n",
      "Epoch: 9 Batch: 0 Log-loss: 0.05375795438885689\n",
      "Epoch: 9 Batch: 40 Log-loss: 0.03566376119852066\n",
      "Epoch: 9 Batch: 80 Log-loss: 0.022528255358338356\n",
      "Epoch: 9 Batch: 120 Log-loss: 0.04966553673148155\n",
      "Epoch: 9 Batch: 160 Log-loss: 0.04052312299609184\n",
      "Epoch: 9 Batch: 200 Log-loss: 0.043721675872802734\n",
      "Epoch: 9 Batch: 240 Log-loss: 0.033193450421094894\n",
      "Epoch: 9 Batch: 280 Log-loss: 0.03570761904120445\n",
      "Epoch: 9 Batch: 320 Log-loss: 0.05424141883850098\n",
      "Epoch: 9 Batch: 360 Log-loss: 0.061622265726327896\n",
      "Epoch: 9 Batch: 400 Log-loss: 0.05305905640125275\n",
      "Epoch: 9 Batch: 440 Log-loss: 0.044677019119262695\n",
      "Epoch: 9 Batch: 480 Log-loss: 0.020708320662379265\n",
      "Epoch: 9 Batch: 520 Log-loss: 0.03081950731575489\n",
      "Epoch: 9 Batch: 560 Log-loss: 0.04477662965655327\n",
      "Epoch average log-loss: 0.04271601612812707\n",
      "In Epoch: 9, val_loss: 0.043571694932457224, best_val_loss: 0.04355558036012038, best_auc: 0.9857270397224687\n",
      "Epoch: 10 Batch: 0 Log-loss: 0.03168674185872078\n",
      "Epoch: 10 Batch: 40 Log-loss: 0.030713146552443504\n",
      "Epoch: 10 Batch: 80 Log-loss: 0.05422669276595116\n",
      "Epoch: 10 Batch: 120 Log-loss: 0.0397786982357502\n",
      "Epoch: 10 Batch: 160 Log-loss: 0.043827515095472336\n",
      "Epoch: 10 Batch: 200 Log-loss: 0.03935283049941063\n",
      "Epoch: 10 Batch: 240 Log-loss: 0.03316338732838631\n",
      "Epoch: 10 Batch: 280 Log-loss: 0.057775821536779404\n",
      "Epoch: 10 Batch: 320 Log-loss: 0.036868926137685776\n",
      "Epoch: 10 Batch: 360 Log-loss: 0.04685946926474571\n",
      "Epoch: 10 Batch: 400 Log-loss: 0.033052001148462296\n",
      "Epoch: 10 Batch: 440 Log-loss: 0.056852977722883224\n",
      "Epoch: 10 Batch: 480 Log-loss: 0.04670096933841705\n",
      "Epoch: 10 Batch: 520 Log-loss: 0.042725514620542526\n",
      "Epoch: 10 Batch: 560 Log-loss: 0.04137592762708664\n",
      "Epoch average log-loss: 0.04260783994104713\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 10, val_loss: 0.04297100655721151, best_val_loss: 0.04297100655721151, best_auc: 0.9857403030158648\n",
      "Epoch: 11 Batch: 0 Log-loss: 0.037359584122896194\n",
      "Epoch: 11 Batch: 40 Log-loss: 0.04534626379609108\n",
      "Epoch: 11 Batch: 80 Log-loss: 0.03693611919879913\n",
      "Epoch: 11 Batch: 120 Log-loss: 0.04253527522087097\n",
      "Epoch: 11 Batch: 160 Log-loss: 0.04262469336390495\n",
      "Epoch: 11 Batch: 200 Log-loss: 0.03110262006521225\n",
      "Epoch: 11 Batch: 240 Log-loss: 0.05106956139206886\n",
      "Epoch: 11 Batch: 280 Log-loss: 0.040989864617586136\n",
      "Epoch: 11 Batch: 320 Log-loss: 0.03464069589972496\n",
      "Epoch: 11 Batch: 360 Log-loss: 0.039953965693712234\n",
      "Epoch: 11 Batch: 400 Log-loss: 0.04075874015688896\n",
      "Epoch: 11 Batch: 440 Log-loss: 0.054483622312545776\n",
      "Epoch: 11 Batch: 480 Log-loss: 0.03820641711354256\n",
      "Epoch: 11 Batch: 520 Log-loss: 0.04810221865773201\n",
      "Epoch: 11 Batch: 560 Log-loss: 0.0409805104136467\n",
      "Epoch average log-loss: 0.04223387380729297\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 11, val_loss: 0.042948304702340116, best_val_loss: 0.042948304702340116, best_auc: 0.9856160503225122\n",
      "Epoch: 12 Batch: 0 Log-loss: 0.052601125091314316\n",
      "Epoch: 12 Batch: 40 Log-loss: 0.03795639052987099\n",
      "Epoch: 12 Batch: 80 Log-loss: 0.05356067046523094\n",
      "Epoch: 12 Batch: 120 Log-loss: 0.03747095912694931\n",
      "Epoch: 12 Batch: 160 Log-loss: 0.041853975504636765\n",
      "Epoch: 12 Batch: 200 Log-loss: 0.03154349699616432\n",
      "Epoch: 12 Batch: 240 Log-loss: 0.054698292165994644\n",
      "Epoch: 12 Batch: 280 Log-loss: 0.03897985816001892\n",
      "Epoch: 12 Batch: 320 Log-loss: 0.05058446153998375\n",
      "Epoch: 12 Batch: 360 Log-loss: 0.037415776401758194\n",
      "Epoch: 12 Batch: 400 Log-loss: 0.03663713112473488\n",
      "Epoch: 12 Batch: 440 Log-loss: 0.029055101796984673\n",
      "Epoch: 12 Batch: 480 Log-loss: 0.03817050904035568\n",
      "Epoch: 12 Batch: 520 Log-loss: 0.03435582295060158\n",
      "Epoch: 12 Batch: 560 Log-loss: 0.05004345253109932\n",
      "Epoch average log-loss: 0.041838051611557604\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 12, val_loss: 0.042364304013881644, best_val_loss: 0.042364304013881644, best_auc: 0.9862390081906828\n",
      "Epoch: 13 Batch: 0 Log-loss: 0.03552580252289772\n",
      "Epoch: 13 Batch: 40 Log-loss: 0.03752527013421059\n",
      "Epoch: 13 Batch: 80 Log-loss: 0.04026111215353012\n",
      "Epoch: 13 Batch: 120 Log-loss: 0.029653659090399742\n",
      "Epoch: 13 Batch: 160 Log-loss: 0.03553520515561104\n",
      "Epoch: 13 Batch: 200 Log-loss: 0.04061855003237724\n",
      "Epoch: 13 Batch: 240 Log-loss: 0.050526004284620285\n",
      "Epoch: 13 Batch: 280 Log-loss: 0.03193741664290428\n",
      "Epoch: 13 Batch: 320 Log-loss: 0.02686634659767151\n",
      "Epoch: 13 Batch: 360 Log-loss: 0.04544985294342041\n",
      "Epoch: 13 Batch: 400 Log-loss: 0.05185350775718689\n",
      "Epoch: 13 Batch: 440 Log-loss: 0.04784635826945305\n",
      "Epoch: 13 Batch: 480 Log-loss: 0.04456937685608864\n",
      "Epoch: 13 Batch: 520 Log-loss: 0.03691776469349861\n",
      "Epoch: 13 Batch: 560 Log-loss: 0.029140980914235115\n",
      "Epoch average log-loss: 0.04131659126640963\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 13, val_loss: 0.0422868816796255, best_val_loss: 0.0422868816796255, best_auc: 0.9868429722666323\n",
      "Epoch: 14 Batch: 0 Log-loss: 0.040695156902074814\n",
      "Epoch: 14 Batch: 40 Log-loss: 0.024747146293520927\n",
      "Epoch: 14 Batch: 80 Log-loss: 0.03769415244460106\n",
      "Epoch: 14 Batch: 120 Log-loss: 0.034258220344781876\n",
      "Epoch: 14 Batch: 160 Log-loss: 0.036434393376111984\n",
      "Epoch: 14 Batch: 200 Log-loss: 0.06289941817522049\n",
      "Epoch: 14 Batch: 240 Log-loss: 0.03329633176326752\n",
      "Epoch: 14 Batch: 280 Log-loss: 0.046488407999277115\n",
      "Epoch: 14 Batch: 320 Log-loss: 0.029467856511473656\n",
      "Epoch: 14 Batch: 360 Log-loss: 0.043155282735824585\n",
      "Epoch: 14 Batch: 400 Log-loss: 0.03900241479277611\n",
      "Epoch: 14 Batch: 440 Log-loss: 0.05263134837150574\n",
      "Epoch: 14 Batch: 480 Log-loss: 0.0370575450360775\n",
      "Epoch: 14 Batch: 520 Log-loss: 0.04399396851658821\n",
      "Epoch: 14 Batch: 560 Log-loss: 0.04450687766075134\n",
      "Epoch average log-loss: 0.04118689155937838\n",
      "In Epoch: 14, val_loss: 0.042342328989682314, best_val_loss: 0.0422868816796255, best_auc: 0.9868429722666323\n",
      "Epoch: 15 Batch: 0 Log-loss: 0.03570451959967613\n",
      "Epoch: 15 Batch: 40 Log-loss: 0.046804849058389664\n",
      "Epoch: 15 Batch: 80 Log-loss: 0.03496365249156952\n",
      "Epoch: 15 Batch: 120 Log-loss: 0.05083649232983589\n",
      "Epoch: 15 Batch: 160 Log-loss: 0.04827257618308067\n",
      "Epoch: 15 Batch: 200 Log-loss: 0.03970834240317345\n",
      "Epoch: 15 Batch: 240 Log-loss: 0.03459244593977928\n",
      "Epoch: 15 Batch: 280 Log-loss: 0.03844117745757103\n",
      "Epoch: 15 Batch: 320 Log-loss: 0.04274521768093109\n",
      "Epoch: 15 Batch: 360 Log-loss: 0.034373391419649124\n",
      "Epoch: 15 Batch: 400 Log-loss: 0.03509568050503731\n",
      "Epoch: 15 Batch: 440 Log-loss: 0.024309733882546425\n",
      "Epoch: 15 Batch: 480 Log-loss: 0.057249877601861954\n",
      "Epoch: 15 Batch: 520 Log-loss: 0.03548198565840721\n",
      "Epoch: 15 Batch: 560 Log-loss: 0.032002031803131104\n",
      "Epoch average log-loss: 0.04078313128723364\n",
      "In Epoch: 15, val_loss: 0.04268635417675248, best_val_loss: 0.0422868816796255, best_auc: 0.9868429722666323\n",
      "Epoch: 16 Batch: 0 Log-loss: 0.040924351662397385\n",
      "Epoch: 16 Batch: 40 Log-loss: 0.0466526560485363\n",
      "Epoch: 16 Batch: 80 Log-loss: 0.02191833406686783\n",
      "Epoch: 16 Batch: 120 Log-loss: 0.04400721564888954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 Batch: 160 Log-loss: 0.055156409740448\n",
      "Epoch: 16 Batch: 200 Log-loss: 0.05040520802140236\n",
      "Epoch: 16 Batch: 240 Log-loss: 0.03421206399798393\n",
      "Epoch: 16 Batch: 280 Log-loss: 0.0357184037566185\n",
      "Epoch: 16 Batch: 320 Log-loss: 0.03231619670987129\n",
      "Epoch: 16 Batch: 360 Log-loss: 0.050858233124017715\n",
      "Epoch: 16 Batch: 400 Log-loss: 0.0367361418902874\n",
      "Epoch: 16 Batch: 440 Log-loss: 0.037022776901721954\n",
      "Epoch: 16 Batch: 480 Log-loss: 0.039808083325624466\n",
      "Epoch: 16 Batch: 520 Log-loss: 0.023804642260074615\n",
      "Epoch: 16 Batch: 560 Log-loss: 0.05193246901035309\n",
      "Epoch average log-loss: 0.04073957859405449\n",
      "In Epoch: 16, val_loss: 0.04253875446571498, best_val_loss: 0.0422868816796255, best_auc: 0.9868429722666323\n",
      "Epoch: 17 Batch: 0 Log-loss: 0.03952150046825409\n",
      "Epoch: 17 Batch: 40 Log-loss: 0.044496387243270874\n",
      "Epoch: 17 Batch: 80 Log-loss: 0.043847810477018356\n",
      "Epoch: 17 Batch: 120 Log-loss: 0.04595677927136421\n",
      "Epoch: 17 Batch: 160 Log-loss: 0.04383542761206627\n",
      "Epoch: 17 Batch: 200 Log-loss: 0.038378261029720306\n",
      "Epoch: 17 Batch: 240 Log-loss: 0.04164914786815643\n",
      "Epoch: 17 Batch: 280 Log-loss: 0.028246717527508736\n",
      "Epoch: 17 Batch: 320 Log-loss: 0.033804673701524734\n",
      "Epoch: 17 Batch: 360 Log-loss: 0.03695141151547432\n",
      "Epoch: 17 Batch: 400 Log-loss: 0.03336956351995468\n",
      "Epoch: 17 Batch: 440 Log-loss: 0.039942432194948196\n",
      "Epoch: 17 Batch: 480 Log-loss: 0.04782344400882721\n",
      "Epoch: 17 Batch: 520 Log-loss: 0.04154963418841362\n",
      "Epoch: 17 Batch: 560 Log-loss: 0.046141937375068665\n",
      "Epoch average log-loss: 0.040605243212277334\n",
      "In Epoch: 17, val_loss: 0.042444675867866426, best_val_loss: 0.0422868816796255, best_auc: 0.9868429722666323\n",
      "Epoch: 18 Batch: 0 Log-loss: 0.03254767879843712\n",
      "Epoch: 18 Batch: 40 Log-loss: 0.03795096278190613\n",
      "Epoch: 18 Batch: 80 Log-loss: 0.03687882423400879\n",
      "Epoch: 18 Batch: 120 Log-loss: 0.05035516619682312\n",
      "Epoch: 18 Batch: 160 Log-loss: 0.03487452492117882\n",
      "Epoch: 18 Batch: 200 Log-loss: 0.04047418013215065\n",
      "Epoch: 18 Batch: 240 Log-loss: 0.036164235323667526\n",
      "Epoch: 18 Batch: 280 Log-loss: 0.05043644830584526\n",
      "Epoch: 18 Batch: 320 Log-loss: 0.03852442279458046\n",
      "Epoch: 18 Batch: 360 Log-loss: 0.03411685302853584\n",
      "Epoch: 18 Batch: 400 Log-loss: 0.0291756484657526\n",
      "Epoch: 18 Batch: 440 Log-loss: 0.026220649480819702\n",
      "Epoch: 18 Batch: 480 Log-loss: 0.03546713665127754\n",
      "Epoch: 18 Batch: 520 Log-loss: 0.0391906201839447\n",
      "Epoch: 18 Batch: 560 Log-loss: 0.038848765194416046\n",
      "Epoch average log-loss: 0.04027271159325859\n",
      "In Epoch: 18, val_loss: 0.042448166596631226, best_val_loss: 0.0422868816796255, best_auc: 0.9868429722666323\n",
      "Epoch: 19 Batch: 0 Log-loss: 0.031597886234521866\n",
      "Epoch: 19 Batch: 40 Log-loss: 0.05000739172101021\n",
      "Epoch: 19 Batch: 80 Log-loss: 0.03535334765911102\n",
      "Epoch: 19 Batch: 120 Log-loss: 0.044795140624046326\n",
      "Epoch: 19 Batch: 160 Log-loss: 0.038304973393678665\n",
      "Epoch: 19 Batch: 200 Log-loss: 0.04985109344124794\n",
      "Epoch: 19 Batch: 240 Log-loss: 0.038869865238666534\n",
      "Epoch: 19 Batch: 280 Log-loss: 0.04209321737289429\n",
      "Epoch: 19 Batch: 320 Log-loss: 0.04542224109172821\n",
      "Epoch: 19 Batch: 360 Log-loss: 0.0340823233127594\n",
      "Epoch: 19 Batch: 400 Log-loss: 0.03751874342560768\n",
      "Epoch: 19 Batch: 440 Log-loss: 0.036412931978702545\n",
      "Epoch: 19 Batch: 480 Log-loss: 0.04245471954345703\n",
      "Epoch: 19 Batch: 520 Log-loss: 0.03308584913611412\n",
      "Epoch: 19 Batch: 560 Log-loss: 0.038819167762994766\n",
      "Epoch average log-loss: 0.04014586334981556\n",
      "In Epoch: 19, val_loss: 0.042728813309228154, best_val_loss: 0.0422868816796255, best_auc: 0.9868429722666323\n",
      "Epoch: 20 Batch: 0 Log-loss: 0.0481482595205307\n",
      "Epoch: 20 Batch: 40 Log-loss: 0.03699871525168419\n",
      "Epoch: 20 Batch: 80 Log-loss: 0.038185518234968185\n",
      "Epoch: 20 Batch: 120 Log-loss: 0.04740035533905029\n",
      "Epoch: 20 Batch: 160 Log-loss: 0.03691846504807472\n",
      "Epoch: 20 Batch: 200 Log-loss: 0.05408580228686333\n",
      "Epoch: 20 Batch: 240 Log-loss: 0.03968854248523712\n",
      "Epoch: 20 Batch: 280 Log-loss: 0.03799239173531532\n",
      "Epoch: 20 Batch: 320 Log-loss: 0.04492506384849548\n",
      "Epoch: 20 Batch: 360 Log-loss: 0.03183126822113991\n",
      "Epoch: 20 Batch: 400 Log-loss: 0.04822491109371185\n",
      "Epoch: 20 Batch: 440 Log-loss: 0.037921059876680374\n",
      "Epoch: 20 Batch: 480 Log-loss: 0.048067450523376465\n",
      "Epoch: 20 Batch: 520 Log-loss: 0.03829392045736313\n",
      "Epoch: 20 Batch: 560 Log-loss: 0.0345003642141819\n",
      "Epoch average log-loss: 0.03993963035089629\n",
      "In Epoch: 20, val_loss: 0.04251773554426341, best_val_loss: 0.0422868816796255, best_auc: 0.9868429722666323\n",
      "Epoch: 21 Batch: 0 Log-loss: 0.048072244971990585\n",
      "Epoch: 21 Batch: 40 Log-loss: 0.03962300717830658\n",
      "Epoch: 21 Batch: 80 Log-loss: 0.03895549476146698\n",
      "Epoch: 21 Batch: 120 Log-loss: 0.041514866054058075\n",
      "Epoch: 21 Batch: 160 Log-loss: 0.047977298498153687\n",
      "Epoch: 21 Batch: 200 Log-loss: 0.0669383630156517\n",
      "Epoch: 21 Batch: 240 Log-loss: 0.036011502146720886\n",
      "Epoch: 21 Batch: 280 Log-loss: 0.056250933557748795\n",
      "Epoch: 21 Batch: 320 Log-loss: 0.04065219685435295\n",
      "Epoch: 21 Batch: 360 Log-loss: 0.046710651367902756\n",
      "Epoch: 21 Batch: 400 Log-loss: 0.04281776025891304\n",
      "Epoch: 21 Batch: 440 Log-loss: 0.028263762593269348\n",
      "Epoch: 21 Batch: 480 Log-loss: 0.04202145338058472\n",
      "Epoch: 21 Batch: 520 Log-loss: 0.03945789486169815\n",
      "Epoch: 21 Batch: 560 Log-loss: 0.03651590272784233\n",
      "Epoch average log-loss: 0.04005479187305484\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 21, val_loss: 0.04221085752796741, best_val_loss: 0.04221085752796741, best_auc: 0.9867376586214188\n",
      "Epoch: 22 Batch: 0 Log-loss: 0.03468739613890648\n",
      "Epoch: 22 Batch: 40 Log-loss: 0.04824043810367584\n",
      "Epoch: 22 Batch: 80 Log-loss: 0.047535140067338943\n",
      "Epoch: 22 Batch: 120 Log-loss: 0.050396621227264404\n",
      "Epoch: 22 Batch: 160 Log-loss: 0.0422174297273159\n",
      "Epoch: 22 Batch: 200 Log-loss: 0.029496999457478523\n",
      "Epoch: 22 Batch: 240 Log-loss: 0.044284407049417496\n",
      "Epoch: 22 Batch: 280 Log-loss: 0.03409421071410179\n",
      "Epoch: 22 Batch: 320 Log-loss: 0.05865311995148659\n",
      "Epoch: 22 Batch: 360 Log-loss: 0.03157930448651314\n",
      "Epoch: 22 Batch: 400 Log-loss: 0.050141606479883194\n",
      "Epoch: 22 Batch: 440 Log-loss: 0.032746896147727966\n",
      "Epoch: 22 Batch: 480 Log-loss: 0.029889509081840515\n",
      "Epoch: 22 Batch: 520 Log-loss: 0.05900297686457634\n",
      "Epoch: 22 Batch: 560 Log-loss: 0.02605939656496048\n",
      "Epoch average log-loss: 0.03992492567548262\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 22, val_loss: 0.04209090345749982, best_val_loss: 0.04209090345749982, best_auc: 0.9867272698746099\n",
      "Epoch: 23 Batch: 0 Log-loss: 0.04265376552939415\n",
      "Epoch: 23 Batch: 40 Log-loss: 0.03435264155268669\n",
      "Epoch: 23 Batch: 80 Log-loss: 0.052886445075273514\n",
      "Epoch: 23 Batch: 120 Log-loss: 0.041167963296175\n",
      "Epoch: 23 Batch: 160 Log-loss: 0.0440930612385273\n",
      "Epoch: 23 Batch: 200 Log-loss: 0.04252338781952858\n",
      "Epoch: 23 Batch: 240 Log-loss: 0.05068403109908104\n",
      "Epoch: 23 Batch: 280 Log-loss: 0.04027346149086952\n",
      "Epoch: 23 Batch: 320 Log-loss: 0.025245994329452515\n",
      "Epoch: 23 Batch: 360 Log-loss: 0.041123177856206894\n",
      "Epoch: 23 Batch: 400 Log-loss: 0.04207483306527138\n",
      "Epoch: 23 Batch: 440 Log-loss: 0.045131344348192215\n",
      "Epoch: 23 Batch: 480 Log-loss: 0.04878542199730873\n",
      "Epoch: 23 Batch: 520 Log-loss: 0.04847307503223419\n",
      "Epoch: 23 Batch: 560 Log-loss: 0.03256437927484512\n",
      "Epoch average log-loss: 0.03974434185573565\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 23, val_loss: 0.04181238094054135, best_val_loss: 0.04181238094054135, best_auc: 0.9873598219084686\n",
      "Epoch: 24 Batch: 0 Log-loss: 0.05752992257475853\n",
      "Epoch: 24 Batch: 40 Log-loss: 0.04212244227528572\n",
      "Epoch: 24 Batch: 80 Log-loss: 0.04213203117251396\n",
      "Epoch: 24 Batch: 120 Log-loss: 0.027554340660572052\n",
      "Epoch: 24 Batch: 160 Log-loss: 0.03902440518140793\n",
      "Epoch: 24 Batch: 200 Log-loss: 0.04494762048125267\n",
      "Epoch: 24 Batch: 240 Log-loss: 0.05052221193909645\n",
      "Epoch: 24 Batch: 280 Log-loss: 0.032288212329149246\n",
      "Epoch: 24 Batch: 320 Log-loss: 0.030148843303322792\n",
      "Epoch: 24 Batch: 360 Log-loss: 0.036800358444452286\n",
      "Epoch: 24 Batch: 400 Log-loss: 0.04075971618294716\n",
      "Epoch: 24 Batch: 440 Log-loss: 0.05243420973420143\n",
      "Epoch: 24 Batch: 480 Log-loss: 0.028190717101097107\n",
      "Epoch: 24 Batch: 520 Log-loss: 0.03723529726266861\n",
      "Epoch: 24 Batch: 560 Log-loss: 0.04137832671403885\n",
      "Epoch average log-loss: 0.03963209191736366\n",
      "In Epoch: 24, val_loss: 0.041885178354936776, best_val_loss: 0.04181238094054135, best_auc: 0.9873598219084686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 Batch: 0 Log-loss: 0.03611734136939049\n",
      "Epoch: 25 Batch: 40 Log-loss: 0.05352422967553139\n",
      "Epoch: 25 Batch: 80 Log-loss: 0.0320788212120533\n",
      "Epoch: 25 Batch: 120 Log-loss: 0.04427117481827736\n",
      "Epoch: 25 Batch: 160 Log-loss: 0.034133996814489365\n",
      "Epoch: 25 Batch: 200 Log-loss: 0.04796934500336647\n",
      "Epoch: 25 Batch: 240 Log-loss: 0.03925077244639397\n",
      "Epoch: 25 Batch: 280 Log-loss: 0.047123413532972336\n",
      "Epoch: 25 Batch: 320 Log-loss: 0.046512797474861145\n",
      "Epoch: 25 Batch: 360 Log-loss: 0.041517406702041626\n",
      "Epoch: 25 Batch: 400 Log-loss: 0.05257832631468773\n",
      "Epoch: 25 Batch: 440 Log-loss: 0.03543580695986748\n",
      "Epoch: 25 Batch: 480 Log-loss: 0.034210748970508575\n",
      "Epoch: 25 Batch: 520 Log-loss: 0.03406984731554985\n",
      "Epoch: 25 Batch: 560 Log-loss: 0.0286001767963171\n",
      "Epoch average log-loss: 0.03961672610270658\n",
      "In Epoch: 25, val_loss: 0.04182684428150176, best_val_loss: 0.04181238094054135, best_auc: 0.9873598219084686\n",
      "Epoch: 26 Batch: 0 Log-loss: 0.03211689367890358\n",
      "Epoch: 26 Batch: 40 Log-loss: 0.04079219326376915\n",
      "Epoch: 26 Batch: 80 Log-loss: 0.043688077479600906\n",
      "Epoch: 26 Batch: 120 Log-loss: 0.03621000051498413\n",
      "Epoch: 26 Batch: 160 Log-loss: 0.04783305153250694\n",
      "Epoch: 26 Batch: 200 Log-loss: 0.04433266818523407\n",
      "Epoch: 26 Batch: 240 Log-loss: 0.04288594797253609\n",
      "Epoch: 26 Batch: 280 Log-loss: 0.03276631236076355\n",
      "Epoch: 26 Batch: 320 Log-loss: 0.027753731235861778\n",
      "Epoch: 26 Batch: 360 Log-loss: 0.03546849265694618\n",
      "Epoch: 26 Batch: 400 Log-loss: 0.04509309306740761\n",
      "Epoch: 26 Batch: 440 Log-loss: 0.02871164120733738\n",
      "Epoch: 26 Batch: 480 Log-loss: 0.04542538896203041\n",
      "Epoch: 26 Batch: 520 Log-loss: 0.030015820637345314\n",
      "Epoch: 26 Batch: 560 Log-loss: 0.0491403192281723\n",
      "Epoch average log-loss: 0.0395242523235668\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 26, val_loss: 0.041753551896783474, best_val_loss: 0.041753551896783474, best_auc: 0.9871562366237879\n",
      "Epoch: 27 Batch: 0 Log-loss: 0.039056722074747086\n",
      "Epoch: 27 Batch: 40 Log-loss: 0.032438747584819794\n",
      "Epoch: 27 Batch: 80 Log-loss: 0.04680080711841583\n",
      "Epoch: 27 Batch: 120 Log-loss: 0.03959120064973831\n",
      "Epoch: 27 Batch: 160 Log-loss: 0.04483268782496452\n",
      "Epoch: 27 Batch: 200 Log-loss: 0.05145670473575592\n",
      "Epoch: 27 Batch: 240 Log-loss: 0.04239398613572121\n",
      "Epoch: 27 Batch: 280 Log-loss: 0.034379854798316956\n",
      "Epoch: 27 Batch: 320 Log-loss: 0.04990946128964424\n",
      "Epoch: 27 Batch: 360 Log-loss: 0.04675692319869995\n",
      "Epoch: 27 Batch: 400 Log-loss: 0.044983312487602234\n",
      "Epoch: 27 Batch: 440 Log-loss: 0.03445667400956154\n",
      "Epoch: 27 Batch: 480 Log-loss: 0.039897236973047256\n",
      "Epoch: 27 Batch: 520 Log-loss: 0.023105375468730927\n",
      "Epoch: 27 Batch: 560 Log-loss: 0.040979839861392975\n",
      "Epoch average log-loss: 0.039313611764061666\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 27, val_loss: 0.04175161343655274, best_val_loss: 0.04175161343655274, best_auc: 0.9874268412300927\n",
      "Epoch: 28 Batch: 0 Log-loss: 0.04143301770091057\n",
      "Epoch: 28 Batch: 40 Log-loss: 0.033631790429353714\n",
      "Epoch: 28 Batch: 80 Log-loss: 0.03320568799972534\n",
      "Epoch: 28 Batch: 120 Log-loss: 0.03362313285470009\n",
      "Epoch: 28 Batch: 160 Log-loss: 0.031467873603105545\n",
      "Epoch: 28 Batch: 200 Log-loss: 0.032427459955215454\n",
      "Epoch: 28 Batch: 240 Log-loss: 0.03273792937397957\n",
      "Epoch: 28 Batch: 280 Log-loss: 0.03414706140756607\n",
      "Epoch: 28 Batch: 320 Log-loss: 0.05215233191847801\n",
      "Epoch: 28 Batch: 360 Log-loss: 0.045494500547647476\n",
      "Epoch: 28 Batch: 400 Log-loss: 0.03351025655865669\n",
      "Epoch: 28 Batch: 440 Log-loss: 0.0388106107711792\n",
      "Epoch: 28 Batch: 480 Log-loss: 0.059112098067998886\n",
      "Epoch: 28 Batch: 520 Log-loss: 0.029142601415514946\n",
      "Epoch: 28 Batch: 560 Log-loss: 0.04032805189490318\n",
      "Epoch average log-loss: 0.039174414555808265\n",
      "In Epoch: 28, val_loss: 0.04199128397898963, best_val_loss: 0.04175161343655274, best_auc: 0.9874268412300927\n",
      "Epoch: 29 Batch: 0 Log-loss: 0.03605872765183449\n",
      "Epoch: 29 Batch: 40 Log-loss: 0.04669317603111267\n",
      "Epoch: 29 Batch: 80 Log-loss: 0.03873319551348686\n",
      "Epoch: 29 Batch: 120 Log-loss: 0.034614451229572296\n",
      "Epoch: 29 Batch: 160 Log-loss: 0.039458129554986954\n",
      "Epoch: 29 Batch: 200 Log-loss: 0.04574296250939369\n",
      "Epoch: 29 Batch: 240 Log-loss: 0.04888784885406494\n",
      "Epoch: 29 Batch: 280 Log-loss: 0.052797410637140274\n",
      "Epoch: 29 Batch: 320 Log-loss: 0.03884025290608406\n",
      "Epoch: 29 Batch: 360 Log-loss: 0.037442397326231\n",
      "Epoch: 29 Batch: 400 Log-loss: 0.034169044345617294\n",
      "Epoch: 29 Batch: 440 Log-loss: 0.038547273725271225\n",
      "Epoch: 29 Batch: 480 Log-loss: 0.039442453533411026\n",
      "Epoch: 29 Batch: 520 Log-loss: 0.04251185059547424\n",
      "Epoch: 29 Batch: 560 Log-loss: 0.03249805048108101\n",
      "Epoch average log-loss: 0.03912507661152631\n",
      "In Epoch: 29, val_loss: 0.04195233409659493, best_val_loss: 0.04175161343655274, best_auc: 0.9874268412300927\n",
      "Epoch: 30 Batch: 0 Log-loss: 0.0415288545191288\n",
      "Epoch: 30 Batch: 40 Log-loss: 0.03895184025168419\n",
      "Epoch: 30 Batch: 80 Log-loss: 0.045517709106206894\n",
      "Epoch: 30 Batch: 120 Log-loss: 0.035254109650850296\n",
      "Epoch: 30 Batch: 160 Log-loss: 0.04978454113006592\n",
      "Epoch: 30 Batch: 200 Log-loss: 0.05006362497806549\n",
      "Epoch: 30 Batch: 240 Log-loss: 0.056391533464193344\n",
      "Epoch: 30 Batch: 280 Log-loss: 0.034270886331796646\n",
      "Epoch: 30 Batch: 320 Log-loss: 0.04633956030011177\n",
      "Epoch: 30 Batch: 360 Log-loss: 0.03800278156995773\n",
      "Epoch: 30 Batch: 400 Log-loss: 0.03986911475658417\n",
      "Epoch: 30 Batch: 440 Log-loss: 0.03752361983060837\n",
      "Epoch: 30 Batch: 480 Log-loss: 0.044563163071870804\n",
      "Epoch: 30 Batch: 520 Log-loss: 0.04235340282320976\n",
      "Epoch: 30 Batch: 560 Log-loss: 0.04807645082473755\n",
      "Epoch average log-loss: 0.03903670711110213\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 30, val_loss: 0.04166190890817393, best_val_loss: 0.04166190890817393, best_auc: 0.9874968658576654\n",
      "Epoch: 31 Batch: 0 Log-loss: 0.043386172503232956\n",
      "Epoch: 31 Batch: 40 Log-loss: 0.05281304195523262\n",
      "Epoch: 31 Batch: 80 Log-loss: 0.05640267953276634\n",
      "Epoch: 31 Batch: 120 Log-loss: 0.028698280453681946\n",
      "Epoch: 31 Batch: 160 Log-loss: 0.037814345210790634\n",
      "Epoch: 31 Batch: 200 Log-loss: 0.036073412746191025\n",
      "Epoch: 31 Batch: 240 Log-loss: 0.03784969076514244\n",
      "Epoch: 31 Batch: 280 Log-loss: 0.035510674118995667\n",
      "Epoch: 31 Batch: 320 Log-loss: 0.03295936807990074\n",
      "Epoch: 31 Batch: 360 Log-loss: 0.04743124544620514\n",
      "Epoch: 31 Batch: 400 Log-loss: 0.03966294974088669\n",
      "Epoch: 31 Batch: 440 Log-loss: 0.056126486510038376\n",
      "Epoch: 31 Batch: 480 Log-loss: 0.030991429463028908\n",
      "Epoch: 31 Batch: 520 Log-loss: 0.04034465178847313\n",
      "Epoch: 31 Batch: 560 Log-loss: 0.039936162531375885\n",
      "Epoch average log-loss: 0.038935033541305786\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 31, val_loss: 0.0416213014229791, best_val_loss: 0.0416213014229791, best_auc: 0.9866807491133921\n",
      "Epoch: 32 Batch: 0 Log-loss: 0.03560655564069748\n",
      "Epoch: 32 Batch: 40 Log-loss: 0.029719362035393715\n",
      "Epoch: 32 Batch: 80 Log-loss: 0.030968422070145607\n",
      "Epoch: 32 Batch: 120 Log-loss: 0.042608004063367844\n",
      "Epoch: 32 Batch: 160 Log-loss: 0.03130704164505005\n",
      "Epoch: 32 Batch: 200 Log-loss: 0.025622578337788582\n",
      "Epoch: 32 Batch: 240 Log-loss: 0.03132202848792076\n",
      "Epoch: 32 Batch: 280 Log-loss: 0.04234391078352928\n",
      "Epoch: 32 Batch: 320 Log-loss: 0.0430433489382267\n",
      "Epoch: 32 Batch: 360 Log-loss: 0.04416531324386597\n",
      "Epoch: 32 Batch: 400 Log-loss: 0.04350602254271507\n",
      "Epoch: 32 Batch: 440 Log-loss: 0.03808652237057686\n",
      "Epoch: 32 Batch: 480 Log-loss: 0.0343695767223835\n",
      "Epoch: 32 Batch: 520 Log-loss: 0.047695014625787735\n",
      "Epoch: 32 Batch: 560 Log-loss: 0.03817657008767128\n",
      "Epoch average log-loss: 0.03905251646148307\n",
      "In Epoch: 32, val_loss: 0.04166212550541813, best_val_loss: 0.0416213014229791, best_auc: 0.9866807491133921\n",
      "Epoch: 33 Batch: 0 Log-loss: 0.03344040364027023\n",
      "Epoch: 33 Batch: 40 Log-loss: 0.03874392807483673\n",
      "Epoch: 33 Batch: 80 Log-loss: 0.050110891461372375\n",
      "Epoch: 33 Batch: 120 Log-loss: 0.038363780826330185\n",
      "Epoch: 33 Batch: 160 Log-loss: 0.04877637326717377\n",
      "Epoch: 33 Batch: 200 Log-loss: 0.04323554411530495\n",
      "Epoch: 33 Batch: 240 Log-loss: 0.037194833159446716\n",
      "Epoch: 33 Batch: 280 Log-loss: 0.04665239527821541\n",
      "Epoch: 33 Batch: 320 Log-loss: 0.04783393442630768\n",
      "Epoch: 33 Batch: 360 Log-loss: 0.0370052233338356\n",
      "Epoch: 33 Batch: 400 Log-loss: 0.0581587590277195\n",
      "Epoch: 33 Batch: 440 Log-loss: 0.02774965763092041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 Batch: 480 Log-loss: 0.04294782876968384\n",
      "Epoch: 33 Batch: 520 Log-loss: 0.04075121879577637\n",
      "Epoch: 33 Batch: 560 Log-loss: 0.030854465439915657\n",
      "Epoch average log-loss: 0.03878066487211202\n",
      "In Epoch: 33, val_loss: 0.04177879731612528, best_val_loss: 0.0416213014229791, best_auc: 0.9866807491133921\n",
      "Epoch: 34 Batch: 0 Log-loss: 0.02281244285404682\n",
      "Epoch: 34 Batch: 40 Log-loss: 0.05169738829135895\n",
      "Epoch: 34 Batch: 80 Log-loss: 0.035210635513067245\n",
      "Epoch: 34 Batch: 120 Log-loss: 0.029563473537564278\n",
      "Epoch: 34 Batch: 160 Log-loss: 0.03337475284934044\n",
      "Epoch: 34 Batch: 200 Log-loss: 0.047140974551439285\n",
      "Epoch: 34 Batch: 240 Log-loss: 0.033271726220846176\n",
      "Epoch: 34 Batch: 280 Log-loss: 0.04380737617611885\n",
      "Epoch: 34 Batch: 320 Log-loss: 0.03285035490989685\n",
      "Epoch: 34 Batch: 360 Log-loss: 0.038936879485845566\n",
      "Epoch: 34 Batch: 400 Log-loss: 0.030008971691131592\n",
      "Epoch: 34 Batch: 440 Log-loss: 0.0343145877122879\n",
      "Epoch: 34 Batch: 480 Log-loss: 0.03468986228108406\n",
      "Epoch: 34 Batch: 520 Log-loss: 0.03199629858136177\n",
      "Epoch: 34 Batch: 560 Log-loss: 0.040392495691776276\n",
      "Epoch average log-loss: 0.03891053100648735\n",
      "In Epoch: 34, val_loss: 0.0418617698233179, best_val_loss: 0.0416213014229791, best_auc: 0.9866807491133921\n",
      "Epoch: 35 Batch: 0 Log-loss: 0.04719521105289459\n",
      "Epoch: 35 Batch: 40 Log-loss: 0.03758075460791588\n",
      "Epoch: 35 Batch: 80 Log-loss: 0.04351087287068367\n",
      "Epoch: 35 Batch: 120 Log-loss: 0.03702186793088913\n",
      "Epoch: 35 Batch: 160 Log-loss: 0.0258078221231699\n",
      "Epoch: 35 Batch: 200 Log-loss: 0.02718006633222103\n",
      "Epoch: 35 Batch: 240 Log-loss: 0.03858741745352745\n",
      "Epoch: 35 Batch: 280 Log-loss: 0.03818261995911598\n",
      "Epoch: 35 Batch: 320 Log-loss: 0.03835039958357811\n",
      "Epoch: 35 Batch: 360 Log-loss: 0.02542421966791153\n",
      "Epoch: 35 Batch: 400 Log-loss: 0.029384851455688477\n",
      "Epoch: 35 Batch: 440 Log-loss: 0.03404982015490532\n",
      "Epoch: 35 Batch: 480 Log-loss: 0.03509274125099182\n",
      "Epoch: 35 Batch: 520 Log-loss: 0.02525612711906433\n",
      "Epoch: 35 Batch: 560 Log-loss: 0.035513125360012054\n",
      "Epoch average log-loss: 0.03898222127796284\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 35, val_loss: 0.04154227325155709, best_val_loss: 0.04154227325155709, best_auc: 0.9869112902003457\n",
      "Epoch: 36 Batch: 0 Log-loss: 0.0370587520301342\n",
      "Epoch: 36 Batch: 40 Log-loss: 0.036394428461790085\n",
      "Epoch: 36 Batch: 80 Log-loss: 0.04110170528292656\n",
      "Epoch: 36 Batch: 120 Log-loss: 0.06043744087219238\n",
      "Epoch: 36 Batch: 160 Log-loss: 0.028780221939086914\n",
      "Epoch: 36 Batch: 200 Log-loss: 0.03685260936617851\n",
      "Epoch: 36 Batch: 240 Log-loss: 0.04821844398975372\n",
      "Epoch: 36 Batch: 280 Log-loss: 0.042465899139642715\n",
      "Epoch: 36 Batch: 320 Log-loss: 0.038384292274713516\n",
      "Epoch: 36 Batch: 360 Log-loss: 0.049424219876527786\n",
      "Epoch: 36 Batch: 400 Log-loss: 0.033075589686632156\n",
      "Epoch: 36 Batch: 440 Log-loss: 0.045406877994537354\n",
      "Epoch: 36 Batch: 480 Log-loss: 0.031036244705319405\n",
      "Epoch: 36 Batch: 520 Log-loss: 0.05060138180851936\n",
      "Epoch: 36 Batch: 560 Log-loss: 0.031744640320539474\n",
      "Epoch average log-loss: 0.03882291274743953\n",
      "In Epoch: 36, val_loss: 0.04190418797233663, best_val_loss: 0.04154227325155709, best_auc: 0.9869112902003457\n",
      "Epoch: 37 Batch: 0 Log-loss: 0.04558316245675087\n",
      "Epoch: 37 Batch: 40 Log-loss: 0.042254675179719925\n",
      "Epoch: 37 Batch: 80 Log-loss: 0.03261808305978775\n",
      "Epoch: 37 Batch: 120 Log-loss: 0.0422704815864563\n",
      "Epoch: 37 Batch: 160 Log-loss: 0.03377433493733406\n",
      "Epoch: 37 Batch: 200 Log-loss: 0.036274876445531845\n",
      "Epoch: 37 Batch: 240 Log-loss: 0.04274823144078255\n",
      "Epoch: 37 Batch: 280 Log-loss: 0.04826554283499718\n",
      "Epoch: 37 Batch: 320 Log-loss: 0.03235278278589249\n",
      "Epoch: 37 Batch: 360 Log-loss: 0.02899748831987381\n",
      "Epoch: 37 Batch: 400 Log-loss: 0.03678058460354805\n",
      "Epoch: 37 Batch: 440 Log-loss: 0.044116437435150146\n",
      "Epoch: 37 Batch: 480 Log-loss: 0.054333481937646866\n",
      "Epoch: 37 Batch: 520 Log-loss: 0.036250658333301544\n",
      "Epoch: 37 Batch: 560 Log-loss: 0.03959512710571289\n",
      "Epoch average log-loss: 0.03859548873068499\n",
      "In Epoch: 37, val_loss: 0.04166800474092141, best_val_loss: 0.04154227325155709, best_auc: 0.9869112902003457\n",
      "Epoch: 38 Batch: 0 Log-loss: 0.05452461540699005\n",
      "Epoch: 38 Batch: 40 Log-loss: 0.03531818091869354\n",
      "Epoch: 38 Batch: 80 Log-loss: 0.045901019126176834\n",
      "Epoch: 38 Batch: 120 Log-loss: 0.033524397760629654\n",
      "Epoch: 38 Batch: 160 Log-loss: 0.04270882532000542\n",
      "Epoch: 38 Batch: 200 Log-loss: 0.02771301567554474\n",
      "Epoch: 38 Batch: 240 Log-loss: 0.02998310513794422\n",
      "Epoch: 38 Batch: 280 Log-loss: 0.031023448333144188\n",
      "Epoch: 38 Batch: 320 Log-loss: 0.030766809359192848\n",
      "Epoch: 38 Batch: 360 Log-loss: 0.04010871425271034\n",
      "Epoch: 38 Batch: 400 Log-loss: 0.03993852436542511\n",
      "Epoch: 38 Batch: 440 Log-loss: 0.034508880227804184\n",
      "Epoch: 38 Batch: 480 Log-loss: 0.05288071557879448\n",
      "Epoch: 38 Batch: 520 Log-loss: 0.03970940038561821\n",
      "Epoch: 38 Batch: 560 Log-loss: 0.05139254406094551\n",
      "Epoch average log-loss: 0.038807507275071526\n",
      "In Epoch: 38, val_loss: 0.04202314584062417, best_val_loss: 0.04154227325155709, best_auc: 0.9869112902003457\n",
      "Epoch: 39 Batch: 0 Log-loss: 0.03676225244998932\n",
      "Epoch: 39 Batch: 40 Log-loss: 0.05913630127906799\n",
      "Epoch: 39 Batch: 80 Log-loss: 0.02901720441877842\n",
      "Epoch: 39 Batch: 120 Log-loss: 0.03554988652467728\n",
      "Epoch: 39 Batch: 160 Log-loss: 0.03279542177915573\n",
      "Epoch: 39 Batch: 200 Log-loss: 0.04194031283259392\n",
      "Epoch: 39 Batch: 240 Log-loss: 0.03619503974914551\n",
      "Epoch: 39 Batch: 280 Log-loss: 0.03999209403991699\n",
      "Epoch: 39 Batch: 320 Log-loss: 0.04055189713835716\n",
      "Epoch: 39 Batch: 360 Log-loss: 0.048242658376693726\n",
      "Epoch: 39 Batch: 400 Log-loss: 0.041658010333776474\n",
      "Epoch: 39 Batch: 440 Log-loss: 0.03166116401553154\n",
      "Epoch: 39 Batch: 480 Log-loss: 0.041288670152425766\n",
      "Epoch: 39 Batch: 520 Log-loss: 0.03759939223527908\n",
      "Epoch: 39 Batch: 560 Log-loss: 0.04009908810257912\n",
      "Epoch average log-loss: 0.03850771918826337\n",
      "In Epoch: 39, val_loss: 0.04181217912592275, best_val_loss: 0.04154227325155709, best_auc: 0.9869112902003457\n",
      "Epoch: 40 Batch: 0 Log-loss: 0.03253359720110893\n",
      "Epoch: 40 Batch: 40 Log-loss: 0.04529928043484688\n",
      "Epoch: 40 Batch: 80 Log-loss: 0.02727520279586315\n",
      "Epoch: 40 Batch: 120 Log-loss: 0.046834904700517654\n",
      "Epoch: 40 Batch: 160 Log-loss: 0.04212826117873192\n",
      "Epoch: 40 Batch: 200 Log-loss: 0.05782670900225639\n",
      "Epoch: 40 Batch: 240 Log-loss: 0.02942744642496109\n",
      "Epoch: 40 Batch: 280 Log-loss: 0.036287810653448105\n",
      "Epoch: 40 Batch: 320 Log-loss: 0.04692763462662697\n",
      "Epoch: 40 Batch: 360 Log-loss: 0.04188133776187897\n",
      "Epoch: 40 Batch: 400 Log-loss: 0.045927513390779495\n",
      "Epoch: 40 Batch: 440 Log-loss: 0.03422844409942627\n",
      "Epoch: 40 Batch: 480 Log-loss: 0.03276650980114937\n",
      "Epoch: 40 Batch: 520 Log-loss: 0.0440344363451004\n",
      "Epoch: 40 Batch: 560 Log-loss: 0.03426419943571091\n",
      "Epoch average log-loss: 0.03865880719386041\n",
      "In Epoch: 40, val_loss: 0.042527507785509566, best_val_loss: 0.04154227325155709, best_auc: 0.9869112902003457\n",
      "Epoch: 41 Batch: 0 Log-loss: 0.032544735819101334\n",
      "Epoch: 41 Batch: 40 Log-loss: 0.04461686685681343\n",
      "Epoch: 41 Batch: 80 Log-loss: 0.04214230179786682\n",
      "Epoch: 41 Batch: 120 Log-loss: 0.04230557754635811\n",
      "Epoch: 41 Batch: 160 Log-loss: 0.036672331392765045\n",
      "Epoch: 41 Batch: 200 Log-loss: 0.03827093914151192\n",
      "Epoch: 41 Batch: 240 Log-loss: 0.04304811730980873\n",
      "Epoch: 41 Batch: 280 Log-loss: 0.04247121512889862\n",
      "Epoch: 41 Batch: 320 Log-loss: 0.04335710033774376\n",
      "Epoch: 41 Batch: 360 Log-loss: 0.041329752653837204\n",
      "Epoch: 41 Batch: 400 Log-loss: 0.045198675245046616\n",
      "Epoch: 41 Batch: 440 Log-loss: 0.04832098260521889\n",
      "Epoch: 41 Batch: 480 Log-loss: 0.031779322773218155\n",
      "Epoch: 41 Batch: 520 Log-loss: 0.032057102769613266\n",
      "Epoch: 41 Batch: 560 Log-loss: 0.04476660490036011\n",
      "Epoch average log-loss: 0.03856522211738463\n",
      "In Epoch: 41, val_loss: 0.041597399168701144, best_val_loss: 0.04154227325155709, best_auc: 0.9869112902003457\n",
      "Epoch: 42 Batch: 0 Log-loss: 0.04112055525183678\n",
      "Epoch: 42 Batch: 40 Log-loss: 0.05200977623462677\n",
      "Epoch: 42 Batch: 80 Log-loss: 0.025559142231941223\n",
      "Epoch: 42 Batch: 120 Log-loss: 0.029893256723880768\n",
      "Epoch: 42 Batch: 160 Log-loss: 0.02896719239652157\n",
      "Epoch: 42 Batch: 200 Log-loss: 0.03282015025615692\n",
      "Epoch: 42 Batch: 240 Log-loss: 0.04207580164074898\n",
      "Epoch: 42 Batch: 280 Log-loss: 0.040471985936164856\n",
      "Epoch: 42 Batch: 320 Log-loss: 0.04267032817006111\n",
      "Epoch: 42 Batch: 360 Log-loss: 0.025638418272137642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 Batch: 400 Log-loss: 0.05925353989005089\n",
      "Epoch: 42 Batch: 440 Log-loss: 0.03504970297217369\n",
      "Epoch: 42 Batch: 480 Log-loss: 0.03468737751245499\n",
      "Epoch: 42 Batch: 520 Log-loss: 0.05605269968509674\n",
      "Epoch: 42 Batch: 560 Log-loss: 0.03757371008396149\n",
      "Epoch average log-loss: 0.03849809550613697\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 42, val_loss: 0.04146947996036714, best_val_loss: 0.04146947996036714, best_auc: 0.987378816496406\n",
      "Epoch: 43 Batch: 0 Log-loss: 0.050220787525177\n",
      "Epoch: 43 Batch: 40 Log-loss: 0.03169426694512367\n",
      "Epoch: 43 Batch: 80 Log-loss: 0.029376229271292686\n",
      "Epoch: 43 Batch: 120 Log-loss: 0.05160730704665184\n",
      "Epoch: 43 Batch: 160 Log-loss: 0.034931741654872894\n",
      "Epoch: 43 Batch: 200 Log-loss: 0.05077892914414406\n",
      "Epoch: 43 Batch: 240 Log-loss: 0.05829188600182533\n",
      "Epoch: 43 Batch: 280 Log-loss: 0.03727436810731888\n",
      "Epoch: 43 Batch: 320 Log-loss: 0.03658908233046532\n",
      "Epoch: 43 Batch: 360 Log-loss: 0.0444166325032711\n",
      "Epoch: 43 Batch: 400 Log-loss: 0.04647878184914589\n",
      "Epoch: 43 Batch: 440 Log-loss: 0.04646439850330353\n",
      "Epoch: 43 Batch: 480 Log-loss: 0.04147312045097351\n",
      "Epoch: 43 Batch: 520 Log-loss: 0.027831457555294037\n",
      "Epoch: 43 Batch: 560 Log-loss: 0.019415559247136116\n",
      "Epoch average log-loss: 0.03868817410298756\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 43, val_loss: 0.04133642792965824, best_val_loss: 0.04133642792965824, best_auc: 0.9875212725198687\n",
      "Epoch: 44 Batch: 0 Log-loss: 0.04418632760643959\n",
      "Epoch: 44 Batch: 40 Log-loss: 0.049238383769989014\n",
      "Epoch: 44 Batch: 80 Log-loss: 0.050707947462797165\n",
      "Epoch: 44 Batch: 120 Log-loss: 0.029543856158852577\n",
      "Epoch: 44 Batch: 160 Log-loss: 0.03987123444676399\n",
      "Epoch: 44 Batch: 200 Log-loss: 0.04701940715312958\n",
      "Epoch: 44 Batch: 240 Log-loss: 0.05054566636681557\n",
      "Epoch: 44 Batch: 280 Log-loss: 0.034359369426965714\n",
      "Epoch: 44 Batch: 320 Log-loss: 0.039147306233644485\n",
      "Epoch: 44 Batch: 360 Log-loss: 0.04214170575141907\n",
      "Epoch: 44 Batch: 400 Log-loss: 0.03724205493927002\n",
      "Epoch: 44 Batch: 440 Log-loss: 0.028617357835173607\n",
      "Epoch: 44 Batch: 480 Log-loss: 0.0512811578810215\n",
      "Epoch: 44 Batch: 520 Log-loss: 0.03905286639928818\n",
      "Epoch: 44 Batch: 560 Log-loss: 0.038357753306627274\n",
      "Epoch average log-loss: 0.03852840683123629\n",
      "In Epoch: 44, val_loss: 0.04142920100296347, best_val_loss: 0.04133642792965824, best_auc: 0.9875212725198687\n",
      "Epoch: 45 Batch: 0 Log-loss: 0.02800757624208927\n",
      "Epoch: 45 Batch: 40 Log-loss: 0.04763980209827423\n",
      "Epoch: 45 Batch: 80 Log-loss: 0.031661227345466614\n",
      "Epoch: 45 Batch: 120 Log-loss: 0.05159477889537811\n",
      "Epoch: 45 Batch: 160 Log-loss: 0.032985080033540726\n",
      "Epoch: 45 Batch: 200 Log-loss: 0.023474888876080513\n",
      "Epoch: 45 Batch: 240 Log-loss: 0.054065827280282974\n",
      "Epoch: 45 Batch: 280 Log-loss: 0.04014742001891136\n",
      "Epoch: 45 Batch: 320 Log-loss: 0.036916591227054596\n",
      "Epoch: 45 Batch: 360 Log-loss: 0.034230224788188934\n",
      "Epoch: 45 Batch: 400 Log-loss: 0.03143102675676346\n",
      "Epoch: 45 Batch: 440 Log-loss: 0.045237284153699875\n",
      "Epoch: 45 Batch: 480 Log-loss: 0.028097333386540413\n",
      "Epoch: 45 Batch: 520 Log-loss: 0.03855646401643753\n",
      "Epoch: 45 Batch: 560 Log-loss: 0.02806725911796093\n",
      "Epoch average log-loss: 0.038596345095096955\n",
      "In Epoch: 45, val_loss: 0.041576259693211604, best_val_loss: 0.04133642792965824, best_auc: 0.9875212725198687\n",
      "Epoch: 46 Batch: 0 Log-loss: 0.042040660977363586\n",
      "Epoch: 46 Batch: 40 Log-loss: 0.03378871828317642\n",
      "Epoch: 46 Batch: 80 Log-loss: 0.03080068528652191\n",
      "Epoch: 46 Batch: 120 Log-loss: 0.034387219697237015\n",
      "Epoch: 46 Batch: 160 Log-loss: 0.05111138895153999\n",
      "Epoch: 46 Batch: 200 Log-loss: 0.03992580994963646\n",
      "Epoch: 46 Batch: 240 Log-loss: 0.03000209480524063\n",
      "Epoch: 46 Batch: 280 Log-loss: 0.033580880612134933\n",
      "Epoch: 46 Batch: 320 Log-loss: 0.031768787652254105\n",
      "Epoch: 46 Batch: 360 Log-loss: 0.03264393284916878\n",
      "Epoch: 46 Batch: 400 Log-loss: 0.03719690442085266\n",
      "Epoch: 46 Batch: 440 Log-loss: 0.04778227582573891\n",
      "Epoch: 46 Batch: 480 Log-loss: 0.04205312207341194\n",
      "Epoch: 46 Batch: 520 Log-loss: 0.03253820911049843\n",
      "Epoch: 46 Batch: 560 Log-loss: 0.032130271196365356\n",
      "Epoch average log-loss: 0.03843222980587078\n",
      "In Epoch: 46, val_loss: 0.041707820148142734, best_val_loss: 0.04133642792965824, best_auc: 0.9875212725198687\n",
      "Epoch: 47 Batch: 0 Log-loss: 0.040477193892002106\n",
      "Epoch: 47 Batch: 40 Log-loss: 0.04231840372085571\n",
      "Epoch: 47 Batch: 80 Log-loss: 0.03519380837678909\n",
      "Epoch: 47 Batch: 120 Log-loss: 0.044865682721138\n",
      "Epoch: 47 Batch: 160 Log-loss: 0.03764917701482773\n",
      "Epoch: 47 Batch: 200 Log-loss: 0.03846735507249832\n",
      "Epoch: 47 Batch: 240 Log-loss: 0.03027997724711895\n",
      "Epoch: 47 Batch: 280 Log-loss: 0.023652510717511177\n",
      "Epoch: 47 Batch: 320 Log-loss: 0.04109976068139076\n",
      "Epoch: 47 Batch: 360 Log-loss: 0.028603561222553253\n",
      "Epoch: 47 Batch: 400 Log-loss: 0.04111587628722191\n",
      "Epoch: 47 Batch: 440 Log-loss: 0.03838163986802101\n",
      "Epoch: 47 Batch: 480 Log-loss: 0.03907119855284691\n",
      "Epoch: 47 Batch: 520 Log-loss: 0.03464734926819801\n",
      "Epoch: 47 Batch: 560 Log-loss: 0.05849970877170563\n",
      "Epoch average log-loss: 0.038414134916716386\n",
      "In Epoch: 47, val_loss: 0.042017255619177594, best_val_loss: 0.04133642792965824, best_auc: 0.9875212725198687\n",
      "Epoch: 48 Batch: 0 Log-loss: 0.0442684032022953\n",
      "Epoch: 48 Batch: 40 Log-loss: 0.05115613713860512\n",
      "Epoch: 48 Batch: 80 Log-loss: 0.03872312232851982\n",
      "Epoch: 48 Batch: 120 Log-loss: 0.04930286481976509\n",
      "Epoch: 48 Batch: 160 Log-loss: 0.022792259231209755\n",
      "Epoch: 48 Batch: 200 Log-loss: 0.046268168836832047\n",
      "Epoch: 48 Batch: 240 Log-loss: 0.04020599648356438\n",
      "Epoch: 48 Batch: 280 Log-loss: 0.034444186836481094\n",
      "Epoch: 48 Batch: 320 Log-loss: 0.03177318349480629\n",
      "Epoch: 48 Batch: 360 Log-loss: 0.039303820580244064\n",
      "Epoch: 48 Batch: 400 Log-loss: 0.03595015034079552\n",
      "Epoch: 48 Batch: 440 Log-loss: 0.04701945558190346\n",
      "Epoch: 48 Batch: 480 Log-loss: 0.034652743488550186\n",
      "Epoch: 48 Batch: 520 Log-loss: 0.035626549273729324\n",
      "Epoch: 48 Batch: 560 Log-loss: 0.03910105675458908\n",
      "Epoch average log-loss: 0.03863375007307955\n",
      "In Epoch: 48, val_loss: 0.04198488749736575, best_val_loss: 0.04133642792965824, best_auc: 0.9875212725198687\n",
      "Epoch: 49 Batch: 0 Log-loss: 0.04179540276527405\n",
      "Epoch: 49 Batch: 40 Log-loss: 0.033372100442647934\n",
      "Epoch: 49 Batch: 80 Log-loss: 0.04682968556880951\n",
      "Epoch: 49 Batch: 120 Log-loss: 0.022007569670677185\n",
      "Epoch: 49 Batch: 160 Log-loss: 0.04267587140202522\n",
      "Epoch: 49 Batch: 200 Log-loss: 0.01940099522471428\n",
      "Epoch: 49 Batch: 240 Log-loss: 0.033030249178409576\n",
      "Epoch: 49 Batch: 280 Log-loss: 0.028123989701271057\n",
      "Epoch: 49 Batch: 320 Log-loss: 0.03359316661953926\n",
      "Epoch: 49 Batch: 360 Log-loss: 0.046667519956827164\n",
      "Epoch: 49 Batch: 400 Log-loss: 0.03525545075535774\n",
      "Epoch: 49 Batch: 440 Log-loss: 0.036530982702970505\n",
      "Epoch: 49 Batch: 480 Log-loss: 0.029423518106341362\n",
      "Epoch: 49 Batch: 520 Log-loss: 0.03907554969191551\n",
      "Epoch: 49 Batch: 560 Log-loss: 0.03243456035852432\n",
      "Epoch average log-loss: 0.03834643845579454\n",
      "In Epoch: 49, val_loss: 0.04170057783905204, best_val_loss: 0.04133642792965824, best_auc: 0.9875212725198687\n",
      "Epoch: 50 Batch: 0 Log-loss: 0.029879899695515633\n",
      "Epoch: 50 Batch: 40 Log-loss: 0.03475571796298027\n",
      "Epoch: 50 Batch: 80 Log-loss: 0.04168407618999481\n",
      "Epoch: 50 Batch: 120 Log-loss: 0.034553032368421555\n",
      "Epoch: 50 Batch: 160 Log-loss: 0.04294028878211975\n",
      "Epoch: 50 Batch: 200 Log-loss: 0.03379259258508682\n",
      "Epoch: 50 Batch: 240 Log-loss: 0.044500429183244705\n",
      "Epoch: 50 Batch: 280 Log-loss: 0.046489372849464417\n",
      "Epoch: 50 Batch: 320 Log-loss: 0.024923279881477356\n",
      "Epoch: 50 Batch: 360 Log-loss: 0.03495364263653755\n",
      "Epoch: 50 Batch: 400 Log-loss: 0.042567815631628036\n",
      "Epoch: 50 Batch: 440 Log-loss: 0.04105303809046745\n",
      "Epoch: 50 Batch: 480 Log-loss: 0.029341189190745354\n",
      "Epoch: 50 Batch: 520 Log-loss: 0.022601904347538948\n",
      "Epoch: 50 Batch: 560 Log-loss: 0.034999534487724304\n",
      "Epoch average log-loss: 0.03856213316321373\n",
      "In Epoch: 50, val_loss: 0.0416670828972868, best_val_loss: 0.04133642792965824, best_auc: 0.9875212725198687\n",
      "Epoch: 51 Batch: 0 Log-loss: 0.04347440227866173\n",
      "Epoch: 51 Batch: 40 Log-loss: 0.028843805193901062\n",
      "Epoch: 51 Batch: 80 Log-loss: 0.03632762283086777\n",
      "Epoch: 51 Batch: 120 Log-loss: 0.034435421228408813\n",
      "Epoch: 51 Batch: 160 Log-loss: 0.040984757244586945\n",
      "Epoch: 51 Batch: 200 Log-loss: 0.03043908067047596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 Batch: 240 Log-loss: 0.026826458051800728\n",
      "Epoch: 51 Batch: 280 Log-loss: 0.03982740640640259\n",
      "Epoch: 51 Batch: 320 Log-loss: 0.05488337576389313\n",
      "Epoch: 51 Batch: 360 Log-loss: 0.03144208714365959\n",
      "Epoch: 51 Batch: 400 Log-loss: 0.048389192670583725\n",
      "Epoch: 51 Batch: 440 Log-loss: 0.045446936041116714\n",
      "Epoch: 51 Batch: 480 Log-loss: 0.03606998175382614\n",
      "Epoch: 51 Batch: 520 Log-loss: 0.04017603024840355\n",
      "Epoch: 51 Batch: 560 Log-loss: 0.03340761363506317\n",
      "Epoch average log-loss: 0.038412581909714\n",
      "In Epoch: 51, val_loss: 0.04181287365074984, best_val_loss: 0.04133642792965824, best_auc: 0.9875212725198687\n",
      "Epoch: 52 Batch: 0 Log-loss: 0.022777898237109184\n",
      "Epoch: 52 Batch: 40 Log-loss: 0.03185644373297691\n",
      "Epoch: 52 Batch: 80 Log-loss: 0.046848878264427185\n",
      "Epoch: 52 Batch: 120 Log-loss: 0.029825745150446892\n",
      "Epoch: 52 Batch: 160 Log-loss: 0.028851039707660675\n",
      "Epoch: 52 Batch: 200 Log-loss: 0.026060840114951134\n",
      "Epoch: 52 Batch: 240 Log-loss: 0.054420020431280136\n",
      "Epoch: 52 Batch: 280 Log-loss: 0.028231779113411903\n",
      "Epoch: 52 Batch: 320 Log-loss: 0.03720070421695709\n",
      "Epoch: 52 Batch: 360 Log-loss: 0.035162705928087234\n",
      "Epoch: 52 Batch: 400 Log-loss: 0.04203808680176735\n",
      "Epoch: 52 Batch: 440 Log-loss: 0.024594048038125038\n",
      "Epoch: 52 Batch: 480 Log-loss: 0.05114399269223213\n",
      "Epoch: 52 Batch: 520 Log-loss: 0.02663697488605976\n",
      "Epoch: 52 Batch: 560 Log-loss: 0.03870445862412453\n",
      "Epoch average log-loss: 0.03869093726721725\n",
      "In Epoch: 52, val_loss: 0.04189098836949368, best_val_loss: 0.04133642792965824, best_auc: 0.9875212725198687\n",
      "Epoch: 53 Batch: 0 Log-loss: 0.018904196098446846\n",
      "Epoch: 53 Batch: 40 Log-loss: 0.030681917443871498\n",
      "Epoch: 53 Batch: 80 Log-loss: 0.03913000598549843\n",
      "Epoch: 53 Batch: 120 Log-loss: 0.03699692338705063\n",
      "Epoch: 53 Batch: 160 Log-loss: 0.03550337627530098\n",
      "Epoch: 53 Batch: 200 Log-loss: 0.018390018492937088\n",
      "Epoch: 53 Batch: 240 Log-loss: 0.044983476400375366\n",
      "Epoch: 53 Batch: 280 Log-loss: 0.04424729943275452\n",
      "Epoch: 53 Batch: 320 Log-loss: 0.041728150099515915\n",
      "Epoch: 53 Batch: 360 Log-loss: 0.061310041695833206\n",
      "Epoch: 53 Batch: 400 Log-loss: 0.02767355740070343\n",
      "Epoch: 53 Batch: 440 Log-loss: 0.03128407523036003\n",
      "Epoch: 53 Batch: 480 Log-loss: 0.05440108850598335\n",
      "Epoch: 53 Batch: 520 Log-loss: 0.04841403663158417\n",
      "Epoch: 53 Batch: 560 Log-loss: 0.0415043979883194\n",
      "Epoch average log-loss: 0.03834360716531852\n",
      "In Epoch: 53, val_loss: 0.041636868741094867, best_val_loss: 0.04133642792965824, best_auc: 0.9875212725198687\n",
      "Epoch: 54 Batch: 0 Log-loss: 0.03645516559481621\n",
      "Epoch: 54 Batch: 40 Log-loss: 0.036459844559431076\n",
      "Epoch: 54 Batch: 80 Log-loss: 0.027845969423651695\n",
      "Epoch: 54 Batch: 120 Log-loss: 0.03276093676686287\n",
      "Epoch: 54 Batch: 160 Log-loss: 0.0365142785012722\n",
      "Epoch: 54 Batch: 200 Log-loss: 0.039231929928064346\n",
      "Epoch: 54 Batch: 240 Log-loss: 0.030051998794078827\n",
      "Epoch: 54 Batch: 280 Log-loss: 0.033553022891283035\n",
      "Epoch: 54 Batch: 320 Log-loss: 0.02685566246509552\n",
      "Epoch: 54 Batch: 360 Log-loss: 0.030435003340244293\n",
      "Epoch: 54 Batch: 400 Log-loss: 0.05016274377703667\n",
      "Epoch: 54 Batch: 440 Log-loss: 0.03440823778510094\n",
      "Epoch: 54 Batch: 480 Log-loss: 0.02955774776637554\n",
      "Epoch: 54 Batch: 520 Log-loss: 0.0599568635225296\n",
      "Epoch: 54 Batch: 560 Log-loss: 0.028685739263892174\n",
      "Epoch average log-loss: 0.03821827025219266\n",
      "In Epoch: 54, val_loss: 0.041791228906700055, best_val_loss: 0.04133642792965824, best_auc: 0.9875212725198687\n",
      "Epoch: 55 Batch: 0 Log-loss: 0.04225519299507141\n",
      "Epoch: 55 Batch: 40 Log-loss: 0.043543290346860886\n",
      "Epoch: 55 Batch: 80 Log-loss: 0.043037280440330505\n",
      "Epoch: 55 Batch: 120 Log-loss: 0.02548808418214321\n",
      "Epoch: 55 Batch: 160 Log-loss: 0.043765053153038025\n",
      "Epoch: 55 Batch: 200 Log-loss: 0.036815352737903595\n",
      "Epoch: 55 Batch: 240 Log-loss: 0.03491668775677681\n",
      "Epoch: 55 Batch: 280 Log-loss: 0.028965389356017113\n",
      "Epoch: 55 Batch: 320 Log-loss: 0.039125025272369385\n",
      "Epoch: 55 Batch: 360 Log-loss: 0.043236181139945984\n",
      "Epoch: 55 Batch: 400 Log-loss: 0.021657295525074005\n",
      "Epoch: 55 Batch: 440 Log-loss: 0.03962593525648117\n",
      "Epoch: 55 Batch: 480 Log-loss: 0.04226108267903328\n",
      "Epoch: 55 Batch: 520 Log-loss: 0.04131769761443138\n",
      "Epoch: 55 Batch: 560 Log-loss: 0.042683523148298264\n",
      "Epoch average log-loss: 0.03825734465249947\n",
      "A pre-trained model at model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt has been loaded.\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn3.pt.\n",
      "\n",
      "Choose the torch base model.\n",
      "## Training on fold 4 ##\n",
      "Epoch: 1 Batch: 0 Log-loss: 0.6678733229637146\n",
      "Epoch: 1 Batch: 40 Log-loss: 0.1340811550617218\n",
      "Epoch: 1 Batch: 80 Log-loss: 0.10781078785657883\n",
      "Epoch: 1 Batch: 120 Log-loss: 0.09120456129312515\n",
      "Epoch: 1 Batch: 160 Log-loss: 0.06269624829292297\n",
      "Epoch: 1 Batch: 200 Log-loss: 0.06395510584115982\n",
      "Epoch: 1 Batch: 240 Log-loss: 0.053463827818632126\n",
      "Epoch: 1 Batch: 280 Log-loss: 0.047895003110170364\n",
      "Epoch: 1 Batch: 320 Log-loss: 0.04767526686191559\n",
      "Epoch: 1 Batch: 360 Log-loss: 0.04736713692545891\n",
      "Epoch: 1 Batch: 400 Log-loss: 0.08250319212675095\n",
      "Epoch: 1 Batch: 440 Log-loss: 0.05732085183262825\n",
      "Epoch: 1 Batch: 480 Log-loss: 0.04828775301575661\n",
      "Epoch: 1 Batch: 520 Log-loss: 0.041482072323560715\n",
      "Epoch: 1 Batch: 560 Log-loss: 0.06490469723939896\n",
      "Epoch average log-loss: 0.0837099615417953\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 1, val_loss: 0.05127793990867164, best_val_loss: 0.05127793990867164, best_auc: 0.9730338136563331\n",
      "Epoch: 2 Batch: 0 Log-loss: 0.05889884755015373\n",
      "Epoch: 2 Batch: 40 Log-loss: 0.037463460117578506\n",
      "Epoch: 2 Batch: 80 Log-loss: 0.04882216453552246\n",
      "Epoch: 2 Batch: 120 Log-loss: 0.05316157639026642\n",
      "Epoch: 2 Batch: 160 Log-loss: 0.048731788992881775\n",
      "Epoch: 2 Batch: 200 Log-loss: 0.05614776536822319\n",
      "Epoch: 2 Batch: 240 Log-loss: 0.04607542231678963\n",
      "Epoch: 2 Batch: 280 Log-loss: 0.05776386335492134\n",
      "Epoch: 2 Batch: 320 Log-loss: 0.04996027052402496\n",
      "Epoch: 2 Batch: 360 Log-loss: 0.05078357085585594\n",
      "Epoch: 2 Batch: 400 Log-loss: 0.04705994203686714\n",
      "Epoch: 2 Batch: 440 Log-loss: 0.045036185532808304\n",
      "Epoch: 2 Batch: 480 Log-loss: 0.044639527797698975\n",
      "Epoch: 2 Batch: 520 Log-loss: 0.0390985831618309\n",
      "Epoch: 2 Batch: 560 Log-loss: 0.04067261889576912\n",
      "Epoch average log-loss: 0.05152791665667402\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 2, val_loss: 0.048290492394952006, best_val_loss: 0.048290492394952006, best_auc: 0.9777229360337211\n",
      "Epoch: 3 Batch: 0 Log-loss: 0.052411437034606934\n",
      "Epoch: 3 Batch: 40 Log-loss: 0.043852608650922775\n",
      "Epoch: 3 Batch: 80 Log-loss: 0.05400644615292549\n",
      "Epoch: 3 Batch: 120 Log-loss: 0.05392082408070564\n",
      "Epoch: 3 Batch: 160 Log-loss: 0.0500328354537487\n",
      "Epoch: 3 Batch: 200 Log-loss: 0.04457227885723114\n",
      "Epoch: 3 Batch: 240 Log-loss: 0.03641733154654503\n",
      "Epoch: 3 Batch: 280 Log-loss: 0.05586570128798485\n",
      "Epoch: 3 Batch: 320 Log-loss: 0.04759328067302704\n",
      "Epoch: 3 Batch: 360 Log-loss: 0.03185274079442024\n",
      "Epoch: 3 Batch: 400 Log-loss: 0.03681528940796852\n",
      "Epoch: 3 Batch: 440 Log-loss: 0.036903511732816696\n",
      "Epoch: 3 Batch: 480 Log-loss: 0.05503202602267265\n",
      "Epoch: 3 Batch: 520 Log-loss: 0.04312342777848244\n",
      "Epoch: 3 Batch: 560 Log-loss: 0.05000970885157585\n",
      "Epoch average log-loss: 0.048758203177047627\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 3, val_loss: 0.04726810143006565, best_val_loss: 0.04726810143006565, best_auc: 0.9813736771281066\n",
      "Epoch: 4 Batch: 0 Log-loss: 0.04970235005021095\n",
      "Epoch: 4 Batch: 40 Log-loss: 0.06155872717499733\n",
      "Epoch: 4 Batch: 80 Log-loss: 0.0636257752776146\n",
      "Epoch: 4 Batch: 120 Log-loss: 0.04680366441607475\n",
      "Epoch: 4 Batch: 160 Log-loss: 0.06599389761686325\n",
      "Epoch: 4 Batch: 200 Log-loss: 0.04851287603378296\n",
      "Epoch: 4 Batch: 240 Log-loss: 0.04546723887324333\n",
      "Epoch: 4 Batch: 280 Log-loss: 0.05315685272216797\n",
      "Epoch: 4 Batch: 320 Log-loss: 0.04501115903258324\n",
      "Epoch: 4 Batch: 360 Log-loss: 0.04362824186682701\n",
      "Epoch: 4 Batch: 400 Log-loss: 0.05884522572159767\n",
      "Epoch: 4 Batch: 440 Log-loss: 0.05023306980729103\n",
      "Epoch: 4 Batch: 480 Log-loss: 0.02766578085720539\n",
      "Epoch: 4 Batch: 520 Log-loss: 0.032626923173666\n",
      "Epoch: 4 Batch: 560 Log-loss: 0.04484901949763298\n",
      "Epoch average log-loss: 0.04695622375501054\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 4, val_loss: 0.044732988995951295, best_val_loss: 0.044732988995951295, best_auc: 0.98449420242315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Batch: 0 Log-loss: 0.02862723357975483\n",
      "Epoch: 5 Batch: 40 Log-loss: 0.05800408124923706\n",
      "Epoch: 5 Batch: 80 Log-loss: 0.04098288714885712\n",
      "Epoch: 5 Batch: 120 Log-loss: 0.043399497866630554\n",
      "Epoch: 5 Batch: 160 Log-loss: 0.040821027010679245\n",
      "Epoch: 5 Batch: 200 Log-loss: 0.047052279114723206\n",
      "Epoch: 5 Batch: 240 Log-loss: 0.03904081508517265\n",
      "Epoch: 5 Batch: 280 Log-loss: 0.029493553563952446\n",
      "Epoch: 5 Batch: 320 Log-loss: 0.045554619282484055\n",
      "Epoch: 5 Batch: 360 Log-loss: 0.05004723370075226\n",
      "Epoch: 5 Batch: 400 Log-loss: 0.04886845126748085\n",
      "Epoch: 5 Batch: 440 Log-loss: 0.030934540554881096\n",
      "Epoch: 5 Batch: 480 Log-loss: 0.030163967981934547\n",
      "Epoch: 5 Batch: 520 Log-loss: 0.04704664275050163\n",
      "Epoch: 5 Batch: 560 Log-loss: 0.06035754829645157\n",
      "Epoch average log-loss: 0.045731993741355836\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 5, val_loss: 0.04354338793246459, best_val_loss: 0.04354338793246459, best_auc: 0.9859634410360395\n",
      "Epoch: 6 Batch: 0 Log-loss: 0.04534727334976196\n",
      "Epoch: 6 Batch: 40 Log-loss: 0.05002230033278465\n",
      "Epoch: 6 Batch: 80 Log-loss: 0.027865076437592506\n",
      "Epoch: 6 Batch: 120 Log-loss: 0.049345824867486954\n",
      "Epoch: 6 Batch: 160 Log-loss: 0.05009659007191658\n",
      "Epoch: 6 Batch: 200 Log-loss: 0.03552338853478432\n",
      "Epoch: 6 Batch: 240 Log-loss: 0.060842081904411316\n",
      "Epoch: 6 Batch: 280 Log-loss: 0.04118881747126579\n",
      "Epoch: 6 Batch: 320 Log-loss: 0.041840389370918274\n",
      "Epoch: 6 Batch: 360 Log-loss: 0.04005371779203415\n",
      "Epoch: 6 Batch: 400 Log-loss: 0.03342363238334656\n",
      "Epoch: 6 Batch: 440 Log-loss: 0.07643315196037292\n",
      "Epoch: 6 Batch: 480 Log-loss: 0.04486934840679169\n",
      "Epoch: 6 Batch: 520 Log-loss: 0.03938157856464386\n",
      "Epoch: 6 Batch: 560 Log-loss: 0.05403412878513336\n",
      "Epoch average log-loss: 0.044735546676175936\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 6, val_loss: 0.04320851051509419, best_val_loss: 0.04320851051509419, best_auc: 0.9862046427499888\n",
      "Epoch: 7 Batch: 0 Log-loss: 0.04915228486061096\n",
      "Epoch: 7 Batch: 40 Log-loss: 0.04614713788032532\n",
      "Epoch: 7 Batch: 80 Log-loss: 0.041184354573488235\n",
      "Epoch: 7 Batch: 120 Log-loss: 0.03931628540158272\n",
      "Epoch: 7 Batch: 160 Log-loss: 0.043427299708127975\n",
      "Epoch: 7 Batch: 200 Log-loss: 0.04197094962000847\n",
      "Epoch: 7 Batch: 240 Log-loss: 0.03579839691519737\n",
      "Epoch: 7 Batch: 280 Log-loss: 0.04717692360281944\n",
      "Epoch: 7 Batch: 320 Log-loss: 0.03296060860157013\n",
      "Epoch: 7 Batch: 360 Log-loss: 0.05340373143553734\n",
      "Epoch: 7 Batch: 400 Log-loss: 0.04022659361362457\n",
      "Epoch: 7 Batch: 440 Log-loss: 0.06200491264462471\n",
      "Epoch: 7 Batch: 480 Log-loss: 0.03656771406531334\n",
      "Epoch: 7 Batch: 520 Log-loss: 0.04194585978984833\n",
      "Epoch: 7 Batch: 560 Log-loss: 0.049837734550237656\n",
      "Epoch average log-loss: 0.04404095309187791\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 7, val_loss: 0.04313504944667046, best_val_loss: 0.04313504944667046, best_auc: 0.9865761422802016\n",
      "Epoch: 8 Batch: 0 Log-loss: 0.030837439000606537\n",
      "Epoch: 8 Batch: 40 Log-loss: 0.04901933670043945\n",
      "Epoch: 8 Batch: 80 Log-loss: 0.05064614117145538\n",
      "Epoch: 8 Batch: 120 Log-loss: 0.05082212761044502\n",
      "Epoch: 8 Batch: 160 Log-loss: 0.0594235360622406\n",
      "Epoch: 8 Batch: 200 Log-loss: 0.04849700257182121\n",
      "Epoch: 8 Batch: 240 Log-loss: 0.05202915146946907\n",
      "Epoch: 8 Batch: 280 Log-loss: 0.038307834416627884\n",
      "Epoch: 8 Batch: 320 Log-loss: 0.027839845046401024\n",
      "Epoch: 8 Batch: 360 Log-loss: 0.06848631054162979\n",
      "Epoch: 8 Batch: 400 Log-loss: 0.054905232042074203\n",
      "Epoch: 8 Batch: 440 Log-loss: 0.03913800045847893\n",
      "Epoch: 8 Batch: 480 Log-loss: 0.040734078735113144\n",
      "Epoch: 8 Batch: 520 Log-loss: 0.057457830756902695\n",
      "Epoch: 8 Batch: 560 Log-loss: 0.02899041585624218\n",
      "Epoch average log-loss: 0.04355298111414803\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 8, val_loss: 0.04286686032299098, best_val_loss: 0.04286686032299098, best_auc: 0.9869539668584449\n",
      "Epoch: 9 Batch: 0 Log-loss: 0.05133206024765968\n",
      "Epoch: 9 Batch: 40 Log-loss: 0.04887871816754341\n",
      "Epoch: 9 Batch: 80 Log-loss: 0.04486456513404846\n",
      "Epoch: 9 Batch: 120 Log-loss: 0.03610246255993843\n",
      "Epoch: 9 Batch: 160 Log-loss: 0.04154016450047493\n",
      "Epoch: 9 Batch: 200 Log-loss: 0.04349808767437935\n",
      "Epoch: 9 Batch: 240 Log-loss: 0.026958787813782692\n",
      "Epoch: 9 Batch: 280 Log-loss: 0.05315117910504341\n",
      "Epoch: 9 Batch: 320 Log-loss: 0.048553600907325745\n",
      "Epoch: 9 Batch: 360 Log-loss: 0.06424877792596817\n",
      "Epoch: 9 Batch: 400 Log-loss: 0.04194707050919533\n",
      "Epoch: 9 Batch: 440 Log-loss: 0.04251641035079956\n",
      "Epoch: 9 Batch: 480 Log-loss: 0.056218866258859634\n",
      "Epoch: 9 Batch: 520 Log-loss: 0.05166098475456238\n",
      "Epoch: 9 Batch: 560 Log-loss: 0.03783407807350159\n",
      "Epoch average log-loss: 0.042918556190228885\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 9, val_loss: 0.0424126255748331, best_val_loss: 0.0424126255748331, best_auc: 0.9873253026596763\n",
      "Epoch: 10 Batch: 0 Log-loss: 0.029664844274520874\n",
      "Epoch: 10 Batch: 40 Log-loss: 0.03838152065873146\n",
      "Epoch: 10 Batch: 80 Log-loss: 0.053899381309747696\n",
      "Epoch: 10 Batch: 120 Log-loss: 0.029023533686995506\n",
      "Epoch: 10 Batch: 160 Log-loss: 0.04811796918511391\n",
      "Epoch: 10 Batch: 200 Log-loss: 0.04152867570519447\n",
      "Epoch: 10 Batch: 240 Log-loss: 0.043385133147239685\n",
      "Epoch: 10 Batch: 280 Log-loss: 0.0404231958091259\n",
      "Epoch: 10 Batch: 320 Log-loss: 0.035916607826948166\n",
      "Epoch: 10 Batch: 360 Log-loss: 0.04773014411330223\n",
      "Epoch: 10 Batch: 400 Log-loss: 0.052330825477838516\n",
      "Epoch: 10 Batch: 440 Log-loss: 0.06171246990561485\n",
      "Epoch: 10 Batch: 480 Log-loss: 0.041138190776109695\n",
      "Epoch: 10 Batch: 520 Log-loss: 0.05539625883102417\n",
      "Epoch: 10 Batch: 560 Log-loss: 0.05047033727169037\n",
      "Epoch average log-loss: 0.04268900927355779\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 10, val_loss: 0.04231207300572167, best_val_loss: 0.04231207300572167, best_auc: 0.9882447240549722\n",
      "Epoch: 11 Batch: 0 Log-loss: 0.0347999706864357\n",
      "Epoch: 11 Batch: 40 Log-loss: 0.030656568706035614\n",
      "Epoch: 11 Batch: 80 Log-loss: 0.07154631614685059\n",
      "Epoch: 11 Batch: 120 Log-loss: 0.04879191145300865\n",
      "Epoch: 11 Batch: 160 Log-loss: 0.0360407792031765\n",
      "Epoch: 11 Batch: 200 Log-loss: 0.04929167032241821\n",
      "Epoch: 11 Batch: 240 Log-loss: 0.036122821271419525\n",
      "Epoch: 11 Batch: 280 Log-loss: 0.03954274207353592\n",
      "Epoch: 11 Batch: 320 Log-loss: 0.03244912996888161\n",
      "Epoch: 11 Batch: 360 Log-loss: 0.03555808588862419\n",
      "Epoch: 11 Batch: 400 Log-loss: 0.05578051134943962\n",
      "Epoch: 11 Batch: 440 Log-loss: 0.04688834026455879\n",
      "Epoch: 11 Batch: 480 Log-loss: 0.0503619909286499\n",
      "Epoch: 11 Batch: 520 Log-loss: 0.046430617570877075\n",
      "Epoch: 11 Batch: 560 Log-loss: 0.04712491109967232\n",
      "Epoch average log-loss: 0.042231101461220534\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 11, val_loss: 0.041937683711311964, best_val_loss: 0.041937683711311964, best_auc: 0.9884538687738078\n",
      "Epoch: 12 Batch: 0 Log-loss: 0.043954525142908096\n",
      "Epoch: 12 Batch: 40 Log-loss: 0.03668617084622383\n",
      "Epoch: 12 Batch: 80 Log-loss: 0.05005558952689171\n",
      "Epoch: 12 Batch: 120 Log-loss: 0.02842283807694912\n",
      "Epoch: 12 Batch: 160 Log-loss: 0.04493379965424538\n",
      "Epoch: 12 Batch: 200 Log-loss: 0.053138136863708496\n",
      "Epoch: 12 Batch: 240 Log-loss: 0.046876341104507446\n",
      "Epoch: 12 Batch: 280 Log-loss: 0.04608507081866264\n",
      "Epoch: 12 Batch: 320 Log-loss: 0.06157637760043144\n",
      "Epoch: 12 Batch: 360 Log-loss: 0.05064116418361664\n",
      "Epoch: 12 Batch: 400 Log-loss: 0.03308015689253807\n",
      "Epoch: 12 Batch: 440 Log-loss: 0.03816329315304756\n",
      "Epoch: 12 Batch: 480 Log-loss: 0.039618924260139465\n",
      "Epoch: 12 Batch: 520 Log-loss: 0.03344476595520973\n",
      "Epoch: 12 Batch: 560 Log-loss: 0.03861639276146889\n",
      "Epoch average log-loss: 0.041862884386708696\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 12, val_loss: 0.04159897981095593, best_val_loss: 0.04159897981095593, best_auc: 0.9881554697067783\n",
      "Epoch: 13 Batch: 0 Log-loss: 0.03876001387834549\n",
      "Epoch: 13 Batch: 40 Log-loss: 0.036244507879018784\n",
      "Epoch: 13 Batch: 80 Log-loss: 0.030260121449828148\n",
      "Epoch: 13 Batch: 120 Log-loss: 0.06463327258825302\n",
      "Epoch: 13 Batch: 160 Log-loss: 0.041780564934015274\n",
      "Epoch: 13 Batch: 200 Log-loss: 0.05023437738418579\n",
      "Epoch: 13 Batch: 240 Log-loss: 0.03282855823636055\n",
      "Epoch: 13 Batch: 280 Log-loss: 0.036211032420396805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 Batch: 320 Log-loss: 0.03228360414505005\n",
      "Epoch: 13 Batch: 360 Log-loss: 0.04339493811130524\n",
      "Epoch: 13 Batch: 400 Log-loss: 0.051560044288635254\n",
      "Epoch: 13 Batch: 440 Log-loss: 0.04266371950507164\n",
      "Epoch: 13 Batch: 480 Log-loss: 0.06025100126862526\n",
      "Epoch: 13 Batch: 520 Log-loss: 0.03210156410932541\n",
      "Epoch: 13 Batch: 560 Log-loss: 0.04835810512304306\n",
      "Epoch average log-loss: 0.041417966250862394\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 13, val_loss: 0.04147807552197166, best_val_loss: 0.04147807552197166, best_auc: 0.9884452844855526\n",
      "Epoch: 14 Batch: 0 Log-loss: 0.043237220495939255\n",
      "Epoch: 14 Batch: 40 Log-loss: 0.04092416167259216\n",
      "Epoch: 14 Batch: 80 Log-loss: 0.04390789195895195\n",
      "Epoch: 14 Batch: 120 Log-loss: 0.08547388762235641\n",
      "Epoch: 14 Batch: 160 Log-loss: 0.03801707178354263\n",
      "Epoch: 14 Batch: 200 Log-loss: 0.03999768942594528\n",
      "Epoch: 14 Batch: 240 Log-loss: 0.04601162672042847\n",
      "Epoch: 14 Batch: 280 Log-loss: 0.05602709576487541\n",
      "Epoch: 14 Batch: 320 Log-loss: 0.028174109756946564\n",
      "Epoch: 14 Batch: 360 Log-loss: 0.049330729991197586\n",
      "Epoch: 14 Batch: 400 Log-loss: 0.036509014666080475\n",
      "Epoch: 14 Batch: 440 Log-loss: 0.04325898364186287\n",
      "Epoch: 14 Batch: 480 Log-loss: 0.02997550368309021\n",
      "Epoch: 14 Batch: 520 Log-loss: 0.03733259439468384\n",
      "Epoch: 14 Batch: 560 Log-loss: 0.04347684234380722\n",
      "Epoch average log-loss: 0.04133865283802152\n",
      "In Epoch: 14, val_loss: 0.042274357945708246, best_val_loss: 0.04147807552197166, best_auc: 0.9884452844855526\n",
      "Epoch: 15 Batch: 0 Log-loss: 0.041560098528862\n",
      "Epoch: 15 Batch: 40 Log-loss: 0.051307257264852524\n",
      "Epoch: 15 Batch: 80 Log-loss: 0.060930121690034866\n",
      "Epoch: 15 Batch: 120 Log-loss: 0.04291294887661934\n",
      "Epoch: 15 Batch: 160 Log-loss: 0.03542535379528999\n",
      "Epoch: 15 Batch: 200 Log-loss: 0.035850342363119125\n",
      "Epoch: 15 Batch: 240 Log-loss: 0.04467179998755455\n",
      "Epoch: 15 Batch: 280 Log-loss: 0.029476338997483253\n",
      "Epoch: 15 Batch: 320 Log-loss: 0.04701307415962219\n",
      "Epoch: 15 Batch: 360 Log-loss: 0.038808148354291916\n",
      "Epoch: 15 Batch: 400 Log-loss: 0.04286004602909088\n",
      "Epoch: 15 Batch: 440 Log-loss: 0.03872930631041527\n",
      "Epoch: 15 Batch: 480 Log-loss: 0.046774595975875854\n",
      "Epoch: 15 Batch: 520 Log-loss: 0.05533517524600029\n",
      "Epoch: 15 Batch: 560 Log-loss: 0.04020915552973747\n",
      "Epoch average log-loss: 0.04124274093192071\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 15, val_loss: 0.04116498803208649, best_val_loss: 0.04116498803208649, best_auc: 0.9885136577140726\n",
      "Epoch: 16 Batch: 0 Log-loss: 0.04926962032914162\n",
      "Epoch: 16 Batch: 40 Log-loss: 0.027636786922812462\n",
      "Epoch: 16 Batch: 80 Log-loss: 0.06214682757854462\n",
      "Epoch: 16 Batch: 120 Log-loss: 0.03580579534173012\n",
      "Epoch: 16 Batch: 160 Log-loss: 0.04208962619304657\n",
      "Epoch: 16 Batch: 200 Log-loss: 0.04719390347599983\n",
      "Epoch: 16 Batch: 240 Log-loss: 0.03733773157000542\n",
      "Epoch: 16 Batch: 280 Log-loss: 0.02793031372129917\n",
      "Epoch: 16 Batch: 320 Log-loss: 0.03843052685260773\n",
      "Epoch: 16 Batch: 360 Log-loss: 0.042914506047964096\n",
      "Epoch: 16 Batch: 400 Log-loss: 0.03761545941233635\n",
      "Epoch: 16 Batch: 440 Log-loss: 0.03737546131014824\n",
      "Epoch: 16 Batch: 480 Log-loss: 0.033797845244407654\n",
      "Epoch: 16 Batch: 520 Log-loss: 0.033978208899497986\n",
      "Epoch: 16 Batch: 560 Log-loss: 0.04278646409511566\n",
      "Epoch average log-loss: 0.04078136338918869\n",
      "In Epoch: 16, val_loss: 0.041327830271921155, best_val_loss: 0.04116498803208649, best_auc: 0.9885136577140726\n",
      "Epoch: 17 Batch: 0 Log-loss: 0.043289050459861755\n",
      "Epoch: 17 Batch: 40 Log-loss: 0.04454612359404564\n",
      "Epoch: 17 Batch: 80 Log-loss: 0.04825292155146599\n",
      "Epoch: 17 Batch: 120 Log-loss: 0.03764205798506737\n",
      "Epoch: 17 Batch: 160 Log-loss: 0.054644230753183365\n",
      "Epoch: 17 Batch: 200 Log-loss: 0.03995596244931221\n",
      "Epoch: 17 Batch: 240 Log-loss: 0.031243838369846344\n",
      "Epoch: 17 Batch: 280 Log-loss: 0.04737390577793121\n",
      "Epoch: 17 Batch: 320 Log-loss: 0.054997265338897705\n",
      "Epoch: 17 Batch: 360 Log-loss: 0.03440332040190697\n",
      "Epoch: 17 Batch: 400 Log-loss: 0.05003955587744713\n",
      "Epoch: 17 Batch: 440 Log-loss: 0.04139033332467079\n",
      "Epoch: 17 Batch: 480 Log-loss: 0.04637160897254944\n",
      "Epoch: 17 Batch: 520 Log-loss: 0.04493744298815727\n",
      "Epoch: 17 Batch: 560 Log-loss: 0.04113294556736946\n",
      "Epoch average log-loss: 0.040814400178247265\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 17, val_loss: 0.040956587708652646, best_val_loss: 0.040956587708652646, best_auc: 0.9887919924243538\n",
      "Epoch: 18 Batch: 0 Log-loss: 0.04440738633275032\n",
      "Epoch: 18 Batch: 40 Log-loss: 0.033456072211265564\n",
      "Epoch: 18 Batch: 80 Log-loss: 0.07186470925807953\n",
      "Epoch: 18 Batch: 120 Log-loss: 0.04614688456058502\n",
      "Epoch: 18 Batch: 160 Log-loss: 0.03129120543599129\n",
      "Epoch: 18 Batch: 200 Log-loss: 0.052780698984861374\n",
      "Epoch: 18 Batch: 240 Log-loss: 0.03925931453704834\n",
      "Epoch: 18 Batch: 280 Log-loss: 0.06720725446939468\n",
      "Epoch: 18 Batch: 320 Log-loss: 0.03587362542748451\n",
      "Epoch: 18 Batch: 360 Log-loss: 0.04546992853283882\n",
      "Epoch: 18 Batch: 400 Log-loss: 0.04280441999435425\n",
      "Epoch: 18 Batch: 440 Log-loss: 0.04513480141758919\n",
      "Epoch: 18 Batch: 480 Log-loss: 0.025696223601698875\n",
      "Epoch: 18 Batch: 520 Log-loss: 0.04181979224085808\n",
      "Epoch: 18 Batch: 560 Log-loss: 0.03326505795121193\n",
      "Epoch average log-loss: 0.04050058340986392\n",
      "In Epoch: 18, val_loss: 0.0412030987010859, best_val_loss: 0.040956587708652646, best_auc: 0.9887919924243538\n",
      "Epoch: 19 Batch: 0 Log-loss: 0.04200361669063568\n",
      "Epoch: 19 Batch: 40 Log-loss: 0.035430461168289185\n",
      "Epoch: 19 Batch: 80 Log-loss: 0.03128654137253761\n",
      "Epoch: 19 Batch: 120 Log-loss: 0.0428350456058979\n",
      "Epoch: 19 Batch: 160 Log-loss: 0.035996485501527786\n",
      "Epoch: 19 Batch: 200 Log-loss: 0.04477307200431824\n",
      "Epoch: 19 Batch: 240 Log-loss: 0.05419488251209259\n",
      "Epoch: 19 Batch: 280 Log-loss: 0.035050131380558014\n",
      "Epoch: 19 Batch: 320 Log-loss: 0.04585830494761467\n",
      "Epoch: 19 Batch: 360 Log-loss: 0.04782594367861748\n",
      "Epoch: 19 Batch: 400 Log-loss: 0.039573729038238525\n",
      "Epoch: 19 Batch: 440 Log-loss: 0.02882981300354004\n",
      "Epoch: 19 Batch: 480 Log-loss: 0.028528163209557533\n",
      "Epoch: 19 Batch: 520 Log-loss: 0.0437326543033123\n",
      "Epoch: 19 Batch: 560 Log-loss: 0.04734750837087631\n",
      "Epoch average log-loss: 0.040389848149581145\n",
      "In Epoch: 19, val_loss: 0.0409706477613395, best_val_loss: 0.040956587708652646, best_auc: 0.9887919924243538\n",
      "Epoch: 20 Batch: 0 Log-loss: 0.035239700227975845\n",
      "Epoch: 20 Batch: 40 Log-loss: 0.035965509712696075\n",
      "Epoch: 20 Batch: 80 Log-loss: 0.029039176180958748\n",
      "Epoch: 20 Batch: 120 Log-loss: 0.03693952038884163\n",
      "Epoch: 20 Batch: 160 Log-loss: 0.03725898638367653\n",
      "Epoch: 20 Batch: 200 Log-loss: 0.0338483490049839\n",
      "Epoch: 20 Batch: 240 Log-loss: 0.049336228519678116\n",
      "Epoch: 20 Batch: 280 Log-loss: 0.04159928113222122\n",
      "Epoch: 20 Batch: 320 Log-loss: 0.0439159981906414\n",
      "Epoch: 20 Batch: 360 Log-loss: 0.04080797731876373\n",
      "Epoch: 20 Batch: 400 Log-loss: 0.04513805732131004\n",
      "Epoch: 20 Batch: 440 Log-loss: 0.03898018226027489\n",
      "Epoch: 20 Batch: 480 Log-loss: 0.05252659320831299\n",
      "Epoch: 20 Batch: 520 Log-loss: 0.037868913263082504\n",
      "Epoch: 20 Batch: 560 Log-loss: 0.03379131481051445\n",
      "Epoch average log-loss: 0.0402522857101368\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 20, val_loss: 0.04030376890709323, best_val_loss: 0.04030376890709323, best_auc: 0.9890797255256679\n",
      "Epoch: 21 Batch: 0 Log-loss: 0.03819691762328148\n",
      "Epoch: 21 Batch: 40 Log-loss: 0.059091825038194656\n",
      "Epoch: 21 Batch: 80 Log-loss: 0.04520377144217491\n",
      "Epoch: 21 Batch: 120 Log-loss: 0.03655395284295082\n",
      "Epoch: 21 Batch: 160 Log-loss: 0.06547322869300842\n",
      "Epoch: 21 Batch: 200 Log-loss: 0.04278445616364479\n",
      "Epoch: 21 Batch: 240 Log-loss: 0.05070643126964569\n",
      "Epoch: 21 Batch: 280 Log-loss: 0.029159529134631157\n",
      "Epoch: 21 Batch: 320 Log-loss: 0.03181855008006096\n",
      "Epoch: 21 Batch: 360 Log-loss: 0.03502577170729637\n",
      "Epoch: 21 Batch: 400 Log-loss: 0.03195875138044357\n",
      "Epoch: 21 Batch: 440 Log-loss: 0.038635559380054474\n",
      "Epoch: 21 Batch: 480 Log-loss: 0.029981277883052826\n",
      "Epoch: 21 Batch: 520 Log-loss: 0.03902792930603027\n",
      "Epoch: 21 Batch: 560 Log-loss: 0.04363400489091873\n",
      "Epoch average log-loss: 0.040169984474778174\n",
      "In Epoch: 21, val_loss: 0.04065369383478557, best_val_loss: 0.04030376890709323, best_auc: 0.9890797255256679\n",
      "Epoch: 22 Batch: 0 Log-loss: 0.03010251186788082\n",
      "Epoch: 22 Batch: 40 Log-loss: 0.03900374099612236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 Batch: 80 Log-loss: 0.023718489333987236\n",
      "Epoch: 22 Batch: 120 Log-loss: 0.050586849451065063\n",
      "Epoch: 22 Batch: 160 Log-loss: 0.04478336498141289\n",
      "Epoch: 22 Batch: 200 Log-loss: 0.05544924736022949\n",
      "Epoch: 22 Batch: 240 Log-loss: 0.043481845408678055\n",
      "Epoch: 22 Batch: 280 Log-loss: 0.0417424738407135\n",
      "Epoch: 22 Batch: 320 Log-loss: 0.04553303122520447\n",
      "Epoch: 22 Batch: 360 Log-loss: 0.03806564211845398\n",
      "Epoch: 22 Batch: 400 Log-loss: 0.0421479307115078\n",
      "Epoch: 22 Batch: 440 Log-loss: 0.04149748012423515\n",
      "Epoch: 22 Batch: 480 Log-loss: 0.03341905400156975\n",
      "Epoch: 22 Batch: 520 Log-loss: 0.03355581313371658\n",
      "Epoch: 22 Batch: 560 Log-loss: 0.04663657769560814\n",
      "Epoch average log-loss: 0.03992026270965913\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 22, val_loss: 0.040286046444048436, best_val_loss: 0.040286046444048436, best_auc: 0.9889374024185579\n",
      "Epoch: 23 Batch: 0 Log-loss: 0.034281231462955475\n",
      "Epoch: 23 Batch: 40 Log-loss: 0.029296962544322014\n",
      "Epoch: 23 Batch: 80 Log-loss: 0.03095093183219433\n",
      "Epoch: 23 Batch: 120 Log-loss: 0.03534560278058052\n",
      "Epoch: 23 Batch: 160 Log-loss: 0.0445224829018116\n",
      "Epoch: 23 Batch: 200 Log-loss: 0.05313162878155708\n",
      "Epoch: 23 Batch: 240 Log-loss: 0.04288473352789879\n",
      "Epoch: 23 Batch: 280 Log-loss: 0.03846726194024086\n",
      "Epoch: 23 Batch: 320 Log-loss: 0.038133762776851654\n",
      "Epoch: 23 Batch: 360 Log-loss: 0.04338221624493599\n",
      "Epoch: 23 Batch: 400 Log-loss: 0.038611654192209244\n",
      "Epoch: 23 Batch: 440 Log-loss: 0.03945494070649147\n",
      "Epoch: 23 Batch: 480 Log-loss: 0.035812634974718094\n",
      "Epoch: 23 Batch: 520 Log-loss: 0.0360373817384243\n",
      "Epoch: 23 Batch: 560 Log-loss: 0.047036074101924896\n",
      "Epoch average log-loss: 0.03973310627043247\n",
      "In Epoch: 23, val_loss: 0.04088594354777501, best_val_loss: 0.040286046444048436, best_auc: 0.9889374024185579\n",
      "Epoch: 24 Batch: 0 Log-loss: 0.04170021042227745\n",
      "Epoch: 24 Batch: 40 Log-loss: 0.04192574322223663\n",
      "Epoch: 24 Batch: 80 Log-loss: 0.05867527425289154\n",
      "Epoch: 24 Batch: 120 Log-loss: 0.03638726845383644\n",
      "Epoch: 24 Batch: 160 Log-loss: 0.02730984427034855\n",
      "Epoch: 24 Batch: 200 Log-loss: 0.05145848169922829\n",
      "Epoch: 24 Batch: 240 Log-loss: 0.04567966237664223\n",
      "Epoch: 24 Batch: 280 Log-loss: 0.03734763339161873\n",
      "Epoch: 24 Batch: 320 Log-loss: 0.038338180631399155\n",
      "Epoch: 24 Batch: 360 Log-loss: 0.030563892796635628\n",
      "Epoch: 24 Batch: 400 Log-loss: 0.03840542212128639\n",
      "Epoch: 24 Batch: 440 Log-loss: 0.045935019850730896\n",
      "Epoch: 24 Batch: 480 Log-loss: 0.026508690789341927\n",
      "Epoch: 24 Batch: 520 Log-loss: 0.045188724994659424\n",
      "Epoch: 24 Batch: 560 Log-loss: 0.027892054989933968\n",
      "Epoch average log-loss: 0.03970441981218755\n",
      "In Epoch: 24, val_loss: 0.04057316907623857, best_val_loss: 0.040286046444048436, best_auc: 0.9889374024185579\n",
      "Epoch: 25 Batch: 0 Log-loss: 0.0479561947286129\n",
      "Epoch: 25 Batch: 40 Log-loss: 0.04425211250782013\n",
      "Epoch: 25 Batch: 80 Log-loss: 0.04209186136722565\n",
      "Epoch: 25 Batch: 120 Log-loss: 0.03828767314553261\n",
      "Epoch: 25 Batch: 160 Log-loss: 0.05837927758693695\n",
      "Epoch: 25 Batch: 200 Log-loss: 0.03368210420012474\n",
      "Epoch: 25 Batch: 240 Log-loss: 0.03408465161919594\n",
      "Epoch: 25 Batch: 280 Log-loss: 0.037529412657022476\n",
      "Epoch: 25 Batch: 320 Log-loss: 0.024508878588676453\n",
      "Epoch: 25 Batch: 360 Log-loss: 0.03594712167978287\n",
      "Epoch: 25 Batch: 400 Log-loss: 0.041023220866918564\n",
      "Epoch: 25 Batch: 440 Log-loss: 0.043552543967962265\n",
      "Epoch: 25 Batch: 480 Log-loss: 0.042053502053022385\n",
      "Epoch: 25 Batch: 520 Log-loss: 0.030883820727467537\n",
      "Epoch: 25 Batch: 560 Log-loss: 0.04124664142727852\n",
      "Epoch average log-loss: 0.03965491425645139\n",
      "In Epoch: 25, val_loss: 0.041094626751024106, best_val_loss: 0.040286046444048436, best_auc: 0.9889374024185579\n",
      "Epoch: 26 Batch: 0 Log-loss: 0.04395320639014244\n",
      "Epoch: 26 Batch: 40 Log-loss: 0.03588917851448059\n",
      "Epoch: 26 Batch: 80 Log-loss: 0.03255593776702881\n",
      "Epoch: 26 Batch: 120 Log-loss: 0.021095218136906624\n",
      "Epoch: 26 Batch: 160 Log-loss: 0.033370908349752426\n",
      "Epoch: 26 Batch: 200 Log-loss: 0.02791411429643631\n",
      "Epoch: 26 Batch: 240 Log-loss: 0.03702852129936218\n",
      "Epoch: 26 Batch: 280 Log-loss: 0.06219818815588951\n",
      "Epoch: 26 Batch: 320 Log-loss: 0.04229922965168953\n",
      "Epoch: 26 Batch: 360 Log-loss: 0.04074702039361\n",
      "Epoch: 26 Batch: 400 Log-loss: 0.038668442517519\n",
      "Epoch: 26 Batch: 440 Log-loss: 0.048084914684295654\n",
      "Epoch: 26 Batch: 480 Log-loss: 0.03269991651177406\n",
      "Epoch: 26 Batch: 520 Log-loss: 0.041752755641937256\n",
      "Epoch: 26 Batch: 560 Log-loss: 0.04169851914048195\n",
      "Epoch average log-loss: 0.039643890209429496\n",
      "In Epoch: 26, val_loss: 0.040828366917801735, best_val_loss: 0.040286046444048436, best_auc: 0.9889374024185579\n",
      "Epoch: 27 Batch: 0 Log-loss: 0.0557357482612133\n",
      "Epoch: 27 Batch: 40 Log-loss: 0.04854830726981163\n",
      "Epoch: 27 Batch: 80 Log-loss: 0.054245203733444214\n",
      "Epoch: 27 Batch: 120 Log-loss: 0.040370017290115356\n",
      "Epoch: 27 Batch: 160 Log-loss: 0.034300994127988815\n",
      "Epoch: 27 Batch: 200 Log-loss: 0.027846818789839745\n",
      "Epoch: 27 Batch: 240 Log-loss: 0.04306196793913841\n",
      "Epoch: 27 Batch: 280 Log-loss: 0.03645618259906769\n",
      "Epoch: 27 Batch: 320 Log-loss: 0.038875482976436615\n",
      "Epoch: 27 Batch: 360 Log-loss: 0.0516546256840229\n",
      "Epoch: 27 Batch: 400 Log-loss: 0.051598306745290756\n",
      "Epoch: 27 Batch: 440 Log-loss: 0.035120557993650436\n",
      "Epoch: 27 Batch: 480 Log-loss: 0.03484324738383293\n",
      "Epoch: 27 Batch: 520 Log-loss: 0.02747194468975067\n",
      "Epoch: 27 Batch: 560 Log-loss: 0.037571582943201065\n",
      "Epoch average log-loss: 0.03937922448385507\n",
      "In Epoch: 27, val_loss: 0.04086215522413308, best_val_loss: 0.040286046444048436, best_auc: 0.9889374024185579\n",
      "Epoch: 28 Batch: 0 Log-loss: 0.03545251861214638\n",
      "Epoch: 28 Batch: 40 Log-loss: 0.0379558727145195\n",
      "Epoch: 28 Batch: 80 Log-loss: 0.05255124345421791\n",
      "Epoch: 28 Batch: 120 Log-loss: 0.040688399225473404\n",
      "Epoch: 28 Batch: 160 Log-loss: 0.02787860669195652\n",
      "Epoch: 28 Batch: 200 Log-loss: 0.04705941304564476\n",
      "Epoch: 28 Batch: 240 Log-loss: 0.04228348657488823\n",
      "Epoch: 28 Batch: 280 Log-loss: 0.045857738703489304\n",
      "Epoch: 28 Batch: 320 Log-loss: 0.05919165909290314\n",
      "Epoch: 28 Batch: 360 Log-loss: 0.03818077594041824\n",
      "Epoch: 28 Batch: 400 Log-loss: 0.06403449922800064\n",
      "Epoch: 28 Batch: 440 Log-loss: 0.028900062665343285\n",
      "Epoch: 28 Batch: 480 Log-loss: 0.05870824679732323\n",
      "Epoch: 28 Batch: 520 Log-loss: 0.04308135807514191\n",
      "Epoch: 28 Batch: 560 Log-loss: 0.045893121510744095\n",
      "Epoch average log-loss: 0.03970920112915337\n",
      "In Epoch: 28, val_loss: 0.040853677960616494, best_val_loss: 0.040286046444048436, best_auc: 0.9889374024185579\n",
      "Epoch: 29 Batch: 0 Log-loss: 0.03701183572411537\n",
      "Epoch: 29 Batch: 40 Log-loss: 0.04310613498091698\n",
      "Epoch: 29 Batch: 80 Log-loss: 0.040877971798181534\n",
      "Epoch: 29 Batch: 120 Log-loss: 0.03226269781589508\n",
      "Epoch: 29 Batch: 160 Log-loss: 0.03173829987645149\n",
      "Epoch: 29 Batch: 200 Log-loss: 0.024587271735072136\n",
      "Epoch: 29 Batch: 240 Log-loss: 0.03413751348853111\n",
      "Epoch: 29 Batch: 280 Log-loss: 0.03466501459479332\n",
      "Epoch: 29 Batch: 320 Log-loss: 0.03836892545223236\n",
      "Epoch: 29 Batch: 360 Log-loss: 0.06374753266572952\n",
      "Epoch: 29 Batch: 400 Log-loss: 0.04026374965906143\n",
      "Epoch: 29 Batch: 440 Log-loss: 0.045016493648290634\n",
      "Epoch: 29 Batch: 480 Log-loss: 0.0369514636695385\n",
      "Epoch: 29 Batch: 520 Log-loss: 0.02672855742275715\n",
      "Epoch: 29 Batch: 560 Log-loss: 0.04123028367757797\n",
      "Epoch average log-loss: 0.03935879475570151\n",
      "In Epoch: 29, val_loss: 0.04070145580160408, best_val_loss: 0.040286046444048436, best_auc: 0.9889374024185579\n",
      "Epoch: 30 Batch: 0 Log-loss: 0.027023514732718468\n",
      "Epoch: 30 Batch: 40 Log-loss: 0.023844510316848755\n",
      "Epoch: 30 Batch: 80 Log-loss: 0.019822178408503532\n",
      "Epoch: 30 Batch: 120 Log-loss: 0.03881016746163368\n",
      "Epoch: 30 Batch: 160 Log-loss: 0.05487729236483574\n",
      "Epoch: 30 Batch: 200 Log-loss: 0.03143183887004852\n",
      "Epoch: 30 Batch: 240 Log-loss: 0.04185076430439949\n",
      "Epoch: 30 Batch: 280 Log-loss: 0.05088154599070549\n",
      "Epoch: 30 Batch: 320 Log-loss: 0.03966108709573746\n",
      "Epoch: 30 Batch: 360 Log-loss: 0.03194547817111015\n",
      "Epoch: 30 Batch: 400 Log-loss: 0.02967824600636959\n",
      "Epoch: 30 Batch: 440 Log-loss: 0.03113712929189205\n",
      "Epoch: 30 Batch: 480 Log-loss: 0.03861641511321068\n",
      "Epoch: 30 Batch: 520 Log-loss: 0.04427193105220795\n",
      "Epoch: 30 Batch: 560 Log-loss: 0.03727096691727638\n",
      "Epoch average log-loss: 0.03932660901420085\n",
      "In Epoch: 30, val_loss: 0.0408264036758632, best_val_loss: 0.040286046444048436, best_auc: 0.9889374024185579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 Batch: 0 Log-loss: 0.039852727204561234\n",
      "Epoch: 31 Batch: 40 Log-loss: 0.06481961160898209\n",
      "Epoch: 31 Batch: 80 Log-loss: 0.03420821577310562\n",
      "Epoch: 31 Batch: 120 Log-loss: 0.03263763710856438\n",
      "Epoch: 31 Batch: 160 Log-loss: 0.0378069244325161\n",
      "Epoch: 31 Batch: 200 Log-loss: 0.053424134850502014\n",
      "Epoch: 31 Batch: 240 Log-loss: 0.04135115444660187\n",
      "Epoch: 31 Batch: 280 Log-loss: 0.030789831653237343\n",
      "Epoch: 31 Batch: 320 Log-loss: 0.0650985836982727\n",
      "Epoch: 31 Batch: 360 Log-loss: 0.05280139669775963\n",
      "Epoch: 31 Batch: 400 Log-loss: 0.029761195182800293\n",
      "Epoch: 31 Batch: 440 Log-loss: 0.04201870039105415\n",
      "Epoch: 31 Batch: 480 Log-loss: 0.056899216026067734\n",
      "Epoch: 31 Batch: 520 Log-loss: 0.033996984362602234\n",
      "Epoch: 31 Batch: 560 Log-loss: 0.021790795028209686\n",
      "Epoch average log-loss: 0.03939591767266393\n",
      "In Epoch: 31, val_loss: 0.04055739081373827, best_val_loss: 0.040286046444048436, best_auc: 0.9889374024185579\n",
      "Epoch: 32 Batch: 0 Log-loss: 0.04677179455757141\n",
      "Epoch: 32 Batch: 40 Log-loss: 0.03357192873954773\n",
      "Epoch: 32 Batch: 80 Log-loss: 0.041417788714170456\n",
      "Epoch: 32 Batch: 120 Log-loss: 0.03521974757313728\n",
      "Epoch: 32 Batch: 160 Log-loss: 0.04536024108529091\n",
      "Epoch: 32 Batch: 200 Log-loss: 0.048387300223112106\n",
      "Epoch: 32 Batch: 240 Log-loss: 0.04344776272773743\n",
      "Epoch: 32 Batch: 280 Log-loss: 0.04757240042090416\n",
      "Epoch: 32 Batch: 320 Log-loss: 0.04153839126229286\n",
      "Epoch: 32 Batch: 360 Log-loss: 0.05130837485194206\n",
      "Epoch: 32 Batch: 400 Log-loss: 0.040454234927892685\n",
      "Epoch: 32 Batch: 440 Log-loss: 0.03632088378071785\n",
      "Epoch: 32 Batch: 480 Log-loss: 0.034365516155958176\n",
      "Epoch: 32 Batch: 520 Log-loss: 0.03631839528679848\n",
      "Epoch: 32 Batch: 560 Log-loss: 0.03545580431818962\n",
      "Epoch average log-loss: 0.03920296390008714\n",
      "In Epoch: 32, val_loss: 0.04030832107357591, best_val_loss: 0.040286046444048436, best_auc: 0.9889374024185579\n",
      "Epoch: 33 Batch: 0 Log-loss: 0.03915673866868019\n",
      "Epoch: 33 Batch: 40 Log-loss: 0.040988147258758545\n",
      "Epoch: 33 Batch: 80 Log-loss: 0.03155050054192543\n",
      "Epoch: 33 Batch: 120 Log-loss: 0.03887573257088661\n",
      "Epoch: 33 Batch: 160 Log-loss: 0.026832425966858864\n",
      "Epoch: 33 Batch: 200 Log-loss: 0.03043902851641178\n",
      "Epoch: 33 Batch: 240 Log-loss: 0.03896898403763771\n",
      "Epoch: 33 Batch: 280 Log-loss: 0.02522503398358822\n",
      "Epoch: 33 Batch: 320 Log-loss: 0.036906033754348755\n",
      "Epoch: 33 Batch: 360 Log-loss: 0.034610096365213394\n",
      "Epoch: 33 Batch: 400 Log-loss: 0.03707267716526985\n",
      "Epoch: 33 Batch: 440 Log-loss: 0.046267956495285034\n",
      "Epoch: 33 Batch: 480 Log-loss: 0.04568427801132202\n",
      "Epoch: 33 Batch: 520 Log-loss: 0.04222685098648071\n",
      "Epoch: 33 Batch: 560 Log-loss: 0.040587399154901505\n",
      "Epoch average log-loss: 0.039081321590180906\n",
      "In Epoch: 33, val_loss: 0.04070280598543135, best_val_loss: 0.040286046444048436, best_auc: 0.9889374024185579\n",
      "Epoch: 34 Batch: 0 Log-loss: 0.04426990821957588\n",
      "Epoch: 34 Batch: 40 Log-loss: 0.027177324518561363\n",
      "Epoch: 34 Batch: 80 Log-loss: 0.032798200845718384\n",
      "Epoch: 34 Batch: 120 Log-loss: 0.03928518295288086\n",
      "Epoch: 34 Batch: 160 Log-loss: 0.041608769446611404\n",
      "Epoch: 34 Batch: 200 Log-loss: 0.024756750091910362\n",
      "Epoch: 34 Batch: 240 Log-loss: 0.03146370127797127\n",
      "Epoch: 34 Batch: 280 Log-loss: 0.0373736247420311\n",
      "Epoch: 34 Batch: 320 Log-loss: 0.04969070851802826\n",
      "Epoch: 34 Batch: 360 Log-loss: 0.03124203346669674\n",
      "Epoch: 34 Batch: 400 Log-loss: 0.03216170147061348\n",
      "Epoch: 34 Batch: 440 Log-loss: 0.05216808617115021\n",
      "Epoch: 34 Batch: 480 Log-loss: 0.0333157442510128\n",
      "Epoch: 34 Batch: 520 Log-loss: 0.03535764664411545\n",
      "Epoch: 34 Batch: 560 Log-loss: 0.042050108313560486\n",
      "Epoch average log-loss: 0.03903758311810504\n",
      "A pre-trained model at model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt has been loaded.\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn4.pt.\n",
      "\n",
      "Choose the torch base model.\n",
      "## Training on fold 5 ##\n",
      "Epoch: 1 Batch: 0 Log-loss: 0.7086462378501892\n",
      "Epoch: 1 Batch: 40 Log-loss: 0.16450510919094086\n",
      "Epoch: 1 Batch: 80 Log-loss: 0.11425530910491943\n",
      "Epoch: 1 Batch: 120 Log-loss: 0.07232071459293365\n",
      "Epoch: 1 Batch: 160 Log-loss: 0.07085921615362167\n",
      "Epoch: 1 Batch: 200 Log-loss: 0.08437573164701462\n",
      "Epoch: 1 Batch: 240 Log-loss: 0.05026733875274658\n",
      "Epoch: 1 Batch: 280 Log-loss: 0.04547746106982231\n",
      "Epoch: 1 Batch: 320 Log-loss: 0.059375178068876266\n",
      "Epoch: 1 Batch: 360 Log-loss: 0.03845396265387535\n",
      "Epoch: 1 Batch: 400 Log-loss: 0.03689489886164665\n",
      "Epoch: 1 Batch: 440 Log-loss: 0.05753954127430916\n",
      "Epoch: 1 Batch: 480 Log-loss: 0.0463794469833374\n",
      "Epoch: 1 Batch: 520 Log-loss: 0.06701286882162094\n",
      "Epoch: 1 Batch: 560 Log-loss: 0.05974169448018074\n",
      "Epoch average log-loss: 0.08491082583287997\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 1, val_loss: 0.050188656764782215, best_val_loss: 0.050188656764782215, best_auc: 0.970825276538585\n",
      "Epoch: 2 Batch: 0 Log-loss: 0.08909332752227783\n",
      "Epoch: 2 Batch: 40 Log-loss: 0.04758773371577263\n",
      "Epoch: 2 Batch: 80 Log-loss: 0.06098068878054619\n",
      "Epoch: 2 Batch: 120 Log-loss: 0.04346552491188049\n",
      "Epoch: 2 Batch: 160 Log-loss: 0.0553583987057209\n",
      "Epoch: 2 Batch: 200 Log-loss: 0.04001708701252937\n",
      "Epoch: 2 Batch: 240 Log-loss: 0.0657535120844841\n",
      "Epoch: 2 Batch: 280 Log-loss: 0.04640531912446022\n",
      "Epoch: 2 Batch: 320 Log-loss: 0.04509997367858887\n",
      "Epoch: 2 Batch: 360 Log-loss: 0.0771714448928833\n",
      "Epoch: 2 Batch: 400 Log-loss: 0.04128729924559593\n",
      "Epoch: 2 Batch: 440 Log-loss: 0.04793933033943176\n",
      "Epoch: 2 Batch: 480 Log-loss: 0.045414626598358154\n",
      "Epoch: 2 Batch: 520 Log-loss: 0.046913985162973404\n",
      "Epoch: 2 Batch: 560 Log-loss: 0.045586273074150085\n",
      "Epoch average log-loss: 0.05220194246087755\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 2, val_loss: 0.047891406344676535, best_val_loss: 0.047891406344676535, best_auc: 0.9758852648722153\n",
      "Epoch: 3 Batch: 0 Log-loss: 0.04751364886760712\n",
      "Epoch: 3 Batch: 40 Log-loss: 0.047879885882139206\n",
      "Epoch: 3 Batch: 80 Log-loss: 0.03958866745233536\n",
      "Epoch: 3 Batch: 120 Log-loss: 0.06668224185705185\n",
      "Epoch: 3 Batch: 160 Log-loss: 0.0398249477148056\n",
      "Epoch: 3 Batch: 200 Log-loss: 0.04487404227256775\n",
      "Epoch: 3 Batch: 240 Log-loss: 0.03652094677090645\n",
      "Epoch: 3 Batch: 280 Log-loss: 0.048130784183740616\n",
      "Epoch: 3 Batch: 320 Log-loss: 0.05549648031592369\n",
      "Epoch: 3 Batch: 360 Log-loss: 0.07344234734773636\n",
      "Epoch: 3 Batch: 400 Log-loss: 0.039339419454336166\n",
      "Epoch: 3 Batch: 440 Log-loss: 0.047086846083402634\n",
      "Epoch: 3 Batch: 480 Log-loss: 0.04824814200401306\n",
      "Epoch: 3 Batch: 520 Log-loss: 0.03423391655087471\n",
      "Epoch: 3 Batch: 560 Log-loss: 0.04186035692691803\n",
      "Epoch average log-loss: 0.049257215541521356\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 3, val_loss: 0.04511898044385732, best_val_loss: 0.04511898044385732, best_auc: 0.9807842117159248\n",
      "Epoch: 4 Batch: 0 Log-loss: 0.051372330635786057\n",
      "Epoch: 4 Batch: 40 Log-loss: 0.054655540734529495\n",
      "Epoch: 4 Batch: 80 Log-loss: 0.02913777530193329\n",
      "Epoch: 4 Batch: 120 Log-loss: 0.06514323502779007\n",
      "Epoch: 4 Batch: 160 Log-loss: 0.050188783556222916\n",
      "Epoch: 4 Batch: 200 Log-loss: 0.05116811767220497\n",
      "Epoch: 4 Batch: 240 Log-loss: 0.04988802969455719\n",
      "Epoch: 4 Batch: 280 Log-loss: 0.038701847195625305\n",
      "Epoch: 4 Batch: 320 Log-loss: 0.0466960109770298\n",
      "Epoch: 4 Batch: 360 Log-loss: 0.04731622710824013\n",
      "Epoch: 4 Batch: 400 Log-loss: 0.043769240379333496\n",
      "Epoch: 4 Batch: 440 Log-loss: 0.0362681970000267\n",
      "Epoch: 4 Batch: 480 Log-loss: 0.05681014433503151\n",
      "Epoch: 4 Batch: 520 Log-loss: 0.035355888307094574\n",
      "Epoch: 4 Batch: 560 Log-loss: 0.045600250363349915\n",
      "Epoch average log-loss: 0.0469701509357297\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 4, val_loss: 0.043885140199627914, best_val_loss: 0.043885140199627914, best_auc: 0.9822740201218031\n",
      "Epoch: 5 Batch: 0 Log-loss: 0.041635170578956604\n",
      "Epoch: 5 Batch: 40 Log-loss: 0.047456320375204086\n",
      "Epoch: 5 Batch: 80 Log-loss: 0.05007247254252434\n",
      "Epoch: 5 Batch: 120 Log-loss: 0.04309579357504845\n",
      "Epoch: 5 Batch: 160 Log-loss: 0.039695996791124344\n",
      "Epoch: 5 Batch: 200 Log-loss: 0.044947024434804916\n",
      "Epoch: 5 Batch: 240 Log-loss: 0.05041845142841339\n",
      "Epoch: 5 Batch: 280 Log-loss: 0.045122355222702026\n",
      "Epoch: 5 Batch: 320 Log-loss: 0.04908667877316475\n",
      "Epoch: 5 Batch: 360 Log-loss: 0.031309474259614944\n",
      "Epoch: 5 Batch: 400 Log-loss: 0.02518155612051487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Batch: 440 Log-loss: 0.0558314323425293\n",
      "Epoch: 5 Batch: 480 Log-loss: 0.03545612469315529\n",
      "Epoch: 5 Batch: 520 Log-loss: 0.061670735478401184\n",
      "Epoch: 5 Batch: 560 Log-loss: 0.053573161363601685\n",
      "Epoch average log-loss: 0.045734232381385354\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 5, val_loss: 0.04312738564701038, best_val_loss: 0.04312738564701038, best_auc: 0.9841317802807378\n",
      "Epoch: 6 Batch: 0 Log-loss: 0.04084121808409691\n",
      "Epoch: 6 Batch: 40 Log-loss: 0.06315923482179642\n",
      "Epoch: 6 Batch: 80 Log-loss: 0.05243361368775368\n",
      "Epoch: 6 Batch: 120 Log-loss: 0.05588553845882416\n",
      "Epoch: 6 Batch: 160 Log-loss: 0.036649931222200394\n",
      "Epoch: 6 Batch: 200 Log-loss: 0.044969309121370316\n",
      "Epoch: 6 Batch: 240 Log-loss: 0.03859952464699745\n",
      "Epoch: 6 Batch: 280 Log-loss: 0.04363716021180153\n",
      "Epoch: 6 Batch: 320 Log-loss: 0.06361734122037888\n",
      "Epoch: 6 Batch: 360 Log-loss: 0.044245537370443344\n",
      "Epoch: 6 Batch: 400 Log-loss: 0.051547180861234665\n",
      "Epoch: 6 Batch: 440 Log-loss: 0.037520259618759155\n",
      "Epoch: 6 Batch: 480 Log-loss: 0.0588594488799572\n",
      "Epoch: 6 Batch: 520 Log-loss: 0.05173173546791077\n",
      "Epoch: 6 Batch: 560 Log-loss: 0.03918492794036865\n",
      "Epoch average log-loss: 0.045035296961266014\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 6, val_loss: 0.04230716696269856, best_val_loss: 0.04230716696269856, best_auc: 0.9851132112658707\n",
      "Epoch: 7 Batch: 0 Log-loss: 0.03991099074482918\n",
      "Epoch: 7 Batch: 40 Log-loss: 0.039790745824575424\n",
      "Epoch: 7 Batch: 80 Log-loss: 0.04165521636605263\n",
      "Epoch: 7 Batch: 120 Log-loss: 0.04177374765276909\n",
      "Epoch: 7 Batch: 160 Log-loss: 0.029319405555725098\n",
      "Epoch: 7 Batch: 200 Log-loss: 0.05148391053080559\n",
      "Epoch: 7 Batch: 240 Log-loss: 0.05108161270618439\n",
      "Epoch: 7 Batch: 280 Log-loss: 0.06100442633032799\n",
      "Epoch: 7 Batch: 320 Log-loss: 0.04782286658883095\n",
      "Epoch: 7 Batch: 360 Log-loss: 0.05185459181666374\n",
      "Epoch: 7 Batch: 400 Log-loss: 0.04891431704163551\n",
      "Epoch: 7 Batch: 440 Log-loss: 0.03938551992177963\n",
      "Epoch: 7 Batch: 480 Log-loss: 0.03777240216732025\n",
      "Epoch: 7 Batch: 520 Log-loss: 0.032817598432302475\n",
      "Epoch: 7 Batch: 560 Log-loss: 0.039004746824502945\n",
      "Epoch average log-loss: 0.04393578607788576\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 7, val_loss: 0.041851648905545186, best_val_loss: 0.041851648905545186, best_auc: 0.9863721842097354\n",
      "Epoch: 8 Batch: 0 Log-loss: 0.03644977882504463\n",
      "Epoch: 8 Batch: 40 Log-loss: 0.037591852247714996\n",
      "Epoch: 8 Batch: 80 Log-loss: 0.061130598187446594\n",
      "Epoch: 8 Batch: 120 Log-loss: 0.037149932235479355\n",
      "Epoch: 8 Batch: 160 Log-loss: 0.05875867232680321\n",
      "Epoch: 8 Batch: 200 Log-loss: 0.05267832800745964\n",
      "Epoch: 8 Batch: 240 Log-loss: 0.040720198303461075\n",
      "Epoch: 8 Batch: 280 Log-loss: 0.049663085490465164\n",
      "Epoch: 8 Batch: 320 Log-loss: 0.03433402627706528\n",
      "Epoch: 8 Batch: 360 Log-loss: 0.046806614845991135\n",
      "Epoch: 8 Batch: 400 Log-loss: 0.041972775012254715\n",
      "Epoch: 8 Batch: 440 Log-loss: 0.042889948934316635\n",
      "Epoch: 8 Batch: 480 Log-loss: 0.04970317706465721\n",
      "Epoch: 8 Batch: 520 Log-loss: 0.03823571652173996\n",
      "Epoch: 8 Batch: 560 Log-loss: 0.042901020497083664\n",
      "Epoch average log-loss: 0.043517650457631264\n",
      "In Epoch: 8, val_loss: 0.04201256218775642, best_val_loss: 0.041851648905545186, best_auc: 0.9863721842097354\n",
      "Epoch: 9 Batch: 0 Log-loss: 0.04206572845578194\n",
      "Epoch: 9 Batch: 40 Log-loss: 0.034865427762269974\n",
      "Epoch: 9 Batch: 80 Log-loss: 0.05130339786410332\n",
      "Epoch: 9 Batch: 120 Log-loss: 0.03858819976449013\n",
      "Epoch: 9 Batch: 160 Log-loss: 0.051365893334150314\n",
      "Epoch: 9 Batch: 200 Log-loss: 0.03890405222773552\n",
      "Epoch: 9 Batch: 240 Log-loss: 0.05742591246962547\n",
      "Epoch: 9 Batch: 280 Log-loss: 0.04904062673449516\n",
      "Epoch: 9 Batch: 320 Log-loss: 0.03393055871129036\n",
      "Epoch: 9 Batch: 360 Log-loss: 0.04540548846125603\n",
      "Epoch: 9 Batch: 400 Log-loss: 0.02159302495419979\n",
      "Epoch: 9 Batch: 440 Log-loss: 0.03306850418448448\n",
      "Epoch: 9 Batch: 480 Log-loss: 0.04016737639904022\n",
      "Epoch: 9 Batch: 520 Log-loss: 0.035867657512426376\n",
      "Epoch: 9 Batch: 560 Log-loss: 0.05180175229907036\n",
      "Epoch average log-loss: 0.04292149798878069\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 9, val_loss: 0.04102465784951664, best_val_loss: 0.04102465784951664, best_auc: 0.9865596217109717\n",
      "Epoch: 10 Batch: 0 Log-loss: 0.05161784216761589\n",
      "Epoch: 10 Batch: 40 Log-loss: 0.04648521915078163\n",
      "Epoch: 10 Batch: 80 Log-loss: 0.04872087016701698\n",
      "Epoch: 10 Batch: 120 Log-loss: 0.03031983971595764\n",
      "Epoch: 10 Batch: 160 Log-loss: 0.037378739565610886\n",
      "Epoch: 10 Batch: 200 Log-loss: 0.0481441430747509\n",
      "Epoch: 10 Batch: 240 Log-loss: 0.050252776592969894\n",
      "Epoch: 10 Batch: 280 Log-loss: 0.03631412610411644\n",
      "Epoch: 10 Batch: 320 Log-loss: 0.03385494276881218\n",
      "Epoch: 10 Batch: 360 Log-loss: 0.045235857367515564\n",
      "Epoch: 10 Batch: 400 Log-loss: 0.060423631221055984\n",
      "Epoch: 10 Batch: 440 Log-loss: 0.03472732752561569\n",
      "Epoch: 10 Batch: 480 Log-loss: 0.03872919827699661\n",
      "Epoch: 10 Batch: 520 Log-loss: 0.05597769841551781\n",
      "Epoch: 10 Batch: 560 Log-loss: 0.03071727603673935\n",
      "Epoch average log-loss: 0.042658883605950645\n",
      "In Epoch: 10, val_loss: 0.04155531645144328, best_val_loss: 0.04102465784951664, best_auc: 0.9865596217109717\n",
      "Epoch: 11 Batch: 0 Log-loss: 0.02828359603881836\n",
      "Epoch: 11 Batch: 40 Log-loss: 0.05275404453277588\n",
      "Epoch: 11 Batch: 80 Log-loss: 0.05411602556705475\n",
      "Epoch: 11 Batch: 120 Log-loss: 0.03569762036204338\n",
      "Epoch: 11 Batch: 160 Log-loss: 0.04214712977409363\n",
      "Epoch: 11 Batch: 200 Log-loss: 0.058250486850738525\n",
      "Epoch: 11 Batch: 240 Log-loss: 0.03334860876202583\n",
      "Epoch: 11 Batch: 280 Log-loss: 0.03558851405978203\n",
      "Epoch: 11 Batch: 320 Log-loss: 0.04376896843314171\n",
      "Epoch: 11 Batch: 360 Log-loss: 0.050565097481012344\n",
      "Epoch: 11 Batch: 400 Log-loss: 0.044891417026519775\n",
      "Epoch: 11 Batch: 440 Log-loss: 0.04535188153386116\n",
      "Epoch: 11 Batch: 480 Log-loss: 0.05103852227330208\n",
      "Epoch: 11 Batch: 520 Log-loss: 0.03315835818648338\n",
      "Epoch: 11 Batch: 560 Log-loss: 0.047563787549734116\n",
      "Epoch average log-loss: 0.042401371197775005\n",
      "In Epoch: 11, val_loss: 0.04131301061043464, best_val_loss: 0.04102465784951664, best_auc: 0.9865596217109717\n",
      "Epoch: 12 Batch: 0 Log-loss: 0.05206506326794624\n",
      "Epoch: 12 Batch: 40 Log-loss: 0.04252871498465538\n",
      "Epoch: 12 Batch: 80 Log-loss: 0.04009808227419853\n",
      "Epoch: 12 Batch: 120 Log-loss: 0.0353417694568634\n",
      "Epoch: 12 Batch: 160 Log-loss: 0.05605902895331383\n",
      "Epoch: 12 Batch: 200 Log-loss: 0.02660122513771057\n",
      "Epoch: 12 Batch: 240 Log-loss: 0.041553888469934464\n",
      "Epoch: 12 Batch: 280 Log-loss: 0.03685406222939491\n",
      "Epoch: 12 Batch: 320 Log-loss: 0.0382557287812233\n",
      "Epoch: 12 Batch: 360 Log-loss: 0.0366920568048954\n",
      "Epoch: 12 Batch: 400 Log-loss: 0.03841051459312439\n",
      "Epoch: 12 Batch: 440 Log-loss: 0.047606904059648514\n",
      "Epoch: 12 Batch: 480 Log-loss: 0.043642088770866394\n",
      "Epoch: 12 Batch: 520 Log-loss: 0.03646603599190712\n",
      "Epoch: 12 Batch: 560 Log-loss: 0.05295628309249878\n",
      "Epoch average log-loss: 0.04208556965126523\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 12, val_loss: 0.04098465141190071, best_val_loss: 0.04098465141190071, best_auc: 0.9866449534225388\n",
      "Epoch: 13 Batch: 0 Log-loss: 0.04566631838679314\n",
      "Epoch: 13 Batch: 40 Log-loss: 0.03511308506131172\n",
      "Epoch: 13 Batch: 80 Log-loss: 0.04143104329705238\n",
      "Epoch: 13 Batch: 120 Log-loss: 0.05446289852261543\n",
      "Epoch: 13 Batch: 160 Log-loss: 0.046815935522317886\n",
      "Epoch: 13 Batch: 200 Log-loss: 0.03855597600340843\n",
      "Epoch: 13 Batch: 240 Log-loss: 0.044696882367134094\n",
      "Epoch: 13 Batch: 280 Log-loss: 0.04998527839779854\n",
      "Epoch: 13 Batch: 320 Log-loss: 0.04079952836036682\n",
      "Epoch: 13 Batch: 360 Log-loss: 0.05840301886200905\n",
      "Epoch: 13 Batch: 400 Log-loss: 0.03766902536153793\n",
      "Epoch: 13 Batch: 440 Log-loss: 0.038433972746133804\n",
      "Epoch: 13 Batch: 480 Log-loss: 0.05011442303657532\n",
      "Epoch: 13 Batch: 520 Log-loss: 0.04049674794077873\n",
      "Epoch: 13 Batch: 560 Log-loss: 0.054391730576753616\n",
      "Epoch average log-loss: 0.04186654059282903\n",
      "In Epoch: 13, val_loss: 0.041000608995510765, best_val_loss: 0.04098465141190071, best_auc: 0.9866449534225388\n",
      "Epoch: 14 Batch: 0 Log-loss: 0.02929597534239292\n",
      "Epoch: 14 Batch: 40 Log-loss: 0.05172931030392647\n",
      "Epoch: 14 Batch: 80 Log-loss: 0.04481905698776245\n",
      "Epoch: 14 Batch: 120 Log-loss: 0.05668291449546814\n",
      "Epoch: 14 Batch: 160 Log-loss: 0.06229761615395546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Batch: 200 Log-loss: 0.0419657863676548\n",
      "Epoch: 14 Batch: 240 Log-loss: 0.029108747839927673\n",
      "Epoch: 14 Batch: 280 Log-loss: 0.03704996034502983\n",
      "Epoch: 14 Batch: 320 Log-loss: 0.037760671228170395\n",
      "Epoch: 14 Batch: 360 Log-loss: 0.03789113089442253\n",
      "Epoch: 14 Batch: 400 Log-loss: 0.05496269837021828\n",
      "Epoch: 14 Batch: 440 Log-loss: 0.031972724944353104\n",
      "Epoch: 14 Batch: 480 Log-loss: 0.028412558138370514\n",
      "Epoch: 14 Batch: 520 Log-loss: 0.03947991877794266\n",
      "Epoch: 14 Batch: 560 Log-loss: 0.03353649377822876\n",
      "Epoch average log-loss: 0.04139187710492739\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 14, val_loss: 0.04054266284817117, best_val_loss: 0.04054266284817117, best_auc: 0.987541433340792\n",
      "Epoch: 15 Batch: 0 Log-loss: 0.03405486419796944\n",
      "Epoch: 15 Batch: 40 Log-loss: 0.05207743123173714\n",
      "Epoch: 15 Batch: 80 Log-loss: 0.050435956567525864\n",
      "Epoch: 15 Batch: 120 Log-loss: 0.03532165661454201\n",
      "Epoch: 15 Batch: 160 Log-loss: 0.05245046690106392\n",
      "Epoch: 15 Batch: 200 Log-loss: 0.03792615607380867\n",
      "Epoch: 15 Batch: 240 Log-loss: 0.040820978581905365\n",
      "Epoch: 15 Batch: 280 Log-loss: 0.05858394876122475\n",
      "Epoch: 15 Batch: 320 Log-loss: 0.04373103752732277\n",
      "Epoch: 15 Batch: 360 Log-loss: 0.03912300243973732\n",
      "Epoch: 15 Batch: 400 Log-loss: 0.03893725201487541\n",
      "Epoch: 15 Batch: 440 Log-loss: 0.05998710170388222\n",
      "Epoch: 15 Batch: 480 Log-loss: 0.05565972626209259\n",
      "Epoch: 15 Batch: 520 Log-loss: 0.05500942841172218\n",
      "Epoch: 15 Batch: 560 Log-loss: 0.03216656297445297\n",
      "Epoch average log-loss: 0.04120230129254716\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 15, val_loss: 0.040250867299631954, best_val_loss: 0.040250867299631954, best_auc: 0.9876152113579941\n",
      "Epoch: 16 Batch: 0 Log-loss: 0.032480765134096146\n",
      "Epoch: 16 Batch: 40 Log-loss: 0.03707680106163025\n",
      "Epoch: 16 Batch: 80 Log-loss: 0.05676078796386719\n",
      "Epoch: 16 Batch: 120 Log-loss: 0.04356275498867035\n",
      "Epoch: 16 Batch: 160 Log-loss: 0.03720010817050934\n",
      "Epoch: 16 Batch: 200 Log-loss: 0.044082533568143845\n",
      "Epoch: 16 Batch: 240 Log-loss: 0.03955793008208275\n",
      "Epoch: 16 Batch: 280 Log-loss: 0.03854867070913315\n",
      "Epoch: 16 Batch: 320 Log-loss: 0.04161759093403816\n",
      "Epoch: 16 Batch: 360 Log-loss: 0.042931005358695984\n",
      "Epoch: 16 Batch: 400 Log-loss: 0.03301344811916351\n",
      "Epoch: 16 Batch: 440 Log-loss: 0.047324877232313156\n",
      "Epoch: 16 Batch: 480 Log-loss: 0.0478927306830883\n",
      "Epoch: 16 Batch: 520 Log-loss: 0.05074361339211464\n",
      "Epoch: 16 Batch: 560 Log-loss: 0.040257491171360016\n",
      "Epoch average log-loss: 0.04132473106562559\n",
      "In Epoch: 16, val_loss: 0.04075837733353233, best_val_loss: 0.040250867299631954, best_auc: 0.9876152113579941\n",
      "Epoch: 17 Batch: 0 Log-loss: 0.03570881113409996\n",
      "Epoch: 17 Batch: 40 Log-loss: 0.05215434730052948\n",
      "Epoch: 17 Batch: 80 Log-loss: 0.03941146656870842\n",
      "Epoch: 17 Batch: 120 Log-loss: 0.06120068207383156\n",
      "Epoch: 17 Batch: 160 Log-loss: 0.038366228342056274\n",
      "Epoch: 17 Batch: 200 Log-loss: 0.04931535944342613\n",
      "Epoch: 17 Batch: 240 Log-loss: 0.04198605939745903\n",
      "Epoch: 17 Batch: 280 Log-loss: 0.04585009440779686\n",
      "Epoch: 17 Batch: 320 Log-loss: 0.035858772695064545\n",
      "Epoch: 17 Batch: 360 Log-loss: 0.03518916293978691\n",
      "Epoch: 17 Batch: 400 Log-loss: 0.04583727940917015\n",
      "Epoch: 17 Batch: 440 Log-loss: 0.04831912741065025\n",
      "Epoch: 17 Batch: 480 Log-loss: 0.041487473994493484\n",
      "Epoch: 17 Batch: 520 Log-loss: 0.047640178352594376\n",
      "Epoch: 17 Batch: 560 Log-loss: 0.03785090893507004\n",
      "Epoch average log-loss: 0.04068735193328134\n",
      "In Epoch: 17, val_loss: 0.040892497434550305, best_val_loss: 0.040250867299631954, best_auc: 0.9876152113579941\n",
      "Epoch: 18 Batch: 0 Log-loss: 0.026402471587061882\n",
      "Epoch: 18 Batch: 40 Log-loss: 0.04108733311295509\n",
      "Epoch: 18 Batch: 80 Log-loss: 0.04748283326625824\n",
      "Epoch: 18 Batch: 120 Log-loss: 0.05455580726265907\n",
      "Epoch: 18 Batch: 160 Log-loss: 0.0518520288169384\n",
      "Epoch: 18 Batch: 200 Log-loss: 0.046869900077581406\n",
      "Epoch: 18 Batch: 240 Log-loss: 0.04932783544063568\n",
      "Epoch: 18 Batch: 280 Log-loss: 0.0407574363052845\n",
      "Epoch: 18 Batch: 320 Log-loss: 0.02745666168630123\n",
      "Epoch: 18 Batch: 360 Log-loss: 0.04934130609035492\n",
      "Epoch: 18 Batch: 400 Log-loss: 0.043474022299051285\n",
      "Epoch: 18 Batch: 440 Log-loss: 0.04985523596405983\n",
      "Epoch: 18 Batch: 480 Log-loss: 0.03872713819146156\n",
      "Epoch: 18 Batch: 520 Log-loss: 0.04336811974644661\n",
      "Epoch: 18 Batch: 560 Log-loss: 0.03315087780356407\n",
      "Epoch average log-loss: 0.040575624224064605\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 18, val_loss: 0.03998695262659272, best_val_loss: 0.03998695262659272, best_auc: 0.9876625643682232\n",
      "Epoch: 19 Batch: 0 Log-loss: 0.034071166068315506\n",
      "Epoch: 19 Batch: 40 Log-loss: 0.04495792090892792\n",
      "Epoch: 19 Batch: 80 Log-loss: 0.03778119012713432\n",
      "Epoch: 19 Batch: 120 Log-loss: 0.04066024348139763\n",
      "Epoch: 19 Batch: 160 Log-loss: 0.039890486747026443\n",
      "Epoch: 19 Batch: 200 Log-loss: 0.047312766313552856\n",
      "Epoch: 19 Batch: 240 Log-loss: 0.04297887906432152\n",
      "Epoch: 19 Batch: 280 Log-loss: 0.05269524082541466\n",
      "Epoch: 19 Batch: 320 Log-loss: 0.030870430171489716\n",
      "Epoch: 19 Batch: 360 Log-loss: 0.03922092169523239\n",
      "Epoch: 19 Batch: 400 Log-loss: 0.041042350232601166\n",
      "Epoch: 19 Batch: 440 Log-loss: 0.03763964772224426\n",
      "Epoch: 19 Batch: 480 Log-loss: 0.03229955956339836\n",
      "Epoch: 19 Batch: 520 Log-loss: 0.03495033457875252\n",
      "Epoch: 19 Batch: 560 Log-loss: 0.03330346941947937\n",
      "Epoch average log-loss: 0.040558834147772616\n",
      "In Epoch: 19, val_loss: 0.04033189610069522, best_val_loss: 0.03998695262659272, best_auc: 0.9876625643682232\n",
      "Epoch: 20 Batch: 0 Log-loss: 0.02799876034259796\n",
      "Epoch: 20 Batch: 40 Log-loss: 0.033970966935157776\n",
      "Epoch: 20 Batch: 80 Log-loss: 0.034226998686790466\n",
      "Epoch: 20 Batch: 120 Log-loss: 0.039581697434186935\n",
      "Epoch: 20 Batch: 160 Log-loss: 0.05851999297738075\n",
      "Epoch: 20 Batch: 200 Log-loss: 0.03091488964855671\n",
      "Epoch: 20 Batch: 240 Log-loss: 0.024941416457295418\n",
      "Epoch: 20 Batch: 280 Log-loss: 0.02791612595319748\n",
      "Epoch: 20 Batch: 320 Log-loss: 0.04751071706414223\n",
      "Epoch: 20 Batch: 360 Log-loss: 0.02946697361767292\n",
      "Epoch: 20 Batch: 400 Log-loss: 0.04380417987704277\n",
      "Epoch: 20 Batch: 440 Log-loss: 0.04628292843699455\n",
      "Epoch: 20 Batch: 480 Log-loss: 0.05062338337302208\n",
      "Epoch: 20 Batch: 520 Log-loss: 0.035769280046224594\n",
      "Epoch: 20 Batch: 560 Log-loss: 0.03703517094254494\n",
      "Epoch average log-loss: 0.04045218727884016\n",
      "In Epoch: 20, val_loss: 0.04025010684031169, best_val_loss: 0.03998695262659272, best_auc: 0.9876625643682232\n",
      "Epoch: 21 Batch: 0 Log-loss: 0.03571037948131561\n",
      "Epoch: 21 Batch: 40 Log-loss: 0.042384374886751175\n",
      "Epoch: 21 Batch: 80 Log-loss: 0.036971401423215866\n",
      "Epoch: 21 Batch: 120 Log-loss: 0.020632339641451836\n",
      "Epoch: 21 Batch: 160 Log-loss: 0.04342178627848625\n",
      "Epoch: 21 Batch: 200 Log-loss: 0.0639013722538948\n",
      "Epoch: 21 Batch: 240 Log-loss: 0.05654667690396309\n",
      "Epoch: 21 Batch: 280 Log-loss: 0.03388109430670738\n",
      "Epoch: 21 Batch: 320 Log-loss: 0.02751261554658413\n",
      "Epoch: 21 Batch: 360 Log-loss: 0.026363739743828773\n",
      "Epoch: 21 Batch: 400 Log-loss: 0.04591253772377968\n",
      "Epoch: 21 Batch: 440 Log-loss: 0.025698108598589897\n",
      "Epoch: 21 Batch: 480 Log-loss: 0.03870129585266113\n",
      "Epoch: 21 Batch: 520 Log-loss: 0.048431288450956345\n",
      "Epoch: 21 Batch: 560 Log-loss: 0.04752160236239433\n",
      "Epoch average log-loss: 0.04024682088555502\n",
      "In Epoch: 21, val_loss: 0.04036369017104121, best_val_loss: 0.03998695262659272, best_auc: 0.9876625643682232\n",
      "Epoch: 22 Batch: 0 Log-loss: 0.0361454114317894\n",
      "Epoch: 22 Batch: 40 Log-loss: 0.039060283452272415\n",
      "Epoch: 22 Batch: 80 Log-loss: 0.02484496496617794\n",
      "Epoch: 22 Batch: 120 Log-loss: 0.029739083722233772\n",
      "Epoch: 22 Batch: 160 Log-loss: 0.032299116253852844\n",
      "Epoch: 22 Batch: 200 Log-loss: 0.04709473252296448\n",
      "Epoch: 22 Batch: 240 Log-loss: 0.04111676663160324\n",
      "Epoch: 22 Batch: 280 Log-loss: 0.04084116965532303\n",
      "Epoch: 22 Batch: 320 Log-loss: 0.04743504896759987\n",
      "Epoch: 22 Batch: 360 Log-loss: 0.0375945046544075\n",
      "Epoch: 22 Batch: 400 Log-loss: 0.03660491481423378\n",
      "Epoch: 22 Batch: 440 Log-loss: 0.04635092243552208\n",
      "Epoch: 22 Batch: 480 Log-loss: 0.04682786390185356\n",
      "Epoch: 22 Batch: 520 Log-loss: 0.03611449897289276\n",
      "Epoch: 22 Batch: 560 Log-loss: 0.03721008449792862\n",
      "Epoch average log-loss: 0.04033254191079842\n",
      "In Epoch: 22, val_loss: 0.04024557060534032, best_val_loss: 0.03998695262659272, best_auc: 0.9876625643682232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 Batch: 0 Log-loss: 0.041823867708444595\n",
      "Epoch: 23 Batch: 40 Log-loss: 0.041080739349126816\n",
      "Epoch: 23 Batch: 80 Log-loss: 0.03779733553528786\n",
      "Epoch: 23 Batch: 120 Log-loss: 0.05411575734615326\n",
      "Epoch: 23 Batch: 160 Log-loss: 0.033252034336328506\n",
      "Epoch: 23 Batch: 200 Log-loss: 0.03434687480330467\n",
      "Epoch: 23 Batch: 240 Log-loss: 0.02286229096353054\n",
      "Epoch: 23 Batch: 280 Log-loss: 0.041796233505010605\n",
      "Epoch: 23 Batch: 320 Log-loss: 0.06717391312122345\n",
      "Epoch: 23 Batch: 360 Log-loss: 0.03653847053647041\n",
      "Epoch: 23 Batch: 400 Log-loss: 0.039779748767614365\n",
      "Epoch: 23 Batch: 440 Log-loss: 0.035373982042074203\n",
      "Epoch: 23 Batch: 480 Log-loss: 0.03532235696911812\n",
      "Epoch: 23 Batch: 520 Log-loss: 0.03239138051867485\n",
      "Epoch: 23 Batch: 560 Log-loss: 0.06409061700105667\n",
      "Epoch average log-loss: 0.040055926376953724\n",
      "In Epoch: 23, val_loss: 0.04033086101568331, best_val_loss: 0.03998695262659272, best_auc: 0.9876625643682232\n",
      "Epoch: 24 Batch: 0 Log-loss: 0.029798155650496483\n",
      "Epoch: 24 Batch: 40 Log-loss: 0.05029350146651268\n",
      "Epoch: 24 Batch: 80 Log-loss: 0.052281979471445084\n",
      "Epoch: 24 Batch: 120 Log-loss: 0.04287709295749664\n",
      "Epoch: 24 Batch: 160 Log-loss: 0.04460284113883972\n",
      "Epoch: 24 Batch: 200 Log-loss: 0.044873472303152084\n",
      "Epoch: 24 Batch: 240 Log-loss: 0.04644150659441948\n",
      "Epoch: 24 Batch: 280 Log-loss: 0.03197844699025154\n",
      "Epoch: 24 Batch: 320 Log-loss: 0.04344711825251579\n",
      "Epoch: 24 Batch: 360 Log-loss: 0.0381002239882946\n",
      "Epoch: 24 Batch: 400 Log-loss: 0.04250076785683632\n",
      "Epoch: 24 Batch: 440 Log-loss: 0.031131545081734657\n",
      "Epoch: 24 Batch: 480 Log-loss: 0.029788343235850334\n",
      "Epoch: 24 Batch: 520 Log-loss: 0.0469927154481411\n",
      "Epoch: 24 Batch: 560 Log-loss: 0.044111963361501694\n",
      "Epoch average log-loss: 0.03996489901494767\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 24, val_loss: 0.03996427345859818, best_val_loss: 0.03996427345859818, best_auc: 0.9877429229764788\n",
      "Epoch: 25 Batch: 0 Log-loss: 0.038668982684612274\n",
      "Epoch: 25 Batch: 40 Log-loss: 0.03419424220919609\n",
      "Epoch: 25 Batch: 80 Log-loss: 0.03363003954291344\n",
      "Epoch: 25 Batch: 120 Log-loss: 0.048309653997421265\n",
      "Epoch: 25 Batch: 160 Log-loss: 0.04821573197841644\n",
      "Epoch: 25 Batch: 200 Log-loss: 0.05467923358082771\n",
      "Epoch: 25 Batch: 240 Log-loss: 0.04167966917157173\n",
      "Epoch: 25 Batch: 280 Log-loss: 0.04271962121129036\n",
      "Epoch: 25 Batch: 320 Log-loss: 0.038806747645139694\n",
      "Epoch: 25 Batch: 360 Log-loss: 0.06694642454385757\n",
      "Epoch: 25 Batch: 400 Log-loss: 0.03904499113559723\n",
      "Epoch: 25 Batch: 440 Log-loss: 0.03819795325398445\n",
      "Epoch: 25 Batch: 480 Log-loss: 0.04952816292643547\n",
      "Epoch: 25 Batch: 520 Log-loss: 0.04018443450331688\n",
      "Epoch: 25 Batch: 560 Log-loss: 0.0368134044110775\n",
      "Epoch average log-loss: 0.039771672314964236\n",
      "In Epoch: 25, val_loss: 0.04047586728103437, best_val_loss: 0.03996427345859818, best_auc: 0.9877429229764788\n",
      "Epoch: 26 Batch: 0 Log-loss: 0.033508192747831345\n",
      "Epoch: 26 Batch: 40 Log-loss: 0.03145293891429901\n",
      "Epoch: 26 Batch: 80 Log-loss: 0.045292068272829056\n",
      "Epoch: 26 Batch: 120 Log-loss: 0.058561842888593674\n",
      "Epoch: 26 Batch: 160 Log-loss: 0.04515261948108673\n",
      "Epoch: 26 Batch: 200 Log-loss: 0.03405287116765976\n",
      "Epoch: 26 Batch: 240 Log-loss: 0.032697539776563644\n",
      "Epoch: 26 Batch: 280 Log-loss: 0.03655180335044861\n",
      "Epoch: 26 Batch: 320 Log-loss: 0.027945421636104584\n",
      "Epoch: 26 Batch: 360 Log-loss: 0.04323878884315491\n",
      "Epoch: 26 Batch: 400 Log-loss: 0.050991546362638474\n",
      "Epoch: 26 Batch: 440 Log-loss: 0.038108449429273605\n",
      "Epoch: 26 Batch: 480 Log-loss: 0.02619295008480549\n",
      "Epoch: 26 Batch: 520 Log-loss: 0.05192367732524872\n",
      "Epoch: 26 Batch: 560 Log-loss: 0.04355516657233238\n",
      "Epoch average log-loss: 0.039512820172655796\n",
      "In Epoch: 26, val_loss: 0.039981007767046474, best_val_loss: 0.03996427345859818, best_auc: 0.9877429229764788\n",
      "Epoch: 27 Batch: 0 Log-loss: 0.03758793696761131\n",
      "Epoch: 27 Batch: 40 Log-loss: 0.05835122987627983\n",
      "Epoch: 27 Batch: 80 Log-loss: 0.03517741337418556\n",
      "Epoch: 27 Batch: 120 Log-loss: 0.0360386036336422\n",
      "Epoch: 27 Batch: 160 Log-loss: 0.039807043969631195\n",
      "Epoch: 27 Batch: 200 Log-loss: 0.03423956409096718\n",
      "Epoch: 27 Batch: 240 Log-loss: 0.029632223770022392\n",
      "Epoch: 27 Batch: 280 Log-loss: 0.04026362672448158\n",
      "Epoch: 27 Batch: 320 Log-loss: 0.02324315719306469\n",
      "Epoch: 27 Batch: 360 Log-loss: 0.04040868952870369\n",
      "Epoch: 27 Batch: 400 Log-loss: 0.042080506682395935\n",
      "Epoch: 27 Batch: 440 Log-loss: 0.03829294070601463\n",
      "Epoch: 27 Batch: 480 Log-loss: 0.03659696504473686\n",
      "Epoch: 27 Batch: 520 Log-loss: 0.0354151614010334\n",
      "Epoch: 27 Batch: 560 Log-loss: 0.03466630354523659\n",
      "Epoch average log-loss: 0.039530687014173185\n",
      "In Epoch: 27, val_loss: 0.040037749407515984, best_val_loss: 0.03996427345859818, best_auc: 0.9877429229764788\n",
      "Epoch: 28 Batch: 0 Log-loss: 0.04483469948172569\n",
      "Epoch: 28 Batch: 40 Log-loss: 0.03978673741221428\n",
      "Epoch: 28 Batch: 80 Log-loss: 0.045243117958307266\n",
      "Epoch: 28 Batch: 120 Log-loss: 0.05911410227417946\n",
      "Epoch: 28 Batch: 160 Log-loss: 0.02874627895653248\n",
      "Epoch: 28 Batch: 200 Log-loss: 0.03637532517313957\n",
      "Epoch: 28 Batch: 240 Log-loss: 0.03432099148631096\n",
      "Epoch: 28 Batch: 280 Log-loss: 0.05297473073005676\n",
      "Epoch: 28 Batch: 320 Log-loss: 0.035236041992902756\n",
      "Epoch: 28 Batch: 360 Log-loss: 0.04090374708175659\n",
      "Epoch: 28 Batch: 400 Log-loss: 0.03626684471964836\n",
      "Epoch: 28 Batch: 440 Log-loss: 0.03247456252574921\n",
      "Epoch: 28 Batch: 480 Log-loss: 0.04415133595466614\n",
      "Epoch: 28 Batch: 520 Log-loss: 0.04740619659423828\n",
      "Epoch: 28 Batch: 560 Log-loss: 0.03435621038079262\n",
      "Epoch average log-loss: 0.03950648122533624\n",
      "In Epoch: 28, val_loss: 0.040232472153312375, best_val_loss: 0.03996427345859818, best_auc: 0.9877429229764788\n",
      "Epoch: 29 Batch: 0 Log-loss: 0.035735491663217545\n",
      "Epoch: 29 Batch: 40 Log-loss: 0.03918835148215294\n",
      "Epoch: 29 Batch: 80 Log-loss: 0.03573858365416527\n",
      "Epoch: 29 Batch: 120 Log-loss: 0.04291146993637085\n",
      "Epoch: 29 Batch: 160 Log-loss: 0.02859480492770672\n",
      "Epoch: 29 Batch: 200 Log-loss: 0.029433680698275566\n",
      "Epoch: 29 Batch: 240 Log-loss: 0.0399349071085453\n",
      "Epoch: 29 Batch: 280 Log-loss: 0.03888300061225891\n",
      "Epoch: 29 Batch: 320 Log-loss: 0.042362332344055176\n",
      "Epoch: 29 Batch: 360 Log-loss: 0.03918435052037239\n",
      "Epoch: 29 Batch: 400 Log-loss: 0.03442839905619621\n",
      "Epoch: 29 Batch: 440 Log-loss: 0.02557438611984253\n",
      "Epoch: 29 Batch: 480 Log-loss: 0.02800808660686016\n",
      "Epoch: 29 Batch: 520 Log-loss: 0.0376136414706707\n",
      "Epoch: 29 Batch: 560 Log-loss: 0.02723045088350773\n",
      "Epoch average log-loss: 0.03950768203940243\n",
      "In Epoch: 29, val_loss: 0.04008036573042432, best_val_loss: 0.03996427345859818, best_auc: 0.9877429229764788\n",
      "Epoch: 30 Batch: 0 Log-loss: 0.037037190049886703\n",
      "Epoch: 30 Batch: 40 Log-loss: 0.043346475809812546\n",
      "Epoch: 30 Batch: 80 Log-loss: 0.037685345858335495\n",
      "Epoch: 30 Batch: 120 Log-loss: 0.029078468680381775\n",
      "Epoch: 30 Batch: 160 Log-loss: 0.052922528237104416\n",
      "Epoch: 30 Batch: 200 Log-loss: 0.03211861103773117\n",
      "Epoch: 30 Batch: 240 Log-loss: 0.029195621609687805\n",
      "Epoch: 30 Batch: 280 Log-loss: 0.052465375512838364\n",
      "Epoch: 30 Batch: 320 Log-loss: 0.022874748334288597\n",
      "Epoch: 30 Batch: 360 Log-loss: 0.02890150435268879\n",
      "Epoch: 30 Batch: 400 Log-loss: 0.03933723270893097\n",
      "Epoch: 30 Batch: 440 Log-loss: 0.04189542308449745\n",
      "Epoch: 30 Batch: 480 Log-loss: 0.034183841198682785\n",
      "Epoch: 30 Batch: 520 Log-loss: 0.04402996227145195\n",
      "Epoch: 30 Batch: 560 Log-loss: 0.051255106925964355\n",
      "Epoch average log-loss: 0.03934485321931009\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 30, val_loss: 0.03983140285767282, best_val_loss: 0.03983140285767282, best_auc: 0.9879188229994146\n",
      "Epoch: 31 Batch: 0 Log-loss: 0.0359894298017025\n",
      "Epoch: 31 Batch: 40 Log-loss: 0.03078700788319111\n",
      "Epoch: 31 Batch: 80 Log-loss: 0.037124019116163254\n",
      "Epoch: 31 Batch: 120 Log-loss: 0.0263868048787117\n",
      "Epoch: 31 Batch: 160 Log-loss: 0.047145772725343704\n",
      "Epoch: 31 Batch: 200 Log-loss: 0.03352247551083565\n",
      "Epoch: 31 Batch: 240 Log-loss: 0.027450701221823692\n",
      "Epoch: 31 Batch: 280 Log-loss: 0.03059925138950348\n",
      "Epoch: 31 Batch: 320 Log-loss: 0.05075136199593544\n",
      "Epoch: 31 Batch: 360 Log-loss: 0.03216498717665672\n",
      "Epoch: 31 Batch: 400 Log-loss: 0.05006559193134308\n",
      "Epoch: 31 Batch: 440 Log-loss: 0.04356275498867035\n",
      "Epoch: 31 Batch: 480 Log-loss: 0.03162863105535507\n",
      "Epoch: 31 Batch: 520 Log-loss: 0.04522879794239998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 Batch: 560 Log-loss: 0.04841785132884979\n",
      "Epoch average log-loss: 0.039468022864977165\n",
      "In Epoch: 31, val_loss: 0.03995899585801625, best_val_loss: 0.03983140285767282, best_auc: 0.9879188229994146\n",
      "Epoch: 32 Batch: 0 Log-loss: 0.042432818561792374\n",
      "Epoch: 32 Batch: 40 Log-loss: 0.04259462654590607\n",
      "Epoch: 32 Batch: 80 Log-loss: 0.030873598530888557\n",
      "Epoch: 32 Batch: 120 Log-loss: 0.046750690788030624\n",
      "Epoch: 32 Batch: 160 Log-loss: 0.04897401109337807\n",
      "Epoch: 32 Batch: 200 Log-loss: 0.03417744114995003\n",
      "Epoch: 32 Batch: 240 Log-loss: 0.03756428882479668\n",
      "Epoch: 32 Batch: 280 Log-loss: 0.03847377002239227\n",
      "Epoch: 32 Batch: 320 Log-loss: 0.027525009587407112\n",
      "Epoch: 32 Batch: 360 Log-loss: 0.044916342943906784\n",
      "Epoch: 32 Batch: 400 Log-loss: 0.03706689551472664\n",
      "Epoch: 32 Batch: 440 Log-loss: 0.025280645117163658\n",
      "Epoch: 32 Batch: 480 Log-loss: 0.03511154279112816\n",
      "Epoch: 32 Batch: 520 Log-loss: 0.05112038180232048\n",
      "Epoch: 32 Batch: 560 Log-loss: 0.037292178720235825\n",
      "Epoch average log-loss: 0.03942644681914576\n",
      "In Epoch: 32, val_loss: 0.04014028248566696, best_val_loss: 0.03983140285767282, best_auc: 0.9879188229994146\n",
      "Epoch: 33 Batch: 0 Log-loss: 0.033625852316617966\n",
      "Epoch: 33 Batch: 40 Log-loss: 0.029636694118380547\n",
      "Epoch: 33 Batch: 80 Log-loss: 0.04232100769877434\n",
      "Epoch: 33 Batch: 120 Log-loss: 0.02577849291265011\n",
      "Epoch: 33 Batch: 160 Log-loss: 0.03179268166422844\n",
      "Epoch: 33 Batch: 200 Log-loss: 0.04892643168568611\n",
      "Epoch: 33 Batch: 240 Log-loss: 0.044747620820999146\n",
      "Epoch: 33 Batch: 280 Log-loss: 0.03946143388748169\n",
      "Epoch: 33 Batch: 320 Log-loss: 0.0391802079975605\n",
      "Epoch: 33 Batch: 360 Log-loss: 0.032574716955423355\n",
      "Epoch: 33 Batch: 400 Log-loss: 0.042183395475149155\n",
      "Epoch: 33 Batch: 440 Log-loss: 0.035108618438243866\n",
      "Epoch: 33 Batch: 480 Log-loss: 0.04271399602293968\n",
      "Epoch: 33 Batch: 520 Log-loss: 0.03142910078167915\n",
      "Epoch: 33 Batch: 560 Log-loss: 0.042696766555309296\n",
      "Epoch average log-loss: 0.03922443899459072\n",
      "In Epoch: 33, val_loss: 0.04007643511881792, best_val_loss: 0.03983140285767282, best_auc: 0.9879188229994146\n",
      "Epoch: 34 Batch: 0 Log-loss: 0.03832075372338295\n",
      "Epoch: 34 Batch: 40 Log-loss: 0.0393049493432045\n",
      "Epoch: 34 Batch: 80 Log-loss: 0.03449748829007149\n",
      "Epoch: 34 Batch: 120 Log-loss: 0.032137490808963776\n",
      "Epoch: 34 Batch: 160 Log-loss: 0.0381593182682991\n",
      "Epoch: 34 Batch: 200 Log-loss: 0.03697237744927406\n",
      "Epoch: 34 Batch: 240 Log-loss: 0.035700563341379166\n",
      "Epoch: 34 Batch: 280 Log-loss: 0.044434189796447754\n",
      "Epoch: 34 Batch: 320 Log-loss: 0.051778730005025864\n",
      "Epoch: 34 Batch: 360 Log-loss: 0.04107866436243057\n",
      "Epoch: 34 Batch: 400 Log-loss: 0.038648996502161026\n",
      "Epoch: 34 Batch: 440 Log-loss: 0.029385825619101524\n",
      "Epoch: 34 Batch: 480 Log-loss: 0.052789997309446335\n",
      "Epoch: 34 Batch: 520 Log-loss: 0.04542684182524681\n",
      "Epoch: 34 Batch: 560 Log-loss: 0.04192531108856201\n",
      "Epoch average log-loss: 0.03908877507118242\n",
      "In Epoch: 34, val_loss: 0.040092539887777896, best_val_loss: 0.03983140285767282, best_auc: 0.9879188229994146\n",
      "Epoch: 35 Batch: 0 Log-loss: 0.04230114817619324\n",
      "Epoch: 35 Batch: 40 Log-loss: 0.047269031405448914\n",
      "Epoch: 35 Batch: 80 Log-loss: 0.03957730531692505\n",
      "Epoch: 35 Batch: 120 Log-loss: 0.03747047856450081\n",
      "Epoch: 35 Batch: 160 Log-loss: 0.03211579844355583\n",
      "Epoch: 35 Batch: 200 Log-loss: 0.04131243750452995\n",
      "Epoch: 35 Batch: 240 Log-loss: 0.023491157218813896\n",
      "Epoch: 35 Batch: 280 Log-loss: 0.039219871163368225\n",
      "Epoch: 35 Batch: 320 Log-loss: 0.046017587184906006\n",
      "Epoch: 35 Batch: 360 Log-loss: 0.031209498643875122\n",
      "Epoch: 35 Batch: 400 Log-loss: 0.05859140679240227\n",
      "Epoch: 35 Batch: 440 Log-loss: 0.03735070303082466\n",
      "Epoch: 35 Batch: 480 Log-loss: 0.0585026741027832\n",
      "Epoch: 35 Batch: 520 Log-loss: 0.0462985597550869\n",
      "Epoch: 35 Batch: 560 Log-loss: 0.030764058232307434\n",
      "Epoch average log-loss: 0.03927493148283767\n",
      "In Epoch: 35, val_loss: 0.04018440882818556, best_val_loss: 0.03983140285767282, best_auc: 0.9879188229994146\n",
      "Epoch: 36 Batch: 0 Log-loss: 0.03535399213433266\n",
      "Epoch: 36 Batch: 40 Log-loss: 0.04334317520260811\n",
      "Epoch: 36 Batch: 80 Log-loss: 0.04498763382434845\n",
      "Epoch: 36 Batch: 120 Log-loss: 0.045065343379974365\n",
      "Epoch: 36 Batch: 160 Log-loss: 0.059988051652908325\n",
      "Epoch: 36 Batch: 200 Log-loss: 0.0317753441631794\n",
      "Epoch: 36 Batch: 240 Log-loss: 0.042217597365379333\n",
      "Epoch: 36 Batch: 280 Log-loss: 0.03289342299103737\n",
      "Epoch: 36 Batch: 320 Log-loss: 0.04084380716085434\n",
      "Epoch: 36 Batch: 360 Log-loss: 0.031538400799036026\n",
      "Epoch: 36 Batch: 400 Log-loss: 0.039176810532808304\n",
      "Epoch: 36 Batch: 440 Log-loss: 0.03102552890777588\n",
      "Epoch: 36 Batch: 480 Log-loss: 0.0405595600605011\n",
      "Epoch: 36 Batch: 520 Log-loss: 0.04325557127594948\n",
      "Epoch: 36 Batch: 560 Log-loss: 0.027584390714764595\n",
      "Epoch average log-loss: 0.03916080642624625\n",
      "In Epoch: 36, val_loss: 0.04038912629478321, best_val_loss: 0.03983140285767282, best_auc: 0.9879188229994146\n",
      "Epoch: 37 Batch: 0 Log-loss: 0.03345296531915665\n",
      "Epoch: 37 Batch: 40 Log-loss: 0.042494092136621475\n",
      "Epoch: 37 Batch: 80 Log-loss: 0.034848399460315704\n",
      "Epoch: 37 Batch: 120 Log-loss: 0.03880247846245766\n",
      "Epoch: 37 Batch: 160 Log-loss: 0.031814366579055786\n",
      "Epoch: 37 Batch: 200 Log-loss: 0.043095480650663376\n",
      "Epoch: 37 Batch: 240 Log-loss: 0.02503994293510914\n",
      "Epoch: 37 Batch: 280 Log-loss: 0.04697970673441887\n",
      "Epoch: 37 Batch: 320 Log-loss: 0.05419091880321503\n",
      "Epoch: 37 Batch: 360 Log-loss: 0.045751865953207016\n",
      "Epoch: 37 Batch: 400 Log-loss: 0.037121254950761795\n",
      "Epoch: 37 Batch: 440 Log-loss: 0.0238229651004076\n",
      "Epoch: 37 Batch: 480 Log-loss: 0.05694602057337761\n",
      "Epoch: 37 Batch: 520 Log-loss: 0.0359659306704998\n",
      "Epoch: 37 Batch: 560 Log-loss: 0.039398442953825\n",
      "Epoch average log-loss: 0.03905039356821882\n",
      "In Epoch: 37, val_loss: 0.039998888110883465, best_val_loss: 0.03983140285767282, best_auc: 0.9879188229994146\n",
      "Epoch: 38 Batch: 0 Log-loss: 0.048141662031412125\n",
      "Epoch: 38 Batch: 40 Log-loss: 0.028576625511050224\n",
      "Epoch: 38 Batch: 80 Log-loss: 0.04953224956989288\n",
      "Epoch: 38 Batch: 120 Log-loss: 0.038165654987096786\n",
      "Epoch: 38 Batch: 160 Log-loss: 0.03425068408250809\n",
      "Epoch: 38 Batch: 200 Log-loss: 0.0365946926176548\n",
      "Epoch: 38 Batch: 240 Log-loss: 0.03229176998138428\n",
      "Epoch: 38 Batch: 280 Log-loss: 0.03701744228601456\n",
      "Epoch: 38 Batch: 320 Log-loss: 0.041420403867959976\n",
      "Epoch: 38 Batch: 360 Log-loss: 0.04808001592755318\n",
      "Epoch: 38 Batch: 400 Log-loss: 0.0349242277443409\n",
      "Epoch: 38 Batch: 440 Log-loss: 0.028934309259057045\n",
      "Epoch: 38 Batch: 480 Log-loss: 0.04149786755442619\n",
      "Epoch: 38 Batch: 520 Log-loss: 0.04446376860141754\n",
      "Epoch: 38 Batch: 560 Log-loss: 0.03909764811396599\n",
      "Epoch average log-loss: 0.03870872035622597\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 38, val_loss: 0.03979638476401761, best_val_loss: 0.03979638476401761, best_auc: 0.9877723851563719\n",
      "Epoch: 39 Batch: 0 Log-loss: 0.034244392067193985\n",
      "Epoch: 39 Batch: 40 Log-loss: 0.040340445935726166\n",
      "Epoch: 39 Batch: 80 Log-loss: 0.02861863374710083\n",
      "Epoch: 39 Batch: 120 Log-loss: 0.03634021058678627\n",
      "Epoch: 39 Batch: 160 Log-loss: 0.025865502655506134\n",
      "Epoch: 39 Batch: 200 Log-loss: 0.027768699452280998\n",
      "Epoch: 39 Batch: 240 Log-loss: 0.0429447703063488\n",
      "Epoch: 39 Batch: 280 Log-loss: 0.02724318765103817\n",
      "Epoch: 39 Batch: 320 Log-loss: 0.0536482073366642\n",
      "Epoch: 39 Batch: 360 Log-loss: 0.041540514677762985\n",
      "Epoch: 39 Batch: 400 Log-loss: 0.033699359744787216\n",
      "Epoch: 39 Batch: 440 Log-loss: 0.032584674656391144\n",
      "Epoch: 39 Batch: 480 Log-loss: 0.038601335138082504\n",
      "Epoch: 39 Batch: 520 Log-loss: 0.05477779731154442\n",
      "Epoch: 39 Batch: 560 Log-loss: 0.04725481942296028\n",
      "Epoch average log-loss: 0.03901356613662626\n",
      "In Epoch: 39, val_loss: 0.03994117335095235, best_val_loss: 0.03979638476401761, best_auc: 0.9877723851563719\n",
      "Epoch: 40 Batch: 0 Log-loss: 0.03820307180285454\n",
      "Epoch: 40 Batch: 40 Log-loss: 0.021349258720874786\n",
      "Epoch: 40 Batch: 80 Log-loss: 0.03337845206260681\n",
      "Epoch: 40 Batch: 120 Log-loss: 0.02769455313682556\n",
      "Epoch: 40 Batch: 160 Log-loss: 0.03924078494310379\n",
      "Epoch: 40 Batch: 200 Log-loss: 0.03232233226299286\n",
      "Epoch: 40 Batch: 240 Log-loss: 0.04273989796638489\n",
      "Epoch: 40 Batch: 280 Log-loss: 0.049452852457761765\n",
      "Epoch: 40 Batch: 320 Log-loss: 0.03147707134485245\n",
      "Epoch: 40 Batch: 360 Log-loss: 0.03123432584106922\n",
      "Epoch: 40 Batch: 400 Log-loss: 0.03053966723382473\n",
      "Epoch: 40 Batch: 440 Log-loss: 0.03794754669070244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 Batch: 480 Log-loss: 0.043814241886138916\n",
      "Epoch: 40 Batch: 520 Log-loss: 0.02988274395465851\n",
      "Epoch: 40 Batch: 560 Log-loss: 0.0433126837015152\n",
      "Epoch average log-loss: 0.038878118130378427\n",
      "In Epoch: 40, val_loss: 0.039897821663461126, best_val_loss: 0.03979638476401761, best_auc: 0.9877723851563719\n",
      "Epoch: 41 Batch: 0 Log-loss: 0.036502476781606674\n",
      "Epoch: 41 Batch: 40 Log-loss: 0.03334697708487511\n",
      "Epoch: 41 Batch: 80 Log-loss: 0.05660061910748482\n",
      "Epoch: 41 Batch: 120 Log-loss: 0.041099730879068375\n",
      "Epoch: 41 Batch: 160 Log-loss: 0.04040705785155296\n",
      "Epoch: 41 Batch: 200 Log-loss: 0.040701113641262054\n",
      "Epoch: 41 Batch: 240 Log-loss: 0.0401398129761219\n",
      "Epoch: 41 Batch: 280 Log-loss: 0.03750528395175934\n",
      "Epoch: 41 Batch: 320 Log-loss: 0.04438940808176994\n",
      "Epoch: 41 Batch: 360 Log-loss: 0.030881604179739952\n",
      "Epoch: 41 Batch: 400 Log-loss: 0.048665035516023636\n",
      "Epoch: 41 Batch: 440 Log-loss: 0.03181444853544235\n",
      "Epoch: 41 Batch: 480 Log-loss: 0.030766338109970093\n",
      "Epoch: 41 Batch: 520 Log-loss: 0.043548334389925\n",
      "Epoch: 41 Batch: 560 Log-loss: 0.04297148436307907\n",
      "Epoch average log-loss: 0.03893609113791691\n",
      "In Epoch: 41, val_loss: 0.039805811677893425, best_val_loss: 0.03979638476401761, best_auc: 0.9877723851563719\n",
      "Epoch: 42 Batch: 0 Log-loss: 0.0460953563451767\n",
      "Epoch: 42 Batch: 40 Log-loss: 0.041659172624349594\n",
      "Epoch: 42 Batch: 80 Log-loss: 0.030680259689688683\n",
      "Epoch: 42 Batch: 120 Log-loss: 0.04370712861418724\n",
      "Epoch: 42 Batch: 160 Log-loss: 0.05908925458788872\n",
      "Epoch: 42 Batch: 200 Log-loss: 0.03853027895092964\n",
      "Epoch: 42 Batch: 240 Log-loss: 0.03695971518754959\n",
      "Epoch: 42 Batch: 280 Log-loss: 0.042295441031455994\n",
      "Epoch: 42 Batch: 320 Log-loss: 0.02975926548242569\n",
      "Epoch: 42 Batch: 360 Log-loss: 0.02797282300889492\n",
      "Epoch: 42 Batch: 400 Log-loss: 0.03572135046124458\n",
      "Epoch: 42 Batch: 440 Log-loss: 0.04684798792004585\n",
      "Epoch: 42 Batch: 480 Log-loss: 0.04982692375779152\n",
      "Epoch: 42 Batch: 520 Log-loss: 0.03498860076069832\n",
      "Epoch: 42 Batch: 560 Log-loss: 0.03166864812374115\n",
      "Epoch average log-loss: 0.03908126611661698\n",
      "In Epoch: 42, val_loss: 0.04011027852071691, best_val_loss: 0.03979638476401761, best_auc: 0.9877723851563719\n",
      "Epoch: 43 Batch: 0 Log-loss: 0.04851314052939415\n",
      "Epoch: 43 Batch: 40 Log-loss: 0.029886044561862946\n",
      "Epoch: 43 Batch: 80 Log-loss: 0.036729197949171066\n",
      "Epoch: 43 Batch: 120 Log-loss: 0.047445956617593765\n",
      "Epoch: 43 Batch: 160 Log-loss: 0.046832937747240067\n",
      "Epoch: 43 Batch: 200 Log-loss: 0.03289732336997986\n",
      "Epoch: 43 Batch: 240 Log-loss: 0.03211545571684837\n",
      "Epoch: 43 Batch: 280 Log-loss: 0.04006122052669525\n",
      "Epoch: 43 Batch: 320 Log-loss: 0.036466870456933975\n",
      "Epoch: 43 Batch: 360 Log-loss: 0.029388166964054108\n",
      "Epoch: 43 Batch: 400 Log-loss: 0.05890796706080437\n",
      "Epoch: 43 Batch: 440 Log-loss: 0.04890149459242821\n",
      "Epoch: 43 Batch: 480 Log-loss: 0.03583268076181412\n",
      "Epoch: 43 Batch: 520 Log-loss: 0.029096374288201332\n",
      "Epoch: 43 Batch: 560 Log-loss: 0.030249260365962982\n",
      "Epoch average log-loss: 0.0387776074897764\n",
      "In Epoch: 43, val_loss: 0.03990768520028538, best_val_loss: 0.03979638476401761, best_auc: 0.9877723851563719\n",
      "Epoch: 44 Batch: 0 Log-loss: 0.05468989908695221\n",
      "Epoch: 44 Batch: 40 Log-loss: 0.036543991416692734\n",
      "Epoch: 44 Batch: 80 Log-loss: 0.0301522184163332\n",
      "Epoch: 44 Batch: 120 Log-loss: 0.05153349041938782\n",
      "Epoch: 44 Batch: 160 Log-loss: 0.027405574917793274\n",
      "Epoch: 44 Batch: 200 Log-loss: 0.059858765453100204\n",
      "Epoch: 44 Batch: 240 Log-loss: 0.034095246344804764\n",
      "Epoch: 44 Batch: 280 Log-loss: 0.041405510157346725\n",
      "Epoch: 44 Batch: 320 Log-loss: 0.04731854796409607\n",
      "Epoch: 44 Batch: 360 Log-loss: 0.041961997747421265\n",
      "Epoch: 44 Batch: 400 Log-loss: 0.03714708611369133\n",
      "Epoch: 44 Batch: 440 Log-loss: 0.04274626448750496\n",
      "Epoch: 44 Batch: 480 Log-loss: 0.02753138728439808\n",
      "Epoch: 44 Batch: 520 Log-loss: 0.045926135033369064\n",
      "Epoch: 44 Batch: 560 Log-loss: 0.040969133377075195\n",
      "Epoch average log-loss: 0.03887001207497503\n",
      "In Epoch: 44, val_loss: 0.04003831172520008, best_val_loss: 0.03979638476401761, best_auc: 0.9877723851563719\n",
      "Epoch: 45 Batch: 0 Log-loss: 0.03107129968702793\n",
      "Epoch: 45 Batch: 40 Log-loss: 0.05054568871855736\n",
      "Epoch: 45 Batch: 80 Log-loss: 0.027967149391770363\n",
      "Epoch: 45 Batch: 120 Log-loss: 0.03193287178874016\n",
      "Epoch: 45 Batch: 160 Log-loss: 0.03202786669135094\n",
      "Epoch: 45 Batch: 200 Log-loss: 0.040858566761016846\n",
      "Epoch: 45 Batch: 240 Log-loss: 0.02802850864827633\n",
      "Epoch: 45 Batch: 280 Log-loss: 0.05020627751946449\n",
      "Epoch: 45 Batch: 320 Log-loss: 0.029508111998438835\n",
      "Epoch: 45 Batch: 360 Log-loss: 0.03788350522518158\n",
      "Epoch: 45 Batch: 400 Log-loss: 0.036331143230199814\n",
      "Epoch: 45 Batch: 440 Log-loss: 0.03399582579731941\n",
      "Epoch: 45 Batch: 480 Log-loss: 0.03853209689259529\n",
      "Epoch: 45 Batch: 520 Log-loss: 0.040581997483968735\n",
      "Epoch: 45 Batch: 560 Log-loss: 0.03892460837960243\n",
      "Epoch average log-loss: 0.03892026126850397\n",
      "In Epoch: 45, val_loss: 0.04008442337568566, best_val_loss: 0.03979638476401761, best_auc: 0.9877723851563719\n",
      "Epoch: 46 Batch: 0 Log-loss: 0.04209604859352112\n",
      "Epoch: 46 Batch: 40 Log-loss: 0.03358093276619911\n",
      "Epoch: 46 Batch: 80 Log-loss: 0.03240884467959404\n",
      "Epoch: 46 Batch: 120 Log-loss: 0.04293153062462807\n",
      "Epoch: 46 Batch: 160 Log-loss: 0.029314497485756874\n",
      "Epoch: 46 Batch: 200 Log-loss: 0.039890747517347336\n",
      "Epoch: 46 Batch: 240 Log-loss: 0.044240038841962814\n",
      "Epoch: 46 Batch: 280 Log-loss: 0.03714222088456154\n",
      "Epoch: 46 Batch: 320 Log-loss: 0.03776593878865242\n",
      "Epoch: 46 Batch: 360 Log-loss: 0.043859273195266724\n",
      "Epoch: 46 Batch: 400 Log-loss: 0.03133225813508034\n",
      "Epoch: 46 Batch: 440 Log-loss: 0.04334026575088501\n",
      "Epoch: 46 Batch: 480 Log-loss: 0.0298153106123209\n",
      "Epoch: 46 Batch: 520 Log-loss: 0.028052011504769325\n",
      "Epoch: 46 Batch: 560 Log-loss: 0.03153933957219124\n",
      "Epoch average log-loss: 0.038844803351509784\n",
      "In Epoch: 46, val_loss: 0.03994923673998191, best_val_loss: 0.03979638476401761, best_auc: 0.9877723851563719\n",
      "Epoch: 47 Batch: 0 Log-loss: 0.03749443218111992\n",
      "Epoch: 47 Batch: 40 Log-loss: 0.03181804344058037\n",
      "Epoch: 47 Batch: 80 Log-loss: 0.05028708279132843\n",
      "Epoch: 47 Batch: 120 Log-loss: 0.04325990751385689\n",
      "Epoch: 47 Batch: 160 Log-loss: 0.03330279886722565\n",
      "Epoch: 47 Batch: 200 Log-loss: 0.03625509887933731\n",
      "Epoch: 47 Batch: 240 Log-loss: 0.034293267875909805\n",
      "Epoch: 47 Batch: 280 Log-loss: 0.040795400738716125\n",
      "Epoch: 47 Batch: 320 Log-loss: 0.049339473247528076\n",
      "Epoch: 47 Batch: 360 Log-loss: 0.03633690997958183\n",
      "Epoch: 47 Batch: 400 Log-loss: 0.030517326667904854\n",
      "Epoch: 47 Batch: 440 Log-loss: 0.048618197441101074\n",
      "Epoch: 47 Batch: 480 Log-loss: 0.03180719166994095\n",
      "Epoch: 47 Batch: 520 Log-loss: 0.03443780541419983\n",
      "Epoch: 47 Batch: 560 Log-loss: 0.03412947431206703\n",
      "Epoch average log-loss: 0.0387629050362323\n",
      "In Epoch: 47, val_loss: 0.040265752481127876, best_val_loss: 0.03979638476401761, best_auc: 0.9877723851563719\n",
      "Epoch: 48 Batch: 0 Log-loss: 0.03910103812813759\n",
      "Epoch: 48 Batch: 40 Log-loss: 0.036507245153188705\n",
      "Epoch: 48 Batch: 80 Log-loss: 0.03804558888077736\n",
      "Epoch: 48 Batch: 120 Log-loss: 0.045930955559015274\n",
      "Epoch: 48 Batch: 160 Log-loss: 0.04724631831049919\n",
      "Epoch: 48 Batch: 200 Log-loss: 0.04354998096823692\n",
      "Epoch: 48 Batch: 240 Log-loss: 0.03898170217871666\n",
      "Epoch: 48 Batch: 280 Log-loss: 0.03447742387652397\n",
      "Epoch: 48 Batch: 320 Log-loss: 0.04632306098937988\n",
      "Epoch: 48 Batch: 360 Log-loss: 0.036892954260110855\n",
      "Epoch: 48 Batch: 400 Log-loss: 0.040629640221595764\n",
      "Epoch: 48 Batch: 440 Log-loss: 0.0496743880212307\n",
      "Epoch: 48 Batch: 480 Log-loss: 0.0334775373339653\n",
      "Epoch: 48 Batch: 520 Log-loss: 0.04217268154025078\n",
      "Epoch: 48 Batch: 560 Log-loss: 0.04351704940199852\n",
      "Epoch average log-loss: 0.03873590098228306\n",
      "In Epoch: 48, val_loss: 0.04020573933907426, best_val_loss: 0.03979638476401761, best_auc: 0.9877723851563719\n",
      "Epoch: 49 Batch: 0 Log-loss: 0.03920203819870949\n",
      "Epoch: 49 Batch: 40 Log-loss: 0.054040804505348206\n",
      "Epoch: 49 Batch: 80 Log-loss: 0.038652341812849045\n",
      "Epoch: 49 Batch: 120 Log-loss: 0.02749592810869217\n",
      "Epoch: 49 Batch: 160 Log-loss: 0.02660669945180416\n",
      "Epoch: 49 Batch: 200 Log-loss: 0.026595287024974823\n",
      "Epoch: 49 Batch: 240 Log-loss: 0.037948284298181534\n",
      "Epoch: 49 Batch: 280 Log-loss: 0.03415548428893089\n",
      "Epoch: 49 Batch: 320 Log-loss: 0.05199163034558296\n",
      "Epoch: 49 Batch: 360 Log-loss: 0.04940921068191528\n",
      "Epoch: 49 Batch: 400 Log-loss: 0.04296426475048065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 Batch: 440 Log-loss: 0.028102077543735504\n",
      "Epoch: 49 Batch: 480 Log-loss: 0.0458376444876194\n",
      "Epoch: 49 Batch: 520 Log-loss: 0.03974338248372078\n",
      "Epoch: 49 Batch: 560 Log-loss: 0.05366545170545578\n",
      "Epoch average log-loss: 0.038693537614640915\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 49, val_loss: 0.039745029597783386, best_val_loss: 0.039745029597783386, best_auc: 0.9876435400755758\n",
      "Epoch: 50 Batch: 0 Log-loss: 0.03581385686993599\n",
      "Epoch: 50 Batch: 40 Log-loss: 0.03389548882842064\n",
      "Epoch: 50 Batch: 80 Log-loss: 0.03419680520892143\n",
      "Epoch: 50 Batch: 120 Log-loss: 0.05455152690410614\n",
      "Epoch: 50 Batch: 160 Log-loss: 0.03931952640414238\n",
      "Epoch: 50 Batch: 200 Log-loss: 0.030357418581843376\n",
      "Epoch: 50 Batch: 240 Log-loss: 0.03431927040219307\n",
      "Epoch: 50 Batch: 280 Log-loss: 0.05310246720910072\n",
      "Epoch: 50 Batch: 320 Log-loss: 0.03371511027216911\n",
      "Epoch: 50 Batch: 360 Log-loss: 0.03383579105138779\n",
      "Epoch: 50 Batch: 400 Log-loss: 0.047167111188173294\n",
      "Epoch: 50 Batch: 440 Log-loss: 0.035698916763067245\n",
      "Epoch: 50 Batch: 480 Log-loss: 0.03766161575913429\n",
      "Epoch: 50 Batch: 520 Log-loss: 0.03858223184943199\n",
      "Epoch: 50 Batch: 560 Log-loss: 0.033584173768758774\n",
      "Epoch average log-loss: 0.038858525285364265\n",
      "In Epoch: 50, val_loss: 0.04011872865285714, best_val_loss: 0.039745029597783386, best_auc: 0.9876435400755758\n",
      "Epoch: 51 Batch: 0 Log-loss: 0.03702489286661148\n",
      "Epoch: 51 Batch: 40 Log-loss: 0.0327049195766449\n",
      "Epoch: 51 Batch: 80 Log-loss: 0.03810393065214157\n",
      "Epoch: 51 Batch: 120 Log-loss: 0.046459928154945374\n",
      "Epoch: 51 Batch: 160 Log-loss: 0.040181878954172134\n",
      "Epoch: 51 Batch: 200 Log-loss: 0.03504026308655739\n",
      "Epoch: 51 Batch: 240 Log-loss: 0.0335114561021328\n",
      "Epoch: 51 Batch: 280 Log-loss: 0.03847547993063927\n",
      "Epoch: 51 Batch: 320 Log-loss: 0.03558661416172981\n",
      "Epoch: 51 Batch: 360 Log-loss: 0.03804577514529228\n",
      "Epoch: 51 Batch: 400 Log-loss: 0.03257334977388382\n",
      "Epoch: 51 Batch: 440 Log-loss: 0.02375064603984356\n",
      "Epoch: 51 Batch: 480 Log-loss: 0.039035528898239136\n",
      "Epoch: 51 Batch: 520 Log-loss: 0.03398443013429642\n",
      "Epoch: 51 Batch: 560 Log-loss: 0.03721725568175316\n",
      "Epoch average log-loss: 0.03885360608609127\n",
      "In Epoch: 51, val_loss: 0.04020484863825757, best_val_loss: 0.039745029597783386, best_auc: 0.9876435400755758\n",
      "Epoch: 52 Batch: 0 Log-loss: 0.04460987076163292\n",
      "Epoch: 52 Batch: 40 Log-loss: 0.047534968703985214\n",
      "Epoch: 52 Batch: 80 Log-loss: 0.03277524933218956\n",
      "Epoch: 52 Batch: 120 Log-loss: 0.04625130072236061\n",
      "Epoch: 52 Batch: 160 Log-loss: 0.05346241965889931\n",
      "Epoch: 52 Batch: 200 Log-loss: 0.04621478542685509\n",
      "Epoch: 52 Batch: 240 Log-loss: 0.04090144857764244\n",
      "Epoch: 52 Batch: 280 Log-loss: 0.045142024755477905\n",
      "Epoch: 52 Batch: 320 Log-loss: 0.03951441869139671\n",
      "Epoch: 52 Batch: 360 Log-loss: 0.04614205285906792\n",
      "Epoch: 52 Batch: 400 Log-loss: 0.029942667111754417\n",
      "Epoch: 52 Batch: 440 Log-loss: 0.033642079681158066\n",
      "Epoch: 52 Batch: 480 Log-loss: 0.04913679137825966\n",
      "Epoch: 52 Batch: 520 Log-loss: 0.046777814626693726\n",
      "Epoch: 52 Batch: 560 Log-loss: 0.04676888883113861\n",
      "Epoch average log-loss: 0.0389346845314971\n",
      "In Epoch: 52, val_loss: 0.04009026931458368, best_val_loss: 0.039745029597783386, best_auc: 0.9876435400755758\n",
      "Epoch: 53 Batch: 0 Log-loss: 0.04519914463162422\n",
      "Epoch: 53 Batch: 40 Log-loss: 0.04165196046233177\n",
      "Epoch: 53 Batch: 80 Log-loss: 0.048695679754018784\n",
      "Epoch: 53 Batch: 120 Log-loss: 0.04448371008038521\n",
      "Epoch: 53 Batch: 160 Log-loss: 0.040132541209459305\n",
      "Epoch: 53 Batch: 200 Log-loss: 0.04924221336841583\n",
      "Epoch: 53 Batch: 240 Log-loss: 0.03225201368331909\n",
      "Epoch: 53 Batch: 280 Log-loss: 0.03953682258725166\n",
      "Epoch: 53 Batch: 320 Log-loss: 0.034904949367046356\n",
      "Epoch: 53 Batch: 360 Log-loss: 0.04344372823834419\n",
      "Epoch: 53 Batch: 400 Log-loss: 0.03194239363074303\n",
      "Epoch: 53 Batch: 440 Log-loss: 0.036583054810762405\n",
      "Epoch: 53 Batch: 480 Log-loss: 0.0334760956466198\n",
      "Epoch: 53 Batch: 520 Log-loss: 0.03371984139084816\n",
      "Epoch: 53 Batch: 560 Log-loss: 0.02493334375321865\n",
      "Epoch average log-loss: 0.03872866344837738\n",
      "In Epoch: 53, val_loss: 0.0400888871237215, best_val_loss: 0.039745029597783386, best_auc: 0.9876435400755758\n",
      "Epoch: 54 Batch: 0 Log-loss: 0.0466202087700367\n",
      "Epoch: 54 Batch: 40 Log-loss: 0.026648327708244324\n",
      "Epoch: 54 Batch: 80 Log-loss: 0.03956491872668266\n",
      "Epoch: 54 Batch: 120 Log-loss: 0.027470864355564117\n",
      "Epoch: 54 Batch: 160 Log-loss: 0.04914608225226402\n",
      "Epoch: 54 Batch: 200 Log-loss: 0.03132759407162666\n",
      "Epoch: 54 Batch: 240 Log-loss: 0.029680028557777405\n",
      "Epoch: 54 Batch: 280 Log-loss: 0.0404377207159996\n",
      "Epoch: 54 Batch: 320 Log-loss: 0.03112407959997654\n",
      "Epoch: 54 Batch: 360 Log-loss: 0.03819756954908371\n",
      "Epoch: 54 Batch: 400 Log-loss: 0.03397535905241966\n",
      "Epoch: 54 Batch: 440 Log-loss: 0.03032216615974903\n",
      "Epoch: 54 Batch: 480 Log-loss: 0.045362308621406555\n",
      "Epoch: 54 Batch: 520 Log-loss: 0.03907156363129616\n",
      "Epoch: 54 Batch: 560 Log-loss: 0.027688130736351013\n",
      "Epoch average log-loss: 0.03869868882466108\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 54, val_loss: 0.03946678895966707, best_val_loss: 0.03946678895966707, best_auc: 0.9878831375442512\n",
      "Epoch: 55 Batch: 0 Log-loss: 0.03860600292682648\n",
      "Epoch: 55 Batch: 40 Log-loss: 0.030380576848983765\n",
      "Epoch: 55 Batch: 80 Log-loss: 0.04765327647328377\n",
      "Epoch: 55 Batch: 120 Log-loss: 0.026153916493058205\n",
      "Epoch: 55 Batch: 160 Log-loss: 0.03969663754105568\n",
      "Epoch: 55 Batch: 200 Log-loss: 0.04224729165434837\n",
      "Epoch: 55 Batch: 240 Log-loss: 0.03410211578011513\n",
      "Epoch: 55 Batch: 280 Log-loss: 0.029941337183117867\n",
      "Epoch: 55 Batch: 320 Log-loss: 0.042652904987335205\n",
      "Epoch: 55 Batch: 360 Log-loss: 0.038815151900053024\n",
      "Epoch: 55 Batch: 400 Log-loss: 0.056063488125801086\n",
      "Epoch: 55 Batch: 440 Log-loss: 0.03705333173274994\n",
      "Epoch: 55 Batch: 480 Log-loss: 0.03888273984193802\n",
      "Epoch: 55 Batch: 520 Log-loss: 0.03944923356175423\n",
      "Epoch: 55 Batch: 560 Log-loss: 0.03179282322525978\n",
      "Epoch average log-loss: 0.03843838252858924\n",
      "In Epoch: 55, val_loss: 0.03966185911611976, best_val_loss: 0.03946678895966707, best_auc: 0.9878831375442512\n",
      "Epoch: 56 Batch: 0 Log-loss: 0.052435021847486496\n",
      "Epoch: 56 Batch: 40 Log-loss: 0.06062133237719536\n",
      "Epoch: 56 Batch: 80 Log-loss: 0.036908578127622604\n",
      "Epoch: 56 Batch: 120 Log-loss: 0.03779483214020729\n",
      "Epoch: 56 Batch: 160 Log-loss: 0.02729201503098011\n",
      "Epoch: 56 Batch: 200 Log-loss: 0.031006425619125366\n",
      "Epoch: 56 Batch: 240 Log-loss: 0.04015609622001648\n",
      "Epoch: 56 Batch: 280 Log-loss: 0.04377215728163719\n",
      "Epoch: 56 Batch: 320 Log-loss: 0.03142746537923813\n",
      "Epoch: 56 Batch: 360 Log-loss: 0.035126373171806335\n",
      "Epoch: 56 Batch: 400 Log-loss: 0.04080553725361824\n",
      "Epoch: 56 Batch: 440 Log-loss: 0.03829595074057579\n",
      "Epoch: 56 Batch: 480 Log-loss: 0.03393537551164627\n",
      "Epoch: 56 Batch: 520 Log-loss: 0.031411368399858475\n",
      "Epoch: 56 Batch: 560 Log-loss: 0.04191961884498596\n",
      "Epoch average log-loss: 0.03856922596626516\n",
      "In Epoch: 56, val_loss: 0.04007639735235344, best_val_loss: 0.03946678895966707, best_auc: 0.9878831375442512\n",
      "Epoch: 57 Batch: 0 Log-loss: 0.04849210008978844\n",
      "Epoch: 57 Batch: 40 Log-loss: 0.03662547469139099\n",
      "Epoch: 57 Batch: 80 Log-loss: 0.043872687965631485\n",
      "Epoch: 57 Batch: 120 Log-loss: 0.0499403215944767\n",
      "Epoch: 57 Batch: 160 Log-loss: 0.027685903012752533\n",
      "Epoch: 57 Batch: 200 Log-loss: 0.04375268518924713\n",
      "Epoch: 57 Batch: 240 Log-loss: 0.04065123572945595\n",
      "Epoch: 57 Batch: 280 Log-loss: 0.03221229091286659\n",
      "Epoch: 57 Batch: 320 Log-loss: 0.03299730643630028\n",
      "Epoch: 57 Batch: 360 Log-loss: 0.04576517641544342\n",
      "Epoch: 57 Batch: 400 Log-loss: 0.034833718091249466\n",
      "Epoch: 57 Batch: 440 Log-loss: 0.04262910410761833\n",
      "Epoch: 57 Batch: 480 Log-loss: 0.032489027827978134\n",
      "Epoch: 57 Batch: 520 Log-loss: 0.045777712017297745\n",
      "Epoch: 57 Batch: 560 Log-loss: 0.051142629235982895\n",
      "Epoch average log-loss: 0.03850414398392396\n",
      "In Epoch: 57, val_loss: 0.03982162755380903, best_val_loss: 0.03946678895966707, best_auc: 0.9878831375442512\n",
      "Epoch: 58 Batch: 0 Log-loss: 0.025433359667658806\n",
      "Epoch: 58 Batch: 40 Log-loss: 0.0370066799223423\n",
      "Epoch: 58 Batch: 80 Log-loss: 0.05609874799847603\n",
      "Epoch: 58 Batch: 120 Log-loss: 0.03664032742381096\n",
      "Epoch: 58 Batch: 160 Log-loss: 0.02940203808248043\n",
      "Epoch: 58 Batch: 200 Log-loss: 0.029327159747481346\n",
      "Epoch: 58 Batch: 240 Log-loss: 0.04569302871823311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58 Batch: 280 Log-loss: 0.03231087699532509\n",
      "Epoch: 58 Batch: 320 Log-loss: 0.03578801080584526\n",
      "Epoch: 58 Batch: 360 Log-loss: 0.044385697692632675\n",
      "Epoch: 58 Batch: 400 Log-loss: 0.03543980047106743\n",
      "Epoch: 58 Batch: 440 Log-loss: 0.03648184612393379\n",
      "Epoch: 58 Batch: 480 Log-loss: 0.033902473747730255\n",
      "Epoch: 58 Batch: 520 Log-loss: 0.03670363873243332\n",
      "Epoch: 58 Batch: 560 Log-loss: 0.04079356789588928\n",
      "Epoch average log-loss: 0.038637869785140666\n",
      "In Epoch: 58, val_loss: 0.039939581177519605, best_val_loss: 0.03946678895966707, best_auc: 0.9878831375442512\n",
      "Epoch: 59 Batch: 0 Log-loss: 0.03637649863958359\n",
      "Epoch: 59 Batch: 40 Log-loss: 0.044730618596076965\n",
      "Epoch: 59 Batch: 80 Log-loss: 0.04293164238333702\n",
      "Epoch: 59 Batch: 120 Log-loss: 0.02865287847816944\n",
      "Epoch: 59 Batch: 160 Log-loss: 0.031877268105745316\n",
      "Epoch: 59 Batch: 200 Log-loss: 0.04718269780278206\n",
      "Epoch: 59 Batch: 240 Log-loss: 0.03435098007321358\n",
      "Epoch: 59 Batch: 280 Log-loss: 0.025717003270983696\n",
      "Epoch: 59 Batch: 320 Log-loss: 0.03342995420098305\n",
      "Epoch: 59 Batch: 360 Log-loss: 0.04508256912231445\n",
      "Epoch: 59 Batch: 400 Log-loss: 0.036383096128702164\n",
      "Epoch: 59 Batch: 440 Log-loss: 0.03273927792906761\n",
      "Epoch: 59 Batch: 480 Log-loss: 0.03204873576760292\n",
      "Epoch: 59 Batch: 520 Log-loss: 0.03734888881444931\n",
      "Epoch: 59 Batch: 560 Log-loss: 0.025465738028287888\n",
      "Epoch average log-loss: 0.0386564644286409\n",
      "In Epoch: 59, val_loss: 0.04001818534317603, best_val_loss: 0.03946678895966707, best_auc: 0.9878831375442512\n",
      "Epoch: 60 Batch: 0 Log-loss: 0.035529643297195435\n",
      "Epoch: 60 Batch: 40 Log-loss: 0.028694815933704376\n",
      "Epoch: 60 Batch: 80 Log-loss: 0.037525925785303116\n",
      "Epoch: 60 Batch: 120 Log-loss: 0.03508412837982178\n",
      "Epoch: 60 Batch: 160 Log-loss: 0.04259108379483223\n",
      "Epoch: 60 Batch: 200 Log-loss: 0.029987884685397148\n",
      "Epoch: 60 Batch: 240 Log-loss: 0.030731476843357086\n",
      "Epoch: 60 Batch: 280 Log-loss: 0.029022522270679474\n",
      "Epoch: 60 Batch: 320 Log-loss: 0.030545851215720177\n",
      "Epoch: 60 Batch: 360 Log-loss: 0.03771883621811867\n",
      "Epoch: 60 Batch: 400 Log-loss: 0.039327677339315414\n",
      "Epoch: 60 Batch: 440 Log-loss: 0.05000191926956177\n",
      "Epoch: 60 Batch: 480 Log-loss: 0.028939127922058105\n",
      "Epoch: 60 Batch: 520 Log-loss: 0.04489782825112343\n",
      "Epoch: 60 Batch: 560 Log-loss: 0.05754817649722099\n",
      "Epoch average log-loss: 0.03873092517855444\n",
      "In Epoch: 60, val_loss: 0.040108467654732284, best_val_loss: 0.03946678895966707, best_auc: 0.9878831375442512\n",
      "Epoch: 61 Batch: 0 Log-loss: 0.04446098580956459\n",
      "Epoch: 61 Batch: 40 Log-loss: 0.05069933831691742\n",
      "Epoch: 61 Batch: 80 Log-loss: 0.038908328860998154\n",
      "Epoch: 61 Batch: 120 Log-loss: 0.03164362162351608\n",
      "Epoch: 61 Batch: 160 Log-loss: 0.035338617861270905\n",
      "Epoch: 61 Batch: 200 Log-loss: 0.038640622049570084\n",
      "Epoch: 61 Batch: 240 Log-loss: 0.03641294315457344\n",
      "Epoch: 61 Batch: 280 Log-loss: 0.03248174116015434\n",
      "Epoch: 61 Batch: 320 Log-loss: 0.04014033451676369\n",
      "Epoch: 61 Batch: 360 Log-loss: 0.031913500279188156\n",
      "Epoch: 61 Batch: 400 Log-loss: 0.03584864363074303\n",
      "Epoch: 61 Batch: 440 Log-loss: 0.04756075516343117\n",
      "Epoch: 61 Batch: 480 Log-loss: 0.04820430278778076\n",
      "Epoch: 61 Batch: 520 Log-loss: 0.03738408163189888\n",
      "Epoch: 61 Batch: 560 Log-loss: 0.030131712555885315\n",
      "Epoch average log-loss: 0.038680362145948624\n",
      "In Epoch: 61, val_loss: 0.039930214700256456, best_val_loss: 0.03946678895966707, best_auc: 0.9878831375442512\n",
      "Epoch: 62 Batch: 0 Log-loss: 0.03532150387763977\n",
      "Epoch: 62 Batch: 40 Log-loss: 0.03155439347028732\n",
      "Epoch: 62 Batch: 80 Log-loss: 0.023935606703162193\n",
      "Epoch: 62 Batch: 120 Log-loss: 0.03441538289189339\n",
      "Epoch: 62 Batch: 160 Log-loss: 0.028825359418988228\n",
      "Epoch: 62 Batch: 200 Log-loss: 0.05521942302584648\n",
      "Epoch: 62 Batch: 240 Log-loss: 0.0423431396484375\n",
      "Epoch: 62 Batch: 280 Log-loss: 0.03060491569340229\n",
      "Epoch: 62 Batch: 320 Log-loss: 0.03560436889529228\n",
      "Epoch: 62 Batch: 360 Log-loss: 0.0345742367208004\n",
      "Epoch: 62 Batch: 400 Log-loss: 0.04113135114312172\n",
      "Epoch: 62 Batch: 440 Log-loss: 0.029061993584036827\n",
      "Epoch: 62 Batch: 480 Log-loss: 0.041080277413129807\n",
      "Epoch: 62 Batch: 520 Log-loss: 0.024565866217017174\n",
      "Epoch: 62 Batch: 560 Log-loss: 0.038486048579216\n",
      "Epoch average log-loss: 0.03861244209443352\n",
      "In Epoch: 62, val_loss: 0.039829984665905455, best_val_loss: 0.03946678895966707, best_auc: 0.9878831375442512\n",
      "Epoch: 63 Batch: 0 Log-loss: 0.03411906957626343\n",
      "Epoch: 63 Batch: 40 Log-loss: 0.03784234821796417\n",
      "Epoch: 63 Batch: 80 Log-loss: 0.029690831899642944\n",
      "Epoch: 63 Batch: 120 Log-loss: 0.046577464789152145\n",
      "Epoch: 63 Batch: 160 Log-loss: 0.039475101977586746\n",
      "Epoch: 63 Batch: 200 Log-loss: 0.04763798788189888\n",
      "Epoch: 63 Batch: 240 Log-loss: 0.0285060852766037\n",
      "Epoch: 63 Batch: 280 Log-loss: 0.03672445937991142\n",
      "Epoch: 63 Batch: 320 Log-loss: 0.03049975074827671\n",
      "Epoch: 63 Batch: 360 Log-loss: 0.04170936718583107\n",
      "Epoch: 63 Batch: 400 Log-loss: 0.0318661667406559\n",
      "Epoch: 63 Batch: 440 Log-loss: 0.04378591105341911\n",
      "Epoch: 63 Batch: 480 Log-loss: 0.039165738970041275\n",
      "Epoch: 63 Batch: 520 Log-loss: 0.04344769939780235\n",
      "Epoch: 63 Batch: 560 Log-loss: 0.04300270602107048\n",
      "Epoch average log-loss: 0.03859852402737098\n",
      "In Epoch: 63, val_loss: 0.0397748074897827, best_val_loss: 0.03946678895966707, best_auc: 0.9878831375442512\n",
      "Epoch: 64 Batch: 0 Log-loss: 0.0439983494579792\n",
      "Epoch: 64 Batch: 40 Log-loss: 0.02648060955107212\n",
      "Epoch: 64 Batch: 80 Log-loss: 0.04834906384348869\n",
      "Epoch: 64 Batch: 120 Log-loss: 0.026135796681046486\n",
      "Epoch: 64 Batch: 160 Log-loss: 0.026287421584129333\n",
      "Epoch: 64 Batch: 200 Log-loss: 0.039849333465099335\n",
      "Epoch: 64 Batch: 240 Log-loss: 0.05194510892033577\n",
      "Epoch: 64 Batch: 280 Log-loss: 0.038797471672296524\n",
      "Epoch: 64 Batch: 320 Log-loss: 0.02294846810400486\n",
      "Epoch: 64 Batch: 360 Log-loss: 0.046944085508584976\n",
      "Epoch: 64 Batch: 400 Log-loss: 0.03415308892726898\n",
      "Epoch: 64 Batch: 440 Log-loss: 0.05544791743159294\n",
      "Epoch: 64 Batch: 480 Log-loss: 0.04664472118020058\n",
      "Epoch: 64 Batch: 520 Log-loss: 0.03224765136837959\n",
      "Epoch: 64 Batch: 560 Log-loss: 0.0346098430454731\n",
      "Epoch average log-loss: 0.03853964440391532\n",
      "In Epoch: 64, val_loss: 0.0400128621569249, best_val_loss: 0.03946678895966707, best_auc: 0.9878831375442512\n",
      "Epoch: 65 Batch: 0 Log-loss: 0.041612572968006134\n",
      "Epoch: 65 Batch: 40 Log-loss: 0.026908479630947113\n",
      "Epoch: 65 Batch: 80 Log-loss: 0.03832561895251274\n",
      "Epoch: 65 Batch: 120 Log-loss: 0.026429729536175728\n",
      "Epoch: 65 Batch: 160 Log-loss: 0.033759091049432755\n",
      "Epoch: 65 Batch: 200 Log-loss: 0.037589289247989655\n",
      "Epoch: 65 Batch: 240 Log-loss: 0.04659466817975044\n",
      "Epoch: 65 Batch: 280 Log-loss: 0.042925525456666946\n",
      "Epoch: 65 Batch: 320 Log-loss: 0.04428606852889061\n",
      "Epoch: 65 Batch: 360 Log-loss: 0.03022625483572483\n",
      "Epoch: 65 Batch: 400 Log-loss: 0.03976065292954445\n",
      "Epoch: 65 Batch: 440 Log-loss: 0.04027001932263374\n",
      "Epoch: 65 Batch: 480 Log-loss: 0.03582329675555229\n",
      "Epoch: 65 Batch: 520 Log-loss: 0.03651781007647514\n",
      "Epoch: 65 Batch: 560 Log-loss: 0.037181220948696136\n",
      "Epoch average log-loss: 0.03869215624872595\n",
      "In Epoch: 65, val_loss: 0.04029919209149028, best_val_loss: 0.03946678895966707, best_auc: 0.9878831375442512\n",
      "Epoch: 66 Batch: 0 Log-loss: 0.030259964987635612\n",
      "Epoch: 66 Batch: 40 Log-loss: 0.03827010467648506\n",
      "Epoch: 66 Batch: 80 Log-loss: 0.05830684304237366\n",
      "Epoch: 66 Batch: 120 Log-loss: 0.03520403429865837\n",
      "Epoch: 66 Batch: 160 Log-loss: 0.03489822894334793\n",
      "Epoch: 66 Batch: 200 Log-loss: 0.028405355289578438\n",
      "Epoch: 66 Batch: 240 Log-loss: 0.038359228521585464\n",
      "Epoch: 66 Batch: 280 Log-loss: 0.04218580201268196\n",
      "Epoch: 66 Batch: 320 Log-loss: 0.03680620715022087\n",
      "Epoch: 66 Batch: 360 Log-loss: 0.03288445621728897\n",
      "Epoch: 66 Batch: 400 Log-loss: 0.0369848906993866\n",
      "Epoch: 66 Batch: 440 Log-loss: 0.04162454977631569\n",
      "Epoch: 66 Batch: 480 Log-loss: 0.053102586418390274\n",
      "Epoch: 66 Batch: 520 Log-loss: 0.032124657183885574\n",
      "Epoch: 66 Batch: 560 Log-loss: 0.03487042710185051\n",
      "Epoch average log-loss: 0.03856234961754775\n",
      "A pre-trained model at model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt has been loaded.\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn5.pt.\n",
      "\n",
      "Choose the torch base model.\n",
      "## Training on fold 6 ##\n",
      "Epoch: 1 Batch: 0 Log-loss: 0.7121606469154358\n",
      "Epoch: 1 Batch: 40 Log-loss: 0.18026800453662872\n",
      "Epoch: 1 Batch: 80 Log-loss: 0.15528522431850433\n",
      "Epoch: 1 Batch: 120 Log-loss: 0.08723738044500351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Batch: 160 Log-loss: 0.059100180864334106\n",
      "Epoch: 1 Batch: 200 Log-loss: 0.07215171307325363\n",
      "Epoch: 1 Batch: 240 Log-loss: 0.09960321336984634\n",
      "Epoch: 1 Batch: 280 Log-loss: 0.048518598079681396\n",
      "Epoch: 1 Batch: 320 Log-loss: 0.06745100021362305\n",
      "Epoch: 1 Batch: 360 Log-loss: 0.05052800476551056\n",
      "Epoch: 1 Batch: 400 Log-loss: 0.05163412168622017\n",
      "Epoch: 1 Batch: 440 Log-loss: 0.055034056305885315\n",
      "Epoch: 1 Batch: 480 Log-loss: 0.058810070157051086\n",
      "Epoch: 1 Batch: 520 Log-loss: 0.04238016530871391\n",
      "Epoch: 1 Batch: 560 Log-loss: 0.05133184418082237\n",
      "Epoch average log-loss: 0.0912608400519405\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 1, val_loss: 0.049592238115194476, best_val_loss: 0.049592238115194476, best_auc: 0.9707809426413018\n",
      "Epoch: 2 Batch: 0 Log-loss: 0.04388587549328804\n",
      "Epoch: 2 Batch: 40 Log-loss: 0.06613855808973312\n",
      "Epoch: 2 Batch: 80 Log-loss: 0.04671518877148628\n",
      "Epoch: 2 Batch: 120 Log-loss: 0.05018557235598564\n",
      "Epoch: 2 Batch: 160 Log-loss: 0.049249231815338135\n",
      "Epoch: 2 Batch: 200 Log-loss: 0.031003780663013458\n",
      "Epoch: 2 Batch: 240 Log-loss: 0.05477552115917206\n",
      "Epoch: 2 Batch: 280 Log-loss: 0.04946427047252655\n",
      "Epoch: 2 Batch: 320 Log-loss: 0.06179485842585564\n",
      "Epoch: 2 Batch: 360 Log-loss: 0.04291073977947235\n",
      "Epoch: 2 Batch: 400 Log-loss: 0.0608849972486496\n",
      "Epoch: 2 Batch: 440 Log-loss: 0.03354384005069733\n",
      "Epoch: 2 Batch: 480 Log-loss: 0.050346121191978455\n",
      "Epoch: 2 Batch: 520 Log-loss: 0.0633067637681961\n",
      "Epoch: 2 Batch: 560 Log-loss: 0.050423961132764816\n",
      "Epoch average log-loss: 0.05116460439749062\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 2, val_loss: 0.047358897106021695, best_val_loss: 0.047358897106021695, best_auc: 0.9764613036979242\n",
      "Epoch: 3 Batch: 0 Log-loss: 0.05243344232439995\n",
      "Epoch: 3 Batch: 40 Log-loss: 0.0336674302816391\n",
      "Epoch: 3 Batch: 80 Log-loss: 0.056297753006219864\n",
      "Epoch: 3 Batch: 120 Log-loss: 0.04701414704322815\n",
      "Epoch: 3 Batch: 160 Log-loss: 0.03956310451030731\n",
      "Epoch: 3 Batch: 200 Log-loss: 0.03874471038579941\n",
      "Epoch: 3 Batch: 240 Log-loss: 0.05444670096039772\n",
      "Epoch: 3 Batch: 280 Log-loss: 0.05690691992640495\n",
      "Epoch: 3 Batch: 320 Log-loss: 0.03953291103243828\n",
      "Epoch: 3 Batch: 360 Log-loss: 0.03901059553027153\n",
      "Epoch: 3 Batch: 400 Log-loss: 0.045007359236478806\n",
      "Epoch: 3 Batch: 440 Log-loss: 0.057971879839897156\n",
      "Epoch: 3 Batch: 480 Log-loss: 0.05080757662653923\n",
      "Epoch: 3 Batch: 520 Log-loss: 0.051168859004974365\n",
      "Epoch: 3 Batch: 560 Log-loss: 0.05603718012571335\n",
      "Epoch average log-loss: 0.048763598618097606\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 3, val_loss: 0.046251350346055216, best_val_loss: 0.046251350346055216, best_auc: 0.9800772083358399\n",
      "Epoch: 4 Batch: 0 Log-loss: 0.031056111678481102\n",
      "Epoch: 4 Batch: 40 Log-loss: 0.047509416937828064\n",
      "Epoch: 4 Batch: 80 Log-loss: 0.04299677535891533\n",
      "Epoch: 4 Batch: 120 Log-loss: 0.04285046085715294\n",
      "Epoch: 4 Batch: 160 Log-loss: 0.04700429365038872\n",
      "Epoch: 4 Batch: 200 Log-loss: 0.02516269125044346\n",
      "Epoch: 4 Batch: 240 Log-loss: 0.03789130970835686\n",
      "Epoch: 4 Batch: 280 Log-loss: 0.052817896008491516\n",
      "Epoch: 4 Batch: 320 Log-loss: 0.04643787816166878\n",
      "Epoch: 4 Batch: 360 Log-loss: 0.04702043905854225\n",
      "Epoch: 4 Batch: 400 Log-loss: 0.05167278274893761\n",
      "Epoch: 4 Batch: 440 Log-loss: 0.06988902390003204\n",
      "Epoch: 4 Batch: 480 Log-loss: 0.04358365014195442\n",
      "Epoch: 4 Batch: 520 Log-loss: 0.03923971578478813\n",
      "Epoch: 4 Batch: 560 Log-loss: 0.03801225125789642\n",
      "Epoch average log-loss: 0.04684294356099729\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 4, val_loss: 0.045446473433162875, best_val_loss: 0.045446473433162875, best_auc: 0.9822665668346815\n",
      "Epoch: 5 Batch: 0 Log-loss: 0.04502302035689354\n",
      "Epoch: 5 Batch: 40 Log-loss: 0.047144461423158646\n",
      "Epoch: 5 Batch: 80 Log-loss: 0.03851333260536194\n",
      "Epoch: 5 Batch: 120 Log-loss: 0.03546958044171333\n",
      "Epoch: 5 Batch: 160 Log-loss: 0.029556691646575928\n",
      "Epoch: 5 Batch: 200 Log-loss: 0.04241064563393593\n",
      "Epoch: 5 Batch: 240 Log-loss: 0.04988895729184151\n",
      "Epoch: 5 Batch: 280 Log-loss: 0.0545785129070282\n",
      "Epoch: 5 Batch: 320 Log-loss: 0.03164060041308403\n",
      "Epoch: 5 Batch: 360 Log-loss: 0.05547972023487091\n",
      "Epoch: 5 Batch: 400 Log-loss: 0.04218612611293793\n",
      "Epoch: 5 Batch: 440 Log-loss: 0.051939669996500015\n",
      "Epoch: 5 Batch: 480 Log-loss: 0.05579614266753197\n",
      "Epoch: 5 Batch: 520 Log-loss: 0.03953103348612785\n",
      "Epoch: 5 Batch: 560 Log-loss: 0.04881558567285538\n",
      "Epoch average log-loss: 0.04562166844095503\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 5, val_loss: 0.04291292535514704, best_val_loss: 0.04291292535514704, best_auc: 0.9849433596788586\n",
      "Epoch: 6 Batch: 0 Log-loss: 0.04497872665524483\n",
      "Epoch: 6 Batch: 40 Log-loss: 0.05225583538413048\n",
      "Epoch: 6 Batch: 80 Log-loss: 0.042843353003263474\n",
      "Epoch: 6 Batch: 120 Log-loss: 0.04719098284840584\n",
      "Epoch: 6 Batch: 160 Log-loss: 0.04970483481884003\n",
      "Epoch: 6 Batch: 200 Log-loss: 0.033669646829366684\n",
      "Epoch: 6 Batch: 240 Log-loss: 0.03697638586163521\n",
      "Epoch: 6 Batch: 280 Log-loss: 0.041893016546964645\n",
      "Epoch: 6 Batch: 320 Log-loss: 0.0444200336933136\n",
      "Epoch: 6 Batch: 360 Log-loss: 0.034672465175390244\n",
      "Epoch: 6 Batch: 400 Log-loss: 0.03948679566383362\n",
      "Epoch: 6 Batch: 440 Log-loss: 0.033850520849227905\n",
      "Epoch: 6 Batch: 480 Log-loss: 0.03752987086772919\n",
      "Epoch: 6 Batch: 520 Log-loss: 0.04334026202559471\n",
      "Epoch: 6 Batch: 560 Log-loss: 0.037571556866168976\n",
      "Epoch average log-loss: 0.044639060270440366\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 6, val_loss: 0.04224586826103549, best_val_loss: 0.04224586826103549, best_auc: 0.985486942256563\n",
      "Epoch: 7 Batch: 0 Log-loss: 0.03871987387537956\n",
      "Epoch: 7 Batch: 40 Log-loss: 0.03464040532708168\n",
      "Epoch: 7 Batch: 80 Log-loss: 0.04993423819541931\n",
      "Epoch: 7 Batch: 120 Log-loss: 0.06058970093727112\n",
      "Epoch: 7 Batch: 160 Log-loss: 0.03586696833372116\n",
      "Epoch: 7 Batch: 200 Log-loss: 0.041768718510866165\n",
      "Epoch: 7 Batch: 240 Log-loss: 0.0590839646756649\n",
      "Epoch: 7 Batch: 280 Log-loss: 0.045438215136528015\n",
      "Epoch: 7 Batch: 320 Log-loss: 0.03963550925254822\n",
      "Epoch: 7 Batch: 360 Log-loss: 0.036793846637010574\n",
      "Epoch: 7 Batch: 400 Log-loss: 0.044566068798303604\n",
      "Epoch: 7 Batch: 440 Log-loss: 0.06114116683602333\n",
      "Epoch: 7 Batch: 480 Log-loss: 0.051727768033742905\n",
      "Epoch: 7 Batch: 520 Log-loss: 0.03599408268928528\n",
      "Epoch: 7 Batch: 560 Log-loss: 0.03675178065896034\n",
      "Epoch average log-loss: 0.04370038745193077\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 7, val_loss: 0.04217527508127636, best_val_loss: 0.04217527508127636, best_auc: 0.9850482050240149\n",
      "Epoch: 8 Batch: 0 Log-loss: 0.03966158628463745\n",
      "Epoch: 8 Batch: 40 Log-loss: 0.04078223183751106\n",
      "Epoch: 8 Batch: 80 Log-loss: 0.051480863243341446\n",
      "Epoch: 8 Batch: 120 Log-loss: 0.05273756757378578\n",
      "Epoch: 8 Batch: 160 Log-loss: 0.028406577184796333\n",
      "Epoch: 8 Batch: 200 Log-loss: 0.05443969741463661\n",
      "Epoch: 8 Batch: 240 Log-loss: 0.03788404539227486\n",
      "Epoch: 8 Batch: 280 Log-loss: 0.02575300820171833\n",
      "Epoch: 8 Batch: 320 Log-loss: 0.038958996534347534\n",
      "Epoch: 8 Batch: 360 Log-loss: 0.034820616245269775\n",
      "Epoch: 8 Batch: 400 Log-loss: 0.039662014693021774\n",
      "Epoch: 8 Batch: 440 Log-loss: 0.04346024990081787\n",
      "Epoch: 8 Batch: 480 Log-loss: 0.052325502038002014\n",
      "Epoch: 8 Batch: 520 Log-loss: 0.03952896222472191\n",
      "Epoch: 8 Batch: 560 Log-loss: 0.044705577194690704\n",
      "Epoch average log-loss: 0.043350324616767466\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 8, val_loss: 0.04134581443254828, best_val_loss: 0.04134581443254828, best_auc: 0.9865092542048345\n",
      "Epoch: 9 Batch: 0 Log-loss: 0.046112459152936935\n",
      "Epoch: 9 Batch: 40 Log-loss: 0.044866662472486496\n",
      "Epoch: 9 Batch: 80 Log-loss: 0.04488372430205345\n",
      "Epoch: 9 Batch: 120 Log-loss: 0.039173424243927\n",
      "Epoch: 9 Batch: 160 Log-loss: 0.05597471073269844\n",
      "Epoch: 9 Batch: 200 Log-loss: 0.03847787156701088\n",
      "Epoch: 9 Batch: 240 Log-loss: 0.05224546417593956\n",
      "Epoch: 9 Batch: 280 Log-loss: 0.03408956900238991\n",
      "Epoch: 9 Batch: 320 Log-loss: 0.037707310169935226\n",
      "Epoch: 9 Batch: 360 Log-loss: 0.03246466815471649\n",
      "Epoch: 9 Batch: 400 Log-loss: 0.04862343892455101\n",
      "Epoch: 9 Batch: 440 Log-loss: 0.03723239526152611\n",
      "Epoch: 9 Batch: 480 Log-loss: 0.03464726731181145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Batch: 520 Log-loss: 0.05491280555725098\n",
      "Epoch: 9 Batch: 560 Log-loss: 0.03436688706278801\n",
      "Epoch average log-loss: 0.04281850759206074\n",
      "In Epoch: 9, val_loss: 0.04158534748820714, best_val_loss: 0.04134581443254828, best_auc: 0.9865092542048345\n",
      "Epoch: 10 Batch: 0 Log-loss: 0.04637132212519646\n",
      "Epoch: 10 Batch: 40 Log-loss: 0.049199048429727554\n",
      "Epoch: 10 Batch: 80 Log-loss: 0.03722526878118515\n",
      "Epoch: 10 Batch: 120 Log-loss: 0.037554871290922165\n",
      "Epoch: 10 Batch: 160 Log-loss: 0.037786856293678284\n",
      "Epoch: 10 Batch: 200 Log-loss: 0.05026208981871605\n",
      "Epoch: 10 Batch: 240 Log-loss: 0.04039931297302246\n",
      "Epoch: 10 Batch: 280 Log-loss: 0.03960171714425087\n",
      "Epoch: 10 Batch: 320 Log-loss: 0.04141657426953316\n",
      "Epoch: 10 Batch: 360 Log-loss: 0.03949221223592758\n",
      "Epoch: 10 Batch: 400 Log-loss: 0.037827763706445694\n",
      "Epoch: 10 Batch: 440 Log-loss: 0.037705231457948685\n",
      "Epoch: 10 Batch: 480 Log-loss: 0.0501534640789032\n",
      "Epoch: 10 Batch: 520 Log-loss: 0.04624855890870094\n",
      "Epoch: 10 Batch: 560 Log-loss: 0.031039290130138397\n",
      "Epoch average log-loss: 0.042494290914120415\n",
      "In Epoch: 10, val_loss: 0.04156817319548931, best_val_loss: 0.04134581443254828, best_auc: 0.9865092542048345\n",
      "Epoch: 11 Batch: 0 Log-loss: 0.04063147306442261\n",
      "Epoch: 11 Batch: 40 Log-loss: 0.04755454137921333\n",
      "Epoch: 11 Batch: 80 Log-loss: 0.03401757776737213\n",
      "Epoch: 11 Batch: 120 Log-loss: 0.041243117302656174\n",
      "Epoch: 11 Batch: 160 Log-loss: 0.04630199074745178\n",
      "Epoch: 11 Batch: 200 Log-loss: 0.03651842474937439\n",
      "Epoch: 11 Batch: 240 Log-loss: 0.05101577565073967\n",
      "Epoch: 11 Batch: 280 Log-loss: 0.053804293274879456\n",
      "Epoch: 11 Batch: 320 Log-loss: 0.043971359729766846\n",
      "Epoch: 11 Batch: 360 Log-loss: 0.0359342135488987\n",
      "Epoch: 11 Batch: 400 Log-loss: 0.048568274825811386\n",
      "Epoch: 11 Batch: 440 Log-loss: 0.03521205857396126\n",
      "Epoch: 11 Batch: 480 Log-loss: 0.043020959943532944\n",
      "Epoch: 11 Batch: 520 Log-loss: 0.03880276903510094\n",
      "Epoch: 11 Batch: 560 Log-loss: 0.03188559412956238\n",
      "Epoch average log-loss: 0.0421120473915445\n",
      "In Epoch: 11, val_loss: 0.04143461527823331, best_val_loss: 0.04134581443254828, best_auc: 0.9865092542048345\n",
      "Epoch: 12 Batch: 0 Log-loss: 0.03779367730021477\n",
      "Epoch: 12 Batch: 40 Log-loss: 0.032788198441267014\n",
      "Epoch: 12 Batch: 80 Log-loss: 0.05896284803748131\n",
      "Epoch: 12 Batch: 120 Log-loss: 0.029209909960627556\n",
      "Epoch: 12 Batch: 160 Log-loss: 0.04105237498879433\n",
      "Epoch: 12 Batch: 200 Log-loss: 0.039274778217077255\n",
      "Epoch: 12 Batch: 240 Log-loss: 0.040173932909965515\n",
      "Epoch: 12 Batch: 280 Log-loss: 0.05172795057296753\n",
      "Epoch: 12 Batch: 320 Log-loss: 0.03136048465967178\n",
      "Epoch: 12 Batch: 360 Log-loss: 0.03999161720275879\n",
      "Epoch: 12 Batch: 400 Log-loss: 0.05199648439884186\n",
      "Epoch: 12 Batch: 440 Log-loss: 0.038065675646066666\n",
      "Epoch: 12 Batch: 480 Log-loss: 0.030731482431292534\n",
      "Epoch: 12 Batch: 520 Log-loss: 0.0598248653113842\n",
      "Epoch: 12 Batch: 560 Log-loss: 0.049917615950107574\n",
      "Epoch average log-loss: 0.04193010091382478\n",
      "In Epoch: 12, val_loss: 0.041720743787492603, best_val_loss: 0.04134581443254828, best_auc: 0.9865092542048345\n",
      "Epoch: 13 Batch: 0 Log-loss: 0.03201674669981003\n",
      "Epoch: 13 Batch: 40 Log-loss: 0.0557401217520237\n",
      "Epoch: 13 Batch: 80 Log-loss: 0.04861263558268547\n",
      "Epoch: 13 Batch: 120 Log-loss: 0.0376569963991642\n",
      "Epoch: 13 Batch: 160 Log-loss: 0.053624700754880905\n",
      "Epoch: 13 Batch: 200 Log-loss: 0.03992500528693199\n",
      "Epoch: 13 Batch: 240 Log-loss: 0.04920916631817818\n",
      "Epoch: 13 Batch: 280 Log-loss: 0.03363245353102684\n",
      "Epoch: 13 Batch: 320 Log-loss: 0.042776476591825485\n",
      "Epoch: 13 Batch: 360 Log-loss: 0.03851449862122536\n",
      "Epoch: 13 Batch: 400 Log-loss: 0.0470082126557827\n",
      "Epoch: 13 Batch: 440 Log-loss: 0.04299648478627205\n",
      "Epoch: 13 Batch: 480 Log-loss: 0.04214029386639595\n",
      "Epoch: 13 Batch: 520 Log-loss: 0.029635226354002953\n",
      "Epoch: 13 Batch: 560 Log-loss: 0.033662814646959305\n",
      "Epoch average log-loss: 0.04155791972298175\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 13, val_loss: 0.04114494715692101, best_val_loss: 0.04114494715692101, best_auc: 0.9872739000090228\n",
      "Epoch: 14 Batch: 0 Log-loss: 0.03648659214377403\n",
      "Epoch: 14 Batch: 40 Log-loss: 0.03132309392094612\n",
      "Epoch: 14 Batch: 80 Log-loss: 0.04207485914230347\n",
      "Epoch: 14 Batch: 120 Log-loss: 0.05517822876572609\n",
      "Epoch: 14 Batch: 160 Log-loss: 0.04164782539010048\n",
      "Epoch: 14 Batch: 200 Log-loss: 0.032008569687604904\n",
      "Epoch: 14 Batch: 240 Log-loss: 0.03775881603360176\n",
      "Epoch: 14 Batch: 280 Log-loss: 0.023379730060696602\n",
      "Epoch: 14 Batch: 320 Log-loss: 0.04252609983086586\n",
      "Epoch: 14 Batch: 360 Log-loss: 0.03105405531823635\n",
      "Epoch: 14 Batch: 400 Log-loss: 0.051822636276483536\n",
      "Epoch: 14 Batch: 440 Log-loss: 0.04350172355771065\n",
      "Epoch: 14 Batch: 480 Log-loss: 0.027510805055499077\n",
      "Epoch: 14 Batch: 520 Log-loss: 0.05895141884684563\n",
      "Epoch: 14 Batch: 560 Log-loss: 0.04969584196805954\n",
      "Epoch average log-loss: 0.04130988186225295\n",
      "In Epoch: 14, val_loss: 0.04127376223163413, best_val_loss: 0.04114494715692101, best_auc: 0.9872739000090228\n",
      "Epoch: 15 Batch: 0 Log-loss: 0.05129111185669899\n",
      "Epoch: 15 Batch: 40 Log-loss: 0.028191542252898216\n",
      "Epoch: 15 Batch: 80 Log-loss: 0.04948362335562706\n",
      "Epoch: 15 Batch: 120 Log-loss: 0.055576350539922714\n",
      "Epoch: 15 Batch: 160 Log-loss: 0.05231054499745369\n",
      "Epoch: 15 Batch: 200 Log-loss: 0.03156132623553276\n",
      "Epoch: 15 Batch: 240 Log-loss: 0.04492989182472229\n",
      "Epoch: 15 Batch: 280 Log-loss: 0.04570557177066803\n",
      "Epoch: 15 Batch: 320 Log-loss: 0.038395047187805176\n",
      "Epoch: 15 Batch: 360 Log-loss: 0.04036946967244148\n",
      "Epoch: 15 Batch: 400 Log-loss: 0.030498234555125237\n",
      "Epoch: 15 Batch: 440 Log-loss: 0.04041832312941551\n",
      "Epoch: 15 Batch: 480 Log-loss: 0.04297061264514923\n",
      "Epoch: 15 Batch: 520 Log-loss: 0.04072874039411545\n",
      "Epoch: 15 Batch: 560 Log-loss: 0.036047857254743576\n",
      "Epoch average log-loss: 0.04101962174421975\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 15, val_loss: 0.04078736299193592, best_val_loss: 0.04078736299193592, best_auc: 0.9879743564475195\n",
      "Epoch: 16 Batch: 0 Log-loss: 0.046601489186286926\n",
      "Epoch: 16 Batch: 40 Log-loss: 0.03834431618452072\n",
      "Epoch: 16 Batch: 80 Log-loss: 0.04136301204562187\n",
      "Epoch: 16 Batch: 120 Log-loss: 0.04832713305950165\n",
      "Epoch: 16 Batch: 160 Log-loss: 0.03302377834916115\n",
      "Epoch: 16 Batch: 200 Log-loss: 0.04709887504577637\n",
      "Epoch: 16 Batch: 240 Log-loss: 0.03022727183997631\n",
      "Epoch: 16 Batch: 280 Log-loss: 0.02211795188486576\n",
      "Epoch: 16 Batch: 320 Log-loss: 0.036119814962148666\n",
      "Epoch: 16 Batch: 360 Log-loss: 0.048627108335494995\n",
      "Epoch: 16 Batch: 400 Log-loss: 0.024890026077628136\n",
      "Epoch: 16 Batch: 440 Log-loss: 0.04241636022925377\n",
      "Epoch: 16 Batch: 480 Log-loss: 0.02725101448595524\n",
      "Epoch: 16 Batch: 520 Log-loss: 0.03580235317349434\n",
      "Epoch: 16 Batch: 560 Log-loss: 0.04636703059077263\n",
      "Epoch average log-loss: 0.04082017192683582\n",
      "In Epoch: 16, val_loss: 0.040889881133672694, best_val_loss: 0.04078736299193592, best_auc: 0.9879743564475195\n",
      "Epoch: 17 Batch: 0 Log-loss: 0.03911760076880455\n",
      "Epoch: 17 Batch: 40 Log-loss: 0.04995697736740112\n",
      "Epoch: 17 Batch: 80 Log-loss: 0.04005400836467743\n",
      "Epoch: 17 Batch: 120 Log-loss: 0.047459110617637634\n",
      "Epoch: 17 Batch: 160 Log-loss: 0.046265482902526855\n",
      "Epoch: 17 Batch: 200 Log-loss: 0.04311353340744972\n",
      "Epoch: 17 Batch: 240 Log-loss: 0.0407027043402195\n",
      "Epoch: 17 Batch: 280 Log-loss: 0.031232155859470367\n",
      "Epoch: 17 Batch: 320 Log-loss: 0.03479525446891785\n",
      "Epoch: 17 Batch: 360 Log-loss: 0.03881010040640831\n",
      "Epoch: 17 Batch: 400 Log-loss: 0.04793938994407654\n",
      "Epoch: 17 Batch: 440 Log-loss: 0.036076053977012634\n",
      "Epoch: 17 Batch: 480 Log-loss: 0.04486880823969841\n",
      "Epoch: 17 Batch: 520 Log-loss: 0.03421743959188461\n",
      "Epoch: 17 Batch: 560 Log-loss: 0.049349647015333176\n",
      "Epoch average log-loss: 0.040645898126864005\n",
      "In Epoch: 17, val_loss: 0.0411007980170825, best_val_loss: 0.04078736299193592, best_auc: 0.9879743564475195\n",
      "Epoch: 18 Batch: 0 Log-loss: 0.03312009200453758\n",
      "Epoch: 18 Batch: 40 Log-loss: 0.04044589027762413\n",
      "Epoch: 18 Batch: 80 Log-loss: 0.02465432696044445\n",
      "Epoch: 18 Batch: 120 Log-loss: 0.03134135529398918\n",
      "Epoch: 18 Batch: 160 Log-loss: 0.0301874577999115\n",
      "Epoch: 18 Batch: 200 Log-loss: 0.030956732109189034\n",
      "Epoch: 18 Batch: 240 Log-loss: 0.03487526997923851\n",
      "Epoch: 18 Batch: 280 Log-loss: 0.036115214228630066\n",
      "Epoch: 18 Batch: 320 Log-loss: 0.03635160252451897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 Batch: 360 Log-loss: 0.030525239184498787\n",
      "Epoch: 18 Batch: 400 Log-loss: 0.036135587841272354\n",
      "Epoch: 18 Batch: 440 Log-loss: 0.030867604538798332\n",
      "Epoch: 18 Batch: 480 Log-loss: 0.03671945631504059\n",
      "Epoch: 18 Batch: 520 Log-loss: 0.05561250075697899\n",
      "Epoch: 18 Batch: 560 Log-loss: 0.02169940061867237\n",
      "Epoch average log-loss: 0.04029120976837086\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 18, val_loss: 0.04055607176224434, best_val_loss: 0.04055607176224434, best_auc: 0.9883170474248234\n",
      "Epoch: 19 Batch: 0 Log-loss: 0.046518176794052124\n",
      "Epoch: 19 Batch: 40 Log-loss: 0.054171815514564514\n",
      "Epoch: 19 Batch: 80 Log-loss: 0.03534221649169922\n",
      "Epoch: 19 Batch: 120 Log-loss: 0.03687874600291252\n",
      "Epoch: 19 Batch: 160 Log-loss: 0.04797367751598358\n",
      "Epoch: 19 Batch: 200 Log-loss: 0.041512154042720795\n",
      "Epoch: 19 Batch: 240 Log-loss: 0.04101502150297165\n",
      "Epoch: 19 Batch: 280 Log-loss: 0.019217751920223236\n",
      "Epoch: 19 Batch: 320 Log-loss: 0.038907039910554886\n",
      "Epoch: 19 Batch: 360 Log-loss: 0.062054336071014404\n",
      "Epoch: 19 Batch: 400 Log-loss: 0.04806090518832207\n",
      "Epoch: 19 Batch: 440 Log-loss: 0.03463464602828026\n",
      "Epoch: 19 Batch: 480 Log-loss: 0.036210332065820694\n",
      "Epoch: 19 Batch: 520 Log-loss: 0.032250307500362396\n",
      "Epoch: 19 Batch: 560 Log-loss: 0.04712608456611633\n",
      "Epoch average log-loss: 0.04034645554077412\n",
      "In Epoch: 19, val_loss: 0.04076337348476353, best_val_loss: 0.04055607176224434, best_auc: 0.9883170474248234\n",
      "Epoch: 20 Batch: 0 Log-loss: 0.040363844484090805\n",
      "Epoch: 20 Batch: 40 Log-loss: 0.031920190900564194\n",
      "Epoch: 20 Batch: 80 Log-loss: 0.039914052933454514\n",
      "Epoch: 20 Batch: 120 Log-loss: 0.05055209621787071\n",
      "Epoch: 20 Batch: 160 Log-loss: 0.048312168568372726\n",
      "Epoch: 20 Batch: 200 Log-loss: 0.047422558069229126\n",
      "Epoch: 20 Batch: 240 Log-loss: 0.04741666838526726\n",
      "Epoch: 20 Batch: 280 Log-loss: 0.02754608541727066\n",
      "Epoch: 20 Batch: 320 Log-loss: 0.038695625960826874\n",
      "Epoch: 20 Batch: 360 Log-loss: 0.039209384471178055\n",
      "Epoch: 20 Batch: 400 Log-loss: 0.030640235170722008\n",
      "Epoch: 20 Batch: 440 Log-loss: 0.03881169855594635\n",
      "Epoch: 20 Batch: 480 Log-loss: 0.04190593957901001\n",
      "Epoch: 20 Batch: 520 Log-loss: 0.026822363957762718\n",
      "Epoch: 20 Batch: 560 Log-loss: 0.046086639165878296\n",
      "Epoch average log-loss: 0.04013645082346297\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 20, val_loss: 0.04035751331953132, best_val_loss: 0.04035751331953132, best_auc: 0.9883642525948328\n",
      "Epoch: 21 Batch: 0 Log-loss: 0.043720792979002\n",
      "Epoch: 21 Batch: 40 Log-loss: 0.04507623240351677\n",
      "Epoch: 21 Batch: 80 Log-loss: 0.054543327540159225\n",
      "Epoch: 21 Batch: 120 Log-loss: 0.04220554232597351\n",
      "Epoch: 21 Batch: 160 Log-loss: 0.03376616910099983\n",
      "Epoch: 21 Batch: 200 Log-loss: 0.04564942046999931\n",
      "Epoch: 21 Batch: 240 Log-loss: 0.028987998142838478\n",
      "Epoch: 21 Batch: 280 Log-loss: 0.04782094433903694\n",
      "Epoch: 21 Batch: 320 Log-loss: 0.025658318772912025\n",
      "Epoch: 21 Batch: 360 Log-loss: 0.02270103245973587\n",
      "Epoch: 21 Batch: 400 Log-loss: 0.022824501618742943\n",
      "Epoch: 21 Batch: 440 Log-loss: 0.048583243042230606\n",
      "Epoch: 21 Batch: 480 Log-loss: 0.03701857849955559\n",
      "Epoch: 21 Batch: 520 Log-loss: 0.042965665459632874\n",
      "Epoch: 21 Batch: 560 Log-loss: 0.04132455214858055\n",
      "Epoch average log-loss: 0.04021004440728575\n",
      "In Epoch: 21, val_loss: 0.04036540798127111, best_val_loss: 0.04035751331953132, best_auc: 0.9883642525948328\n",
      "Epoch: 22 Batch: 0 Log-loss: 0.040912289172410965\n",
      "Epoch: 22 Batch: 40 Log-loss: 0.026940977200865746\n",
      "Epoch: 22 Batch: 80 Log-loss: 0.0401010699570179\n",
      "Epoch: 22 Batch: 120 Log-loss: 0.03679382801055908\n",
      "Epoch: 22 Batch: 160 Log-loss: 0.05406556650996208\n",
      "Epoch: 22 Batch: 200 Log-loss: 0.04908675327897072\n",
      "Epoch: 22 Batch: 240 Log-loss: 0.039790257811546326\n",
      "Epoch: 22 Batch: 280 Log-loss: 0.041605833917856216\n",
      "Epoch: 22 Batch: 320 Log-loss: 0.04030371084809303\n",
      "Epoch: 22 Batch: 360 Log-loss: 0.0395917110145092\n",
      "Epoch: 22 Batch: 400 Log-loss: 0.046004801988601685\n",
      "Epoch: 22 Batch: 440 Log-loss: 0.03617832437157631\n",
      "Epoch: 22 Batch: 480 Log-loss: 0.03974733129143715\n",
      "Epoch: 22 Batch: 520 Log-loss: 0.04425208270549774\n",
      "Epoch: 22 Batch: 560 Log-loss: 0.034060537815093994\n",
      "Epoch average log-loss: 0.03998999531447355\n",
      "In Epoch: 22, val_loss: 0.04112398400869326, best_val_loss: 0.04035751331953132, best_auc: 0.9883642525948328\n",
      "Epoch: 23 Batch: 0 Log-loss: 0.032460737973451614\n",
      "Epoch: 23 Batch: 40 Log-loss: 0.035745251923799515\n",
      "Epoch: 23 Batch: 80 Log-loss: 0.0462515689432621\n",
      "Epoch: 23 Batch: 120 Log-loss: 0.05164920166134834\n",
      "Epoch: 23 Batch: 160 Log-loss: 0.030374005436897278\n",
      "Epoch: 23 Batch: 200 Log-loss: 0.043293412774801254\n",
      "Epoch: 23 Batch: 240 Log-loss: 0.04175460338592529\n",
      "Epoch: 23 Batch: 280 Log-loss: 0.030674120411276817\n",
      "Epoch: 23 Batch: 320 Log-loss: 0.03606143966317177\n",
      "Epoch: 23 Batch: 360 Log-loss: 0.0316060446202755\n",
      "Epoch: 23 Batch: 400 Log-loss: 0.041419390588998795\n",
      "Epoch: 23 Batch: 440 Log-loss: 0.05727972462773323\n",
      "Epoch: 23 Batch: 480 Log-loss: 0.04079221189022064\n",
      "Epoch: 23 Batch: 520 Log-loss: 0.04155690595507622\n",
      "Epoch: 23 Batch: 560 Log-loss: 0.023261237889528275\n",
      "Epoch average log-loss: 0.03992133450561336\n",
      "In Epoch: 23, val_loss: 0.04041795886478198, best_val_loss: 0.04035751331953132, best_auc: 0.9883642525948328\n",
      "Epoch: 24 Batch: 0 Log-loss: 0.033606406301259995\n",
      "Epoch: 24 Batch: 40 Log-loss: 0.050423458218574524\n",
      "Epoch: 24 Batch: 80 Log-loss: 0.03714090213179588\n",
      "Epoch: 24 Batch: 120 Log-loss: 0.029418358579277992\n",
      "Epoch: 24 Batch: 160 Log-loss: 0.04592816159129143\n",
      "Epoch: 24 Batch: 200 Log-loss: 0.035004470497369766\n",
      "Epoch: 24 Batch: 240 Log-loss: 0.03461683914065361\n",
      "Epoch: 24 Batch: 280 Log-loss: 0.03140077739953995\n",
      "Epoch: 24 Batch: 320 Log-loss: 0.03857128694653511\n",
      "Epoch: 24 Batch: 360 Log-loss: 0.02845270186662674\n",
      "Epoch: 24 Batch: 400 Log-loss: 0.039802681654691696\n",
      "Epoch: 24 Batch: 440 Log-loss: 0.05855482816696167\n",
      "Epoch: 24 Batch: 480 Log-loss: 0.03688938543200493\n",
      "Epoch: 24 Batch: 520 Log-loss: 0.05143529549241066\n",
      "Epoch: 24 Batch: 560 Log-loss: 0.05706198886036873\n",
      "Epoch average log-loss: 0.039598446327727285\n",
      "In Epoch: 24, val_loss: 0.040923424966900664, best_val_loss: 0.04035751331953132, best_auc: 0.9883642525948328\n",
      "Epoch: 25 Batch: 0 Log-loss: 0.03832056373357773\n",
      "Epoch: 25 Batch: 40 Log-loss: 0.040087271481752396\n",
      "Epoch: 25 Batch: 80 Log-loss: 0.05062044784426689\n",
      "Epoch: 25 Batch: 120 Log-loss: 0.03488518297672272\n",
      "Epoch: 25 Batch: 160 Log-loss: 0.049963757395744324\n",
      "Epoch: 25 Batch: 200 Log-loss: 0.0595732145011425\n",
      "Epoch: 25 Batch: 240 Log-loss: 0.030607150867581367\n",
      "Epoch: 25 Batch: 280 Log-loss: 0.05502902343869209\n",
      "Epoch: 25 Batch: 320 Log-loss: 0.027331620454788208\n",
      "Epoch: 25 Batch: 360 Log-loss: 0.037476547062397\n",
      "Epoch: 25 Batch: 400 Log-loss: 0.036718130111694336\n",
      "Epoch: 25 Batch: 440 Log-loss: 0.038998741656541824\n",
      "Epoch: 25 Batch: 480 Log-loss: 0.05001574754714966\n",
      "Epoch: 25 Batch: 520 Log-loss: 0.03944137692451477\n",
      "Epoch: 25 Batch: 560 Log-loss: 0.05973491072654724\n",
      "Epoch average log-loss: 0.03969256784766913\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 25, val_loss: 0.04026700785373786, best_val_loss: 0.04026700785373786, best_auc: 0.9885021811710502\n",
      "Epoch: 26 Batch: 0 Log-loss: 0.03449253365397453\n",
      "Epoch: 26 Batch: 40 Log-loss: 0.04529504105448723\n",
      "Epoch: 26 Batch: 80 Log-loss: 0.04036002978682518\n",
      "Epoch: 26 Batch: 120 Log-loss: 0.03274773433804512\n",
      "Epoch: 26 Batch: 160 Log-loss: 0.04714367166161537\n",
      "Epoch: 26 Batch: 200 Log-loss: 0.04069351777434349\n",
      "Epoch: 26 Batch: 240 Log-loss: 0.047072600573301315\n",
      "Epoch: 26 Batch: 280 Log-loss: 0.04795709624886513\n",
      "Epoch: 26 Batch: 320 Log-loss: 0.046329960227012634\n",
      "Epoch: 26 Batch: 360 Log-loss: 0.03817202150821686\n",
      "Epoch: 26 Batch: 400 Log-loss: 0.03642800450325012\n",
      "Epoch: 26 Batch: 440 Log-loss: 0.036405038088560104\n",
      "Epoch: 26 Batch: 480 Log-loss: 0.03824763372540474\n",
      "Epoch: 26 Batch: 520 Log-loss: 0.04136820510029793\n",
      "Epoch: 26 Batch: 560 Log-loss: 0.041211530566215515\n",
      "Epoch average log-loss: 0.039552012654686615\n",
      "In Epoch: 26, val_loss: 0.040374758745096474, best_val_loss: 0.04026700785373786, best_auc: 0.9885021811710502\n",
      "Epoch: 27 Batch: 0 Log-loss: 0.028484053909778595\n",
      "Epoch: 27 Batch: 40 Log-loss: 0.03990909457206726\n",
      "Epoch: 27 Batch: 80 Log-loss: 0.037914108484983444\n",
      "Epoch: 27 Batch: 120 Log-loss: 0.04003669694066048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 Batch: 160 Log-loss: 0.04445083439350128\n",
      "Epoch: 27 Batch: 200 Log-loss: 0.04302610084414482\n",
      "Epoch: 27 Batch: 240 Log-loss: 0.04082886874675751\n",
      "Epoch: 27 Batch: 280 Log-loss: 0.04320888593792915\n",
      "Epoch: 27 Batch: 320 Log-loss: 0.050552982836961746\n",
      "Epoch: 27 Batch: 360 Log-loss: 0.04534360393881798\n",
      "Epoch: 27 Batch: 400 Log-loss: 0.040201324969530106\n",
      "Epoch: 27 Batch: 440 Log-loss: 0.04261687770485878\n",
      "Epoch: 27 Batch: 480 Log-loss: 0.05068185552954674\n",
      "Epoch: 27 Batch: 520 Log-loss: 0.027292611077427864\n",
      "Epoch: 27 Batch: 560 Log-loss: 0.03557008132338524\n",
      "Epoch average log-loss: 0.03939937364775688\n",
      "In Epoch: 27, val_loss: 0.04050555761631245, best_val_loss: 0.04026700785373786, best_auc: 0.9885021811710502\n",
      "Epoch: 28 Batch: 0 Log-loss: 0.037180572748184204\n",
      "Epoch: 28 Batch: 40 Log-loss: 0.04345827177166939\n",
      "Epoch: 28 Batch: 80 Log-loss: 0.031039198860526085\n",
      "Epoch: 28 Batch: 120 Log-loss: 0.04836682975292206\n",
      "Epoch: 28 Batch: 160 Log-loss: 0.04740059748291969\n",
      "Epoch: 28 Batch: 200 Log-loss: 0.03648345172405243\n",
      "Epoch: 28 Batch: 240 Log-loss: 0.058944400399923325\n",
      "Epoch: 28 Batch: 280 Log-loss: 0.040979571640491486\n",
      "Epoch: 28 Batch: 320 Log-loss: 0.04011547192931175\n",
      "Epoch: 28 Batch: 360 Log-loss: 0.04033888131380081\n",
      "Epoch: 28 Batch: 400 Log-loss: 0.0332675538957119\n",
      "Epoch: 28 Batch: 440 Log-loss: 0.040754664689302444\n",
      "Epoch: 28 Batch: 480 Log-loss: 0.02381390519440174\n",
      "Epoch: 28 Batch: 520 Log-loss: 0.026833482086658478\n",
      "Epoch: 28 Batch: 560 Log-loss: 0.0520901121199131\n",
      "Epoch average log-loss: 0.039448975355896566\n",
      "In Epoch: 28, val_loss: 0.04060855252594737, best_val_loss: 0.04026700785373786, best_auc: 0.9885021811710502\n",
      "Epoch: 29 Batch: 0 Log-loss: 0.03412593528628349\n",
      "Epoch: 29 Batch: 40 Log-loss: 0.028092078864574432\n",
      "Epoch: 29 Batch: 80 Log-loss: 0.04743009805679321\n",
      "Epoch: 29 Batch: 120 Log-loss: 0.0361216738820076\n",
      "Epoch: 29 Batch: 160 Log-loss: 0.04061935469508171\n",
      "Epoch: 29 Batch: 200 Log-loss: 0.040883999317884445\n",
      "Epoch: 29 Batch: 240 Log-loss: 0.04869208112359047\n",
      "Epoch: 29 Batch: 280 Log-loss: 0.044514577835798264\n",
      "Epoch: 29 Batch: 320 Log-loss: 0.03871214762330055\n",
      "Epoch: 29 Batch: 360 Log-loss: 0.042446959763765335\n",
      "Epoch: 29 Batch: 400 Log-loss: 0.03272249549627304\n",
      "Epoch: 29 Batch: 440 Log-loss: 0.03886270523071289\n",
      "Epoch: 29 Batch: 480 Log-loss: 0.036835603415966034\n",
      "Epoch: 29 Batch: 520 Log-loss: 0.025951435789465904\n",
      "Epoch: 29 Batch: 560 Log-loss: 0.04239238426089287\n",
      "Epoch average log-loss: 0.03925950446698282\n",
      "In Epoch: 29, val_loss: 0.040449604408979455, best_val_loss: 0.04026700785373786, best_auc: 0.9885021811710502\n",
      "Epoch: 30 Batch: 0 Log-loss: 0.043775979429483414\n",
      "Epoch: 30 Batch: 40 Log-loss: 0.031093647703528404\n",
      "Epoch: 30 Batch: 80 Log-loss: 0.043825987726449966\n",
      "Epoch: 30 Batch: 120 Log-loss: 0.04836592078208923\n",
      "Epoch: 30 Batch: 160 Log-loss: 0.03376040235161781\n",
      "Epoch: 30 Batch: 200 Log-loss: 0.03996475040912628\n",
      "Epoch: 30 Batch: 240 Log-loss: 0.04231448844075203\n",
      "Epoch: 30 Batch: 280 Log-loss: 0.04010867327451706\n",
      "Epoch: 30 Batch: 320 Log-loss: 0.028232822194695473\n",
      "Epoch: 30 Batch: 360 Log-loss: 0.037823352962732315\n",
      "Epoch: 30 Batch: 400 Log-loss: 0.029301529750227928\n",
      "Epoch: 30 Batch: 440 Log-loss: 0.05922618880867958\n",
      "Epoch: 30 Batch: 480 Log-loss: 0.027907216921448708\n",
      "Epoch: 30 Batch: 520 Log-loss: 0.047363221645355225\n",
      "Epoch: 30 Batch: 560 Log-loss: 0.04110061749815941\n",
      "Epoch average log-loss: 0.03929551170274083\n",
      "In Epoch: 30, val_loss: 0.04050514507124104, best_val_loss: 0.04026700785373786, best_auc: 0.9885021811710502\n",
      "Epoch: 31 Batch: 0 Log-loss: 0.04248074069619179\n",
      "Epoch: 31 Batch: 40 Log-loss: 0.040255703032016754\n",
      "Epoch: 31 Batch: 80 Log-loss: 0.02611609734594822\n",
      "Epoch: 31 Batch: 120 Log-loss: 0.04080693796277046\n",
      "Epoch: 31 Batch: 160 Log-loss: 0.05829557776451111\n",
      "Epoch: 31 Batch: 200 Log-loss: 0.030151179060339928\n",
      "Epoch: 31 Batch: 240 Log-loss: 0.03830892965197563\n",
      "Epoch: 31 Batch: 280 Log-loss: 0.04390906170010567\n",
      "Epoch: 31 Batch: 320 Log-loss: 0.04760587587952614\n",
      "Epoch: 31 Batch: 360 Log-loss: 0.03225645050406456\n",
      "Epoch: 31 Batch: 400 Log-loss: 0.03507254272699356\n",
      "Epoch: 31 Batch: 440 Log-loss: 0.03128621354699135\n",
      "Epoch: 31 Batch: 480 Log-loss: 0.04442162811756134\n",
      "Epoch: 31 Batch: 520 Log-loss: 0.03306660056114197\n",
      "Epoch: 31 Batch: 560 Log-loss: 0.03573613986372948\n",
      "Epoch average log-loss: 0.039235284139535254\n",
      "In Epoch: 31, val_loss: 0.04073118986120751, best_val_loss: 0.04026700785373786, best_auc: 0.9885021811710502\n",
      "Epoch: 32 Batch: 0 Log-loss: 0.034273453056812286\n",
      "Epoch: 32 Batch: 40 Log-loss: 0.02950688637793064\n",
      "Epoch: 32 Batch: 80 Log-loss: 0.032837726175785065\n",
      "Epoch: 32 Batch: 120 Log-loss: 0.042409107089042664\n",
      "Epoch: 32 Batch: 160 Log-loss: 0.04479781910777092\n",
      "Epoch: 32 Batch: 200 Log-loss: 0.030913615599274635\n",
      "Epoch: 32 Batch: 240 Log-loss: 0.04443536698818207\n",
      "Epoch: 32 Batch: 280 Log-loss: 0.07544959336519241\n",
      "Epoch: 32 Batch: 320 Log-loss: 0.04410639405250549\n",
      "Epoch: 32 Batch: 360 Log-loss: 0.04226287826895714\n",
      "Epoch: 32 Batch: 400 Log-loss: 0.0636272206902504\n",
      "Epoch: 32 Batch: 440 Log-loss: 0.039964865893125534\n",
      "Epoch: 32 Batch: 480 Log-loss: 0.0406019501388073\n",
      "Epoch: 32 Batch: 520 Log-loss: 0.04731890559196472\n",
      "Epoch: 32 Batch: 560 Log-loss: 0.04232200235128403\n",
      "Epoch average log-loss: 0.03910713328181633\n",
      "In Epoch: 32, val_loss: 0.040529097484202074, best_val_loss: 0.04026700785373786, best_auc: 0.9885021811710502\n",
      "Epoch: 33 Batch: 0 Log-loss: 0.03152799978852272\n",
      "Epoch: 33 Batch: 40 Log-loss: 0.03483470156788826\n",
      "Epoch: 33 Batch: 80 Log-loss: 0.03587036952376366\n",
      "Epoch: 33 Batch: 120 Log-loss: 0.035660240799188614\n",
      "Epoch: 33 Batch: 160 Log-loss: 0.03146064653992653\n",
      "Epoch: 33 Batch: 200 Log-loss: 0.04135243594646454\n",
      "Epoch: 33 Batch: 240 Log-loss: 0.03846314921975136\n",
      "Epoch: 33 Batch: 280 Log-loss: 0.04250580072402954\n",
      "Epoch: 33 Batch: 320 Log-loss: 0.04473618045449257\n",
      "Epoch: 33 Batch: 360 Log-loss: 0.0460796058177948\n",
      "Epoch: 33 Batch: 400 Log-loss: 0.03472726419568062\n",
      "Epoch: 33 Batch: 440 Log-loss: 0.030130909755825996\n",
      "Epoch: 33 Batch: 480 Log-loss: 0.04291287437081337\n",
      "Epoch: 33 Batch: 520 Log-loss: 0.046014830470085144\n",
      "Epoch: 33 Batch: 560 Log-loss: 0.061830516904592514\n",
      "Epoch average log-loss: 0.03889281845518521\n",
      "In Epoch: 33, val_loss: 0.04043838181264602, best_val_loss: 0.04026700785373786, best_auc: 0.9885021811710502\n",
      "Epoch: 34 Batch: 0 Log-loss: 0.039663299918174744\n",
      "Epoch: 34 Batch: 40 Log-loss: 0.038672663271427155\n",
      "Epoch: 34 Batch: 80 Log-loss: 0.0368448905646801\n",
      "Epoch: 34 Batch: 120 Log-loss: 0.04219329357147217\n",
      "Epoch: 34 Batch: 160 Log-loss: 0.03636462613940239\n",
      "Epoch: 34 Batch: 200 Log-loss: 0.03527676314115524\n",
      "Epoch: 34 Batch: 240 Log-loss: 0.03545031324028969\n",
      "Epoch: 34 Batch: 280 Log-loss: 0.04752497002482414\n",
      "Epoch: 34 Batch: 320 Log-loss: 0.03258948773145676\n",
      "Epoch: 34 Batch: 360 Log-loss: 0.03664512559771538\n",
      "Epoch: 34 Batch: 400 Log-loss: 0.03473454713821411\n",
      "Epoch: 34 Batch: 440 Log-loss: 0.039488811045885086\n",
      "Epoch: 34 Batch: 480 Log-loss: 0.05381244793534279\n",
      "Epoch: 34 Batch: 520 Log-loss: 0.030383015051484108\n",
      "Epoch: 34 Batch: 560 Log-loss: 0.04266425594687462\n",
      "Epoch average log-loss: 0.03915300761083407\n",
      "In Epoch: 34, val_loss: 0.04047903333161979, best_val_loss: 0.04026700785373786, best_auc: 0.9885021811710502\n",
      "Epoch: 35 Batch: 0 Log-loss: 0.049164071679115295\n",
      "Epoch: 35 Batch: 40 Log-loss: 0.03724614158272743\n",
      "Epoch: 35 Batch: 80 Log-loss: 0.028682323172688484\n",
      "Epoch: 35 Batch: 120 Log-loss: 0.033811312168836594\n",
      "Epoch: 35 Batch: 160 Log-loss: 0.04435218870639801\n",
      "Epoch: 35 Batch: 200 Log-loss: 0.054908160120248795\n",
      "Epoch: 35 Batch: 240 Log-loss: 0.04248685762286186\n",
      "Epoch: 35 Batch: 280 Log-loss: 0.03092719055712223\n",
      "Epoch: 35 Batch: 320 Log-loss: 0.034692540764808655\n",
      "Epoch: 35 Batch: 360 Log-loss: 0.03533479571342468\n",
      "Epoch: 35 Batch: 400 Log-loss: 0.03677351400256157\n",
      "Epoch: 35 Batch: 440 Log-loss: 0.03877189755439758\n",
      "Epoch: 35 Batch: 480 Log-loss: 0.04178032651543617\n",
      "Epoch: 35 Batch: 520 Log-loss: 0.03008621744811535\n",
      "Epoch: 35 Batch: 560 Log-loss: 0.03666982054710388\n",
      "Epoch average log-loss: 0.03904194341240717\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 35, val_loss: 0.040186706425053946, best_val_loss: 0.040186706425053946, best_auc: 0.9883710208829597\n",
      "Epoch: 36 Batch: 0 Log-loss: 0.0385710671544075\n",
      "Epoch: 36 Batch: 40 Log-loss: 0.044892389327287674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 Batch: 80 Log-loss: 0.05629301443696022\n",
      "Epoch: 36 Batch: 120 Log-loss: 0.03914165124297142\n",
      "Epoch: 36 Batch: 160 Log-loss: 0.04202214255928993\n",
      "Epoch: 36 Batch: 200 Log-loss: 0.03558553382754326\n",
      "Epoch: 36 Batch: 240 Log-loss: 0.031003592535853386\n",
      "Epoch: 36 Batch: 280 Log-loss: 0.03468476980924606\n",
      "Epoch: 36 Batch: 320 Log-loss: 0.03438184782862663\n",
      "Epoch: 36 Batch: 360 Log-loss: 0.03372533246874809\n",
      "Epoch: 36 Batch: 400 Log-loss: 0.030062591657042503\n",
      "Epoch: 36 Batch: 440 Log-loss: 0.03302799165248871\n",
      "Epoch: 36 Batch: 480 Log-loss: 0.03464243933558464\n",
      "Epoch: 36 Batch: 520 Log-loss: 0.031690459698438644\n",
      "Epoch: 36 Batch: 560 Log-loss: 0.037094637751579285\n",
      "Epoch average log-loss: 0.03904217526516212\n",
      "In Epoch: 36, val_loss: 0.04060404463356092, best_val_loss: 0.040186706425053946, best_auc: 0.9883710208829597\n",
      "Epoch: 37 Batch: 0 Log-loss: 0.03825089707970619\n",
      "Epoch: 37 Batch: 40 Log-loss: 0.04768577218055725\n",
      "Epoch: 37 Batch: 80 Log-loss: 0.03868989273905754\n",
      "Epoch: 37 Batch: 120 Log-loss: 0.04555198922753334\n",
      "Epoch: 37 Batch: 160 Log-loss: 0.04143926873803139\n",
      "Epoch: 37 Batch: 200 Log-loss: 0.06236406788229942\n",
      "Epoch: 37 Batch: 240 Log-loss: 0.04479501023888588\n",
      "Epoch: 37 Batch: 280 Log-loss: 0.0438087172806263\n",
      "Epoch: 37 Batch: 320 Log-loss: 0.036323193460702896\n",
      "Epoch: 37 Batch: 360 Log-loss: 0.030826784670352936\n",
      "Epoch: 37 Batch: 400 Log-loss: 0.029744789004325867\n",
      "Epoch: 37 Batch: 440 Log-loss: 0.029485128819942474\n",
      "Epoch: 37 Batch: 480 Log-loss: 0.027338949963450432\n",
      "Epoch: 37 Batch: 520 Log-loss: 0.03828427940607071\n",
      "Epoch: 37 Batch: 560 Log-loss: 0.025160206481814384\n",
      "Epoch average log-loss: 0.03892950292543641\n",
      "In Epoch: 37, val_loss: 0.04040324670053768, best_val_loss: 0.040186706425053946, best_auc: 0.9883710208829597\n",
      "Epoch: 38 Batch: 0 Log-loss: 0.03528125211596489\n",
      "Epoch: 38 Batch: 40 Log-loss: 0.04619893431663513\n",
      "Epoch: 38 Batch: 80 Log-loss: 0.05033816397190094\n",
      "Epoch: 38 Batch: 120 Log-loss: 0.033995043486356735\n",
      "Epoch: 38 Batch: 160 Log-loss: 0.04269171133637428\n",
      "Epoch: 38 Batch: 200 Log-loss: 0.07056953758001328\n",
      "Epoch: 38 Batch: 240 Log-loss: 0.04506989195942879\n",
      "Epoch: 38 Batch: 280 Log-loss: 0.04066227003931999\n",
      "Epoch: 38 Batch: 320 Log-loss: 0.03704117611050606\n",
      "Epoch: 38 Batch: 360 Log-loss: 0.05402145907282829\n",
      "Epoch: 38 Batch: 400 Log-loss: 0.04371904954314232\n",
      "Epoch: 38 Batch: 440 Log-loss: 0.03995794430375099\n",
      "Epoch: 38 Batch: 480 Log-loss: 0.043161142617464066\n",
      "Epoch: 38 Batch: 520 Log-loss: 0.03604259341955185\n",
      "Epoch: 38 Batch: 560 Log-loss: 0.03296636417508125\n",
      "Epoch average log-loss: 0.03886471903783136\n",
      "In Epoch: 38, val_loss: 0.04044603847851689, best_val_loss: 0.040186706425053946, best_auc: 0.9883710208829597\n",
      "Epoch: 39 Batch: 0 Log-loss: 0.04981042817234993\n",
      "Epoch: 39 Batch: 40 Log-loss: 0.03630971163511276\n",
      "Epoch: 39 Batch: 80 Log-loss: 0.028822166845202446\n",
      "Epoch: 39 Batch: 120 Log-loss: 0.0452030748128891\n",
      "Epoch: 39 Batch: 160 Log-loss: 0.043265681713819504\n",
      "Epoch: 39 Batch: 200 Log-loss: 0.039520423859357834\n",
      "Epoch: 39 Batch: 240 Log-loss: 0.032414909452199936\n",
      "Epoch: 39 Batch: 280 Log-loss: 0.04997335374355316\n",
      "Epoch: 39 Batch: 320 Log-loss: 0.03615770861506462\n",
      "Epoch: 39 Batch: 360 Log-loss: 0.042914699763059616\n",
      "Epoch: 39 Batch: 400 Log-loss: 0.04346966743469238\n",
      "Epoch: 39 Batch: 440 Log-loss: 0.03130762651562691\n",
      "Epoch: 39 Batch: 480 Log-loss: 0.0326087512075901\n",
      "Epoch: 39 Batch: 520 Log-loss: 0.03958697244524956\n",
      "Epoch: 39 Batch: 560 Log-loss: 0.03129606321454048\n",
      "Epoch average log-loss: 0.03866481996873128\n",
      "In Epoch: 39, val_loss: 0.04086586830838971, best_val_loss: 0.040186706425053946, best_auc: 0.9883710208829597\n",
      "Epoch: 40 Batch: 0 Log-loss: 0.03043055534362793\n",
      "Epoch: 40 Batch: 40 Log-loss: 0.05269777774810791\n",
      "Epoch: 40 Batch: 80 Log-loss: 0.02986520528793335\n",
      "Epoch: 40 Batch: 120 Log-loss: 0.03614969924092293\n",
      "Epoch: 40 Batch: 160 Log-loss: 0.049359794706106186\n",
      "Epoch: 40 Batch: 200 Log-loss: 0.047569919377565384\n",
      "Epoch: 40 Batch: 240 Log-loss: 0.04810939356684685\n",
      "Epoch: 40 Batch: 280 Log-loss: 0.052538830786943436\n",
      "Epoch: 40 Batch: 320 Log-loss: 0.03783954307436943\n",
      "Epoch: 40 Batch: 360 Log-loss: 0.03662746399641037\n",
      "Epoch: 40 Batch: 400 Log-loss: 0.03489003702998161\n",
      "Epoch: 40 Batch: 440 Log-loss: 0.0363367460668087\n",
      "Epoch: 40 Batch: 480 Log-loss: 0.04568994417786598\n",
      "Epoch: 40 Batch: 520 Log-loss: 0.030538955703377724\n",
      "Epoch: 40 Batch: 560 Log-loss: 0.02820902317762375\n",
      "Epoch average log-loss: 0.03877236867722656\n",
      "In Epoch: 40, val_loss: 0.04029921319389854, best_val_loss: 0.040186706425053946, best_auc: 0.9883710208829597\n",
      "Epoch: 41 Batch: 0 Log-loss: 0.03230425715446472\n",
      "Epoch: 41 Batch: 40 Log-loss: 0.031395576894283295\n",
      "Epoch: 41 Batch: 80 Log-loss: 0.04012296721339226\n",
      "Epoch: 41 Batch: 120 Log-loss: 0.033225227147340775\n",
      "Epoch: 41 Batch: 160 Log-loss: 0.0328420028090477\n",
      "Epoch: 41 Batch: 200 Log-loss: 0.04291641712188721\n",
      "Epoch: 41 Batch: 240 Log-loss: 0.03576202690601349\n",
      "Epoch: 41 Batch: 280 Log-loss: 0.03933008387684822\n",
      "Epoch: 41 Batch: 320 Log-loss: 0.02431599795818329\n",
      "Epoch: 41 Batch: 360 Log-loss: 0.052891701459884644\n",
      "Epoch: 41 Batch: 400 Log-loss: 0.028118900954723358\n",
      "Epoch: 41 Batch: 440 Log-loss: 0.03253036364912987\n",
      "Epoch: 41 Batch: 480 Log-loss: 0.02263588458299637\n",
      "Epoch: 41 Batch: 520 Log-loss: 0.028711579740047455\n",
      "Epoch: 41 Batch: 560 Log-loss: 0.033424388617277145\n",
      "Epoch average log-loss: 0.03862225490988099\n",
      "In Epoch: 41, val_loss: 0.04053238938806552, best_val_loss: 0.040186706425053946, best_auc: 0.9883710208829597\n",
      "Epoch: 42 Batch: 0 Log-loss: 0.053172484040260315\n",
      "Epoch: 42 Batch: 40 Log-loss: 0.0411171056330204\n",
      "Epoch: 42 Batch: 80 Log-loss: 0.05085422471165657\n",
      "Epoch: 42 Batch: 120 Log-loss: 0.033133458346128464\n",
      "Epoch: 42 Batch: 160 Log-loss: 0.03757542744278908\n",
      "Epoch: 42 Batch: 200 Log-loss: 0.0400732159614563\n",
      "Epoch: 42 Batch: 240 Log-loss: 0.03039626032114029\n",
      "Epoch: 42 Batch: 280 Log-loss: 0.03557218983769417\n",
      "Epoch: 42 Batch: 320 Log-loss: 0.050270576030015945\n",
      "Epoch: 42 Batch: 360 Log-loss: 0.03303675353527069\n",
      "Epoch: 42 Batch: 400 Log-loss: 0.03316058591008186\n",
      "Epoch: 42 Batch: 440 Log-loss: 0.030751772224903107\n",
      "Epoch: 42 Batch: 480 Log-loss: 0.051110271364450455\n",
      "Epoch: 42 Batch: 520 Log-loss: 0.038357701152563095\n",
      "Epoch: 42 Batch: 560 Log-loss: 0.04060837998986244\n",
      "Epoch average log-loss: 0.03879723139772458\n",
      "In Epoch: 42, val_loss: 0.040612477898617695, best_val_loss: 0.040186706425053946, best_auc: 0.9883710208829597\n",
      "Epoch: 43 Batch: 0 Log-loss: 0.03626561909914017\n",
      "Epoch: 43 Batch: 40 Log-loss: 0.04257172718644142\n",
      "Epoch: 43 Batch: 80 Log-loss: 0.03677674010396004\n",
      "Epoch: 43 Batch: 120 Log-loss: 0.03787804767489433\n",
      "Epoch: 43 Batch: 160 Log-loss: 0.03622953221201897\n",
      "Epoch: 43 Batch: 200 Log-loss: 0.0642922893166542\n",
      "Epoch: 43 Batch: 240 Log-loss: 0.044783756136894226\n",
      "Epoch: 43 Batch: 280 Log-loss: 0.049032196402549744\n",
      "Epoch: 43 Batch: 320 Log-loss: 0.041433315724134445\n",
      "Epoch: 43 Batch: 360 Log-loss: 0.030193017795681953\n",
      "Epoch: 43 Batch: 400 Log-loss: 0.037839606404304504\n",
      "Epoch: 43 Batch: 440 Log-loss: 0.031974177807569504\n",
      "Epoch: 43 Batch: 480 Log-loss: 0.040185872465372086\n",
      "Epoch: 43 Batch: 520 Log-loss: 0.03499634936451912\n",
      "Epoch: 43 Batch: 560 Log-loss: 0.044565845280885696\n",
      "Epoch average log-loss: 0.03882262013253889\n",
      "In Epoch: 43, val_loss: 0.04051718086056903, best_val_loss: 0.040186706425053946, best_auc: 0.9883710208829597\n",
      "Epoch: 44 Batch: 0 Log-loss: 0.03491484001278877\n",
      "Epoch: 44 Batch: 40 Log-loss: 0.03096175007522106\n",
      "Epoch: 44 Batch: 80 Log-loss: 0.03693221136927605\n",
      "Epoch: 44 Batch: 120 Log-loss: 0.038118306547403336\n",
      "Epoch: 44 Batch: 160 Log-loss: 0.044459059834480286\n",
      "Epoch: 44 Batch: 200 Log-loss: 0.04566984251141548\n",
      "Epoch: 44 Batch: 240 Log-loss: 0.03274935111403465\n",
      "Epoch: 44 Batch: 280 Log-loss: 0.031042858958244324\n",
      "Epoch: 44 Batch: 320 Log-loss: 0.03333887830376625\n",
      "Epoch: 44 Batch: 360 Log-loss: 0.023889733478426933\n",
      "Epoch: 44 Batch: 400 Log-loss: 0.033007171005010605\n",
      "Epoch: 44 Batch: 440 Log-loss: 0.044184837490320206\n",
      "Epoch: 44 Batch: 480 Log-loss: 0.038197312504053116\n",
      "Epoch: 44 Batch: 520 Log-loss: 0.04980911314487457\n",
      "Epoch: 44 Batch: 560 Log-loss: 0.04028893634676933\n",
      "Epoch average log-loss: 0.03856373263656029\n",
      "In Epoch: 44, val_loss: 0.04058741187456936, best_val_loss: 0.040186706425053946, best_auc: 0.9883710208829597\n",
      "Epoch: 45 Batch: 0 Log-loss: 0.03411976620554924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 Batch: 40 Log-loss: 0.04276290163397789\n",
      "Epoch: 45 Batch: 80 Log-loss: 0.046612005680799484\n",
      "Epoch: 45 Batch: 120 Log-loss: 0.0325748585164547\n",
      "Epoch: 45 Batch: 160 Log-loss: 0.026742802932858467\n",
      "Epoch: 45 Batch: 200 Log-loss: 0.05154842138290405\n",
      "Epoch: 45 Batch: 240 Log-loss: 0.03629693761467934\n",
      "Epoch: 45 Batch: 280 Log-loss: 0.06662656366825104\n",
      "Epoch: 45 Batch: 320 Log-loss: 0.041639167815446854\n",
      "Epoch: 45 Batch: 360 Log-loss: 0.048293378204107285\n",
      "Epoch: 45 Batch: 400 Log-loss: 0.04073016718029976\n",
      "Epoch: 45 Batch: 440 Log-loss: 0.029411233961582184\n",
      "Epoch: 45 Batch: 480 Log-loss: 0.04373687878251076\n",
      "Epoch: 45 Batch: 520 Log-loss: 0.024737780913710594\n",
      "Epoch: 45 Batch: 560 Log-loss: 0.03696656599640846\n",
      "Epoch average log-loss: 0.03863581693066018\n",
      "In Epoch: 45, val_loss: 0.040446733205950984, best_val_loss: 0.040186706425053946, best_auc: 0.9883710208829597\n",
      "Epoch: 46 Batch: 0 Log-loss: 0.038462307304143906\n",
      "Epoch: 46 Batch: 40 Log-loss: 0.032674603164196014\n",
      "Epoch: 46 Batch: 80 Log-loss: 0.07084537297487259\n",
      "Epoch: 46 Batch: 120 Log-loss: 0.037775758653879166\n",
      "Epoch: 46 Batch: 160 Log-loss: 0.03828912973403931\n",
      "Epoch: 46 Batch: 200 Log-loss: 0.04624282941222191\n",
      "Epoch: 46 Batch: 240 Log-loss: 0.03219548985362053\n",
      "Epoch: 46 Batch: 280 Log-loss: 0.05369153991341591\n",
      "Epoch: 46 Batch: 320 Log-loss: 0.038211945444345474\n",
      "Epoch: 46 Batch: 360 Log-loss: 0.05373641476035118\n",
      "Epoch: 46 Batch: 400 Log-loss: 0.03225907310843468\n",
      "Epoch: 46 Batch: 440 Log-loss: 0.04100494459271431\n",
      "Epoch: 46 Batch: 480 Log-loss: 0.03200078010559082\n",
      "Epoch: 46 Batch: 520 Log-loss: 0.038222309201955795\n",
      "Epoch: 46 Batch: 560 Log-loss: 0.06182160601019859\n",
      "Epoch average log-loss: 0.038485680320965394\n",
      "In Epoch: 46, val_loss: 0.04046250836817678, best_val_loss: 0.040186706425053946, best_auc: 0.9883710208829597\n",
      "Epoch: 47 Batch: 0 Log-loss: 0.04751254618167877\n",
      "Epoch: 47 Batch: 40 Log-loss: 0.04729163646697998\n",
      "Epoch: 47 Batch: 80 Log-loss: 0.0309718269854784\n",
      "Epoch: 47 Batch: 120 Log-loss: 0.04501519724726677\n",
      "Epoch: 47 Batch: 160 Log-loss: 0.019775936380028725\n",
      "Epoch: 47 Batch: 200 Log-loss: 0.04315704107284546\n",
      "Epoch: 47 Batch: 240 Log-loss: 0.040589798241853714\n",
      "Epoch: 47 Batch: 280 Log-loss: 0.04778363183140755\n",
      "Epoch: 47 Batch: 320 Log-loss: 0.04380851611495018\n",
      "Epoch: 47 Batch: 360 Log-loss: 0.039968475699424744\n",
      "Epoch: 47 Batch: 400 Log-loss: 0.037204112857580185\n",
      "Epoch: 47 Batch: 440 Log-loss: 0.04266509786248207\n",
      "Epoch: 47 Batch: 480 Log-loss: 0.03621479868888855\n",
      "Epoch: 47 Batch: 520 Log-loss: 0.0470159649848938\n",
      "Epoch: 47 Batch: 560 Log-loss: 0.02863321080803871\n",
      "Epoch average log-loss: 0.0385207726320784\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 47, val_loss: 0.04018010180569651, best_val_loss: 0.04018010180569651, best_auc: 0.9885332830454797\n",
      "Epoch: 48 Batch: 0 Log-loss: 0.05193670094013214\n",
      "Epoch: 48 Batch: 40 Log-loss: 0.04589935019612312\n",
      "Epoch: 48 Batch: 80 Log-loss: 0.029975367709994316\n",
      "Epoch: 48 Batch: 120 Log-loss: 0.05860741063952446\n",
      "Epoch: 48 Batch: 160 Log-loss: 0.02696717530488968\n",
      "Epoch: 48 Batch: 200 Log-loss: 0.02746116928756237\n",
      "Epoch: 48 Batch: 240 Log-loss: 0.02613206021487713\n",
      "Epoch: 48 Batch: 280 Log-loss: 0.05139771103858948\n",
      "Epoch: 48 Batch: 320 Log-loss: 0.04327995702624321\n",
      "Epoch: 48 Batch: 360 Log-loss: 0.03192701190710068\n",
      "Epoch: 48 Batch: 400 Log-loss: 0.021517110988497734\n",
      "Epoch: 48 Batch: 440 Log-loss: 0.05282412841916084\n",
      "Epoch: 48 Batch: 480 Log-loss: 0.03133736178278923\n",
      "Epoch: 48 Batch: 520 Log-loss: 0.04585343226790428\n",
      "Epoch: 48 Batch: 560 Log-loss: 0.03703247010707855\n",
      "Epoch average log-loss: 0.03847171036925699\n",
      "In Epoch: 48, val_loss: 0.0404479831706155, best_val_loss: 0.04018010180569651, best_auc: 0.9885332830454797\n",
      "Epoch: 49 Batch: 0 Log-loss: 0.05428242310881615\n",
      "Epoch: 49 Batch: 40 Log-loss: 0.03438017517328262\n",
      "Epoch: 49 Batch: 80 Log-loss: 0.04938475787639618\n",
      "Epoch: 49 Batch: 120 Log-loss: 0.05104999244213104\n",
      "Epoch: 49 Batch: 160 Log-loss: 0.05101094767451286\n",
      "Epoch: 49 Batch: 200 Log-loss: 0.03132464736700058\n",
      "Epoch: 49 Batch: 240 Log-loss: 0.0322323776781559\n",
      "Epoch: 49 Batch: 280 Log-loss: 0.05893753468990326\n",
      "Epoch: 49 Batch: 320 Log-loss: 0.033526461571455\n",
      "Epoch: 49 Batch: 360 Log-loss: 0.04290003702044487\n",
      "Epoch: 49 Batch: 400 Log-loss: 0.03684626892209053\n",
      "Epoch: 49 Batch: 440 Log-loss: 0.05391981825232506\n",
      "Epoch: 49 Batch: 480 Log-loss: 0.04600916802883148\n",
      "Epoch: 49 Batch: 520 Log-loss: 0.03633493185043335\n",
      "Epoch: 49 Batch: 560 Log-loss: 0.03656522557139397\n",
      "Epoch average log-loss: 0.03858527485281229\n",
      "In Epoch: 49, val_loss: 0.04048676612182387, best_val_loss: 0.04018010180569651, best_auc: 0.9885332830454797\n",
      "Epoch: 50 Batch: 0 Log-loss: 0.034226853400468826\n",
      "Epoch: 50 Batch: 40 Log-loss: 0.04514523968100548\n",
      "Epoch: 50 Batch: 80 Log-loss: 0.030915817245841026\n",
      "Epoch: 50 Batch: 120 Log-loss: 0.029113104566931725\n",
      "Epoch: 50 Batch: 160 Log-loss: 0.051904719322919846\n",
      "Epoch: 50 Batch: 200 Log-loss: 0.03721420094370842\n",
      "Epoch: 50 Batch: 240 Log-loss: 0.045562878251075745\n",
      "Epoch: 50 Batch: 280 Log-loss: 0.02750968746840954\n",
      "Epoch: 50 Batch: 320 Log-loss: 0.047670841217041016\n",
      "Epoch: 50 Batch: 360 Log-loss: 0.03290153667330742\n",
      "Epoch: 50 Batch: 400 Log-loss: 0.026153311133384705\n",
      "Epoch: 50 Batch: 440 Log-loss: 0.0325874499976635\n",
      "Epoch: 50 Batch: 480 Log-loss: 0.0349467433989048\n",
      "Epoch: 50 Batch: 520 Log-loss: 0.04484565183520317\n",
      "Epoch: 50 Batch: 560 Log-loss: 0.036420825868844986\n",
      "Epoch average log-loss: 0.038661496637256015\n",
      "In Epoch: 50, val_loss: 0.04073751221741686, best_val_loss: 0.04018010180569651, best_auc: 0.9885332830454797\n",
      "Epoch: 51 Batch: 0 Log-loss: 0.04090534523129463\n",
      "Epoch: 51 Batch: 40 Log-loss: 0.04180576279759407\n",
      "Epoch: 51 Batch: 80 Log-loss: 0.047864753752946854\n",
      "Epoch: 51 Batch: 120 Log-loss: 0.04071364179253578\n",
      "Epoch: 51 Batch: 160 Log-loss: 0.03679405525326729\n",
      "Epoch: 51 Batch: 200 Log-loss: 0.022634224966168404\n",
      "Epoch: 51 Batch: 240 Log-loss: 0.04332849010825157\n",
      "Epoch: 51 Batch: 280 Log-loss: 0.029556818306446075\n",
      "Epoch: 51 Batch: 320 Log-loss: 0.04556654393672943\n",
      "Epoch: 51 Batch: 360 Log-loss: 0.04029517620801926\n",
      "Epoch: 51 Batch: 400 Log-loss: 0.04393002763390541\n",
      "Epoch: 51 Batch: 440 Log-loss: 0.04369731247425079\n",
      "Epoch: 51 Batch: 480 Log-loss: 0.04212659224867821\n",
      "Epoch: 51 Batch: 520 Log-loss: 0.029734069481492043\n",
      "Epoch: 51 Batch: 560 Log-loss: 0.04120153188705444\n",
      "Epoch average log-loss: 0.03850564856880478\n",
      "In Epoch: 51, val_loss: 0.040553703308155435, best_val_loss: 0.04018010180569651, best_auc: 0.9885332830454797\n",
      "Epoch: 52 Batch: 0 Log-loss: 0.03613010421395302\n",
      "Epoch: 52 Batch: 40 Log-loss: 0.03378573805093765\n",
      "Epoch: 52 Batch: 80 Log-loss: 0.029775872826576233\n",
      "Epoch: 52 Batch: 120 Log-loss: 0.04167106747627258\n",
      "Epoch: 52 Batch: 160 Log-loss: 0.04488920792937279\n",
      "Epoch: 52 Batch: 200 Log-loss: 0.049536824226379395\n",
      "Epoch: 52 Batch: 240 Log-loss: 0.026780812069773674\n",
      "Epoch: 52 Batch: 280 Log-loss: 0.02932843752205372\n",
      "Epoch: 52 Batch: 320 Log-loss: 0.03920581936836243\n",
      "Epoch: 52 Batch: 360 Log-loss: 0.0404871366918087\n",
      "Epoch: 52 Batch: 400 Log-loss: 0.033214397728443146\n",
      "Epoch: 52 Batch: 440 Log-loss: 0.03137556090950966\n",
      "Epoch: 52 Batch: 480 Log-loss: 0.031573131680488586\n",
      "Epoch: 52 Batch: 520 Log-loss: 0.04431276395916939\n",
      "Epoch: 52 Batch: 560 Log-loss: 0.06176755204796791\n",
      "Epoch average log-loss: 0.038535070788514404\n",
      "In Epoch: 52, val_loss: 0.040299090794569926, best_val_loss: 0.04018010180569651, best_auc: 0.9885332830454797\n",
      "Epoch: 53 Batch: 0 Log-loss: 0.041842520236968994\n",
      "Epoch: 53 Batch: 40 Log-loss: 0.01582171581685543\n",
      "Epoch: 53 Batch: 80 Log-loss: 0.027443239465355873\n",
      "Epoch: 53 Batch: 120 Log-loss: 0.05066417157649994\n",
      "Epoch: 53 Batch: 160 Log-loss: 0.022548681125044823\n",
      "Epoch: 53 Batch: 200 Log-loss: 0.020275281742215157\n",
      "Epoch: 53 Batch: 240 Log-loss: 0.024694932624697685\n",
      "Epoch: 53 Batch: 280 Log-loss: 0.041905466467142105\n",
      "Epoch: 53 Batch: 320 Log-loss: 0.035411033779382706\n",
      "Epoch: 53 Batch: 360 Log-loss: 0.03551555424928665\n",
      "Epoch: 53 Batch: 400 Log-loss: 0.04128916934132576\n",
      "Epoch: 53 Batch: 440 Log-loss: 0.032345693558454514\n",
      "Epoch: 53 Batch: 480 Log-loss: 0.03225414454936981\n",
      "Epoch: 53 Batch: 520 Log-loss: 0.04965733364224434\n",
      "Epoch: 53 Batch: 560 Log-loss: 0.03909802809357643\n",
      "Epoch average log-loss: 0.03857769602244454\n",
      "In Epoch: 53, val_loss: 0.040641152975227406, best_val_loss: 0.04018010180569651, best_auc: 0.9885332830454797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54 Batch: 0 Log-loss: 0.030182503163814545\n",
      "Epoch: 54 Batch: 40 Log-loss: 0.05473877117037773\n",
      "Epoch: 54 Batch: 80 Log-loss: 0.05677187815308571\n",
      "Epoch: 54 Batch: 120 Log-loss: 0.034550223499536514\n",
      "Epoch: 54 Batch: 160 Log-loss: 0.035177625715732574\n",
      "Epoch: 54 Batch: 200 Log-loss: 0.03984302654862404\n",
      "Epoch: 54 Batch: 240 Log-loss: 0.033442337065935135\n",
      "Epoch: 54 Batch: 280 Log-loss: 0.038646306842565536\n",
      "Epoch: 54 Batch: 320 Log-loss: 0.039395157247781754\n",
      "Epoch: 54 Batch: 360 Log-loss: 0.03454142063856125\n",
      "Epoch: 54 Batch: 400 Log-loss: 0.03911645710468292\n",
      "Epoch: 54 Batch: 440 Log-loss: 0.03354009985923767\n",
      "Epoch: 54 Batch: 480 Log-loss: 0.046332817524671555\n",
      "Epoch: 54 Batch: 520 Log-loss: 0.04332655295729637\n",
      "Epoch: 54 Batch: 560 Log-loss: 0.03787366300821304\n",
      "Epoch average log-loss: 0.038455198196295115\n",
      "In Epoch: 54, val_loss: 0.04076072098576161, best_val_loss: 0.04018010180569651, best_auc: 0.9885332830454797\n",
      "Epoch: 55 Batch: 0 Log-loss: 0.04144451394677162\n",
      "Epoch: 55 Batch: 40 Log-loss: 0.0547616183757782\n",
      "Epoch: 55 Batch: 80 Log-loss: 0.03970342129468918\n",
      "Epoch: 55 Batch: 120 Log-loss: 0.04583965614438057\n",
      "Epoch: 55 Batch: 160 Log-loss: 0.043814871460199356\n",
      "Epoch: 55 Batch: 200 Log-loss: 0.03691268339753151\n",
      "Epoch: 55 Batch: 240 Log-loss: 0.054283540695905685\n",
      "Epoch: 55 Batch: 280 Log-loss: 0.023129291832447052\n",
      "Epoch: 55 Batch: 320 Log-loss: 0.03799417242407799\n",
      "Epoch: 55 Batch: 360 Log-loss: 0.03210477903485298\n",
      "Epoch: 55 Batch: 400 Log-loss: 0.04351549223065376\n",
      "Epoch: 55 Batch: 440 Log-loss: 0.04032742232084274\n",
      "Epoch: 55 Batch: 480 Log-loss: 0.03767877817153931\n",
      "Epoch: 55 Batch: 520 Log-loss: 0.03500035032629967\n",
      "Epoch: 55 Batch: 560 Log-loss: 0.03964555636048317\n",
      "Epoch average log-loss: 0.03847437648468518\n",
      "In Epoch: 55, val_loss: 0.04033557967750414, best_val_loss: 0.04018010180569651, best_auc: 0.9885332830454797\n",
      "Epoch: 56 Batch: 0 Log-loss: 0.03711475804448128\n",
      "Epoch: 56 Batch: 40 Log-loss: 0.03228998929262161\n",
      "Epoch: 56 Batch: 80 Log-loss: 0.0516720712184906\n",
      "Epoch: 56 Batch: 120 Log-loss: 0.03783701732754707\n",
      "Epoch: 56 Batch: 160 Log-loss: 0.041860487312078476\n",
      "Epoch: 56 Batch: 200 Log-loss: 0.034593239426612854\n",
      "Epoch: 56 Batch: 240 Log-loss: 0.03616932034492493\n",
      "Epoch: 56 Batch: 280 Log-loss: 0.03349298983812332\n",
      "Epoch: 56 Batch: 320 Log-loss: 0.04912582412362099\n",
      "Epoch: 56 Batch: 360 Log-loss: 0.0433785580098629\n",
      "Epoch: 56 Batch: 400 Log-loss: 0.03802168741822243\n",
      "Epoch: 56 Batch: 440 Log-loss: 0.03458099067211151\n",
      "Epoch: 56 Batch: 480 Log-loss: 0.03498709574341774\n",
      "Epoch: 56 Batch: 520 Log-loss: 0.05969530716538429\n",
      "Epoch: 56 Batch: 560 Log-loss: 0.031020522117614746\n",
      "Epoch average log-loss: 0.0386247377476788\n",
      "In Epoch: 56, val_loss: 0.040639391964929265, best_val_loss: 0.04018010180569651, best_auc: 0.9885332830454797\n",
      "Epoch: 57 Batch: 0 Log-loss: 0.033248741179704666\n",
      "Epoch: 57 Batch: 40 Log-loss: 0.041488226503133774\n",
      "Epoch: 57 Batch: 80 Log-loss: 0.025804392993450165\n",
      "Epoch: 57 Batch: 120 Log-loss: 0.02883499301970005\n",
      "Epoch: 57 Batch: 160 Log-loss: 0.02961765229701996\n",
      "Epoch: 57 Batch: 200 Log-loss: 0.051344480365514755\n",
      "Epoch: 57 Batch: 240 Log-loss: 0.05846771225333214\n",
      "Epoch: 57 Batch: 280 Log-loss: 0.04693153128027916\n",
      "Epoch: 57 Batch: 320 Log-loss: 0.03858185186982155\n",
      "Epoch: 57 Batch: 360 Log-loss: 0.04942167177796364\n",
      "Epoch: 57 Batch: 400 Log-loss: 0.046568337827920914\n",
      "Epoch: 57 Batch: 440 Log-loss: 0.04293147101998329\n",
      "Epoch: 57 Batch: 480 Log-loss: 0.0299969669431448\n",
      "Epoch: 57 Batch: 520 Log-loss: 0.03848911076784134\n",
      "Epoch: 57 Batch: 560 Log-loss: 0.03866113722324371\n",
      "Epoch average log-loss: 0.038310178997926415\n",
      "In Epoch: 57, val_loss: 0.04054262815777847, best_val_loss: 0.04018010180569651, best_auc: 0.9885332830454797\n",
      "Epoch: 58 Batch: 0 Log-loss: 0.034386876970529556\n",
      "Epoch: 58 Batch: 40 Log-loss: 0.05012324079871178\n",
      "Epoch: 58 Batch: 80 Log-loss: 0.0503537617623806\n",
      "Epoch: 58 Batch: 120 Log-loss: 0.03372042253613472\n",
      "Epoch: 58 Batch: 160 Log-loss: 0.0430225171148777\n",
      "Epoch: 58 Batch: 200 Log-loss: 0.05472927913069725\n",
      "Epoch: 58 Batch: 240 Log-loss: 0.0446060486137867\n",
      "Epoch: 58 Batch: 280 Log-loss: 0.036523159593343735\n",
      "Epoch: 58 Batch: 320 Log-loss: 0.02726237289607525\n",
      "Epoch: 58 Batch: 360 Log-loss: 0.043033212423324585\n",
      "Epoch: 58 Batch: 400 Log-loss: 0.03659874573349953\n",
      "Epoch: 58 Batch: 440 Log-loss: 0.03919941559433937\n",
      "Epoch: 58 Batch: 480 Log-loss: 0.0363428108394146\n",
      "Epoch: 58 Batch: 520 Log-loss: 0.039793942123651505\n",
      "Epoch: 58 Batch: 560 Log-loss: 0.041020214557647705\n",
      "Epoch average log-loss: 0.0385347605716171\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 58, val_loss: 0.04017193529996758, best_val_loss: 0.04017193529996758, best_auc: 0.9884878069519111\n",
      "Epoch: 59 Batch: 0 Log-loss: 0.025886883959174156\n",
      "Epoch: 59 Batch: 40 Log-loss: 0.04603319242596626\n",
      "Epoch: 59 Batch: 80 Log-loss: 0.024259427562355995\n",
      "Epoch: 59 Batch: 120 Log-loss: 0.03662611171603203\n",
      "Epoch: 59 Batch: 160 Log-loss: 0.03598824515938759\n",
      "Epoch: 59 Batch: 200 Log-loss: 0.028224622830748558\n",
      "Epoch: 59 Batch: 240 Log-loss: 0.03065630979835987\n",
      "Epoch: 59 Batch: 280 Log-loss: 0.038198310881853104\n",
      "Epoch: 59 Batch: 320 Log-loss: 0.02883322350680828\n",
      "Epoch: 59 Batch: 360 Log-loss: 0.04094086214900017\n",
      "Epoch: 59 Batch: 400 Log-loss: 0.0395595021545887\n",
      "Epoch: 59 Batch: 440 Log-loss: 0.031486090272665024\n",
      "Epoch: 59 Batch: 480 Log-loss: 0.03657344728708267\n",
      "Epoch: 59 Batch: 520 Log-loss: 0.03025670163333416\n",
      "Epoch: 59 Batch: 560 Log-loss: 0.04772273451089859\n",
      "Epoch average log-loss: 0.03848724749072322\n",
      "In Epoch: 59, val_loss: 0.040368820366798994, best_val_loss: 0.04017193529996758, best_auc: 0.9884878069519111\n",
      "Epoch: 60 Batch: 0 Log-loss: 0.038666192442178726\n",
      "Epoch: 60 Batch: 40 Log-loss: 0.04751348868012428\n",
      "Epoch: 60 Batch: 80 Log-loss: 0.02636966109275818\n",
      "Epoch: 60 Batch: 120 Log-loss: 0.03842611610889435\n",
      "Epoch: 60 Batch: 160 Log-loss: 0.030950471758842468\n",
      "Epoch: 60 Batch: 200 Log-loss: 0.04469313099980354\n",
      "Epoch: 60 Batch: 240 Log-loss: 0.0312640555202961\n",
      "Epoch: 60 Batch: 280 Log-loss: 0.04282565414905548\n",
      "Epoch: 60 Batch: 320 Log-loss: 0.030953802168369293\n",
      "Epoch: 60 Batch: 360 Log-loss: 0.03283356502652168\n",
      "Epoch: 60 Batch: 400 Log-loss: 0.03364783152937889\n",
      "Epoch: 60 Batch: 440 Log-loss: 0.02965915948152542\n",
      "Epoch: 60 Batch: 480 Log-loss: 0.049874529242515564\n",
      "Epoch: 60 Batch: 520 Log-loss: 0.038040172308683395\n",
      "Epoch: 60 Batch: 560 Log-loss: 0.03960532695055008\n",
      "Epoch average log-loss: 0.03839037433665778\n",
      "In Epoch: 60, val_loss: 0.040574555568550004, best_val_loss: 0.04017193529996758, best_auc: 0.9884878069519111\n",
      "Epoch: 61 Batch: 0 Log-loss: 0.03541792929172516\n",
      "Epoch: 61 Batch: 40 Log-loss: 0.03830398991703987\n",
      "Epoch: 61 Batch: 80 Log-loss: 0.04196784272789955\n",
      "Epoch: 61 Batch: 120 Log-loss: 0.03358658775687218\n",
      "Epoch: 61 Batch: 160 Log-loss: 0.04302877187728882\n",
      "Epoch: 61 Batch: 200 Log-loss: 0.028423160314559937\n",
      "Epoch: 61 Batch: 240 Log-loss: 0.03497976437211037\n",
      "Epoch: 61 Batch: 280 Log-loss: 0.029952986165881157\n",
      "Epoch: 61 Batch: 320 Log-loss: 0.037567682564258575\n",
      "Epoch: 61 Batch: 360 Log-loss: 0.03758406266570091\n",
      "Epoch: 61 Batch: 400 Log-loss: 0.03453812003135681\n",
      "Epoch: 61 Batch: 440 Log-loss: 0.03374588117003441\n",
      "Epoch: 61 Batch: 480 Log-loss: 0.0467870719730854\n",
      "Epoch: 61 Batch: 520 Log-loss: 0.042397499084472656\n",
      "Epoch: 61 Batch: 560 Log-loss: 0.03389482572674751\n",
      "Epoch average log-loss: 0.038415603219930615\n",
      "In Epoch: 61, val_loss: 0.04042789181947463, best_val_loss: 0.04017193529996758, best_auc: 0.9884878069519111\n",
      "Epoch: 62 Batch: 0 Log-loss: 0.04663274809718132\n",
      "Epoch: 62 Batch: 40 Log-loss: 0.022752203047275543\n",
      "Epoch: 62 Batch: 80 Log-loss: 0.032125361263751984\n",
      "Epoch: 62 Batch: 120 Log-loss: 0.04346677288413048\n",
      "Epoch: 62 Batch: 160 Log-loss: 0.038522519171237946\n",
      "Epoch: 62 Batch: 200 Log-loss: 0.031865194439888\n",
      "Epoch: 62 Batch: 240 Log-loss: 0.03018450178205967\n",
      "Epoch: 62 Batch: 280 Log-loss: 0.04950245842337608\n",
      "Epoch: 62 Batch: 320 Log-loss: 0.0320366770029068\n",
      "Epoch: 62 Batch: 360 Log-loss: 0.057274699211120605\n",
      "Epoch: 62 Batch: 400 Log-loss: 0.04462602362036705\n",
      "Epoch: 62 Batch: 440 Log-loss: 0.030106158927083015\n",
      "Epoch: 62 Batch: 480 Log-loss: 0.035335149616003036\n",
      "Epoch: 62 Batch: 520 Log-loss: 0.037741370499134064\n",
      "Epoch: 62 Batch: 560 Log-loss: 0.028011968359351158\n",
      "Epoch average log-loss: 0.03851748225279152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Epoch: 62, val_loss: 0.04035477878829759, best_val_loss: 0.04017193529996758, best_auc: 0.9884878069519111\n",
      "Epoch: 63 Batch: 0 Log-loss: 0.03785282373428345\n",
      "Epoch: 63 Batch: 40 Log-loss: 0.04228709265589714\n",
      "Epoch: 63 Batch: 80 Log-loss: 0.032104965299367905\n",
      "Epoch: 63 Batch: 120 Log-loss: 0.04533010721206665\n",
      "Epoch: 63 Batch: 160 Log-loss: 0.0485551618039608\n",
      "Epoch: 63 Batch: 200 Log-loss: 0.0406598262488842\n",
      "Epoch: 63 Batch: 240 Log-loss: 0.05346819758415222\n",
      "Epoch: 63 Batch: 280 Log-loss: 0.03446628898382187\n",
      "Epoch: 63 Batch: 320 Log-loss: 0.03861179202795029\n",
      "Epoch: 63 Batch: 360 Log-loss: 0.041025567799806595\n",
      "Epoch: 63 Batch: 400 Log-loss: 0.04565812274813652\n",
      "Epoch: 63 Batch: 440 Log-loss: 0.03800908103585243\n",
      "Epoch: 63 Batch: 480 Log-loss: 0.029123537242412567\n",
      "Epoch: 63 Batch: 520 Log-loss: 0.03334012255072594\n",
      "Epoch: 63 Batch: 560 Log-loss: 0.03838293254375458\n",
      "Epoch average log-loss: 0.038482333726382684\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 63, val_loss: 0.040002458294543956, best_val_loss: 0.040002458294543956, best_auc: 0.9885077247716612\n",
      "Epoch: 64 Batch: 0 Log-loss: 0.03777708858251572\n",
      "Epoch: 64 Batch: 40 Log-loss: 0.05277794599533081\n",
      "Epoch: 64 Batch: 80 Log-loss: 0.036693889647722244\n",
      "Epoch: 64 Batch: 120 Log-loss: 0.05012066289782524\n",
      "Epoch: 64 Batch: 160 Log-loss: 0.036563221365213394\n",
      "Epoch: 64 Batch: 200 Log-loss: 0.03128516301512718\n",
      "Epoch: 64 Batch: 240 Log-loss: 0.037638697773218155\n",
      "Epoch: 64 Batch: 280 Log-loss: 0.04849379137158394\n",
      "Epoch: 64 Batch: 320 Log-loss: 0.044764842838048935\n",
      "Epoch: 64 Batch: 360 Log-loss: 0.036722902208566666\n",
      "Epoch: 64 Batch: 400 Log-loss: 0.044130269438028336\n",
      "Epoch: 64 Batch: 440 Log-loss: 0.050592631101608276\n",
      "Epoch: 64 Batch: 480 Log-loss: 0.03520648926496506\n",
      "Epoch: 64 Batch: 520 Log-loss: 0.03753561154007912\n",
      "Epoch: 64 Batch: 560 Log-loss: 0.035967662930488586\n",
      "Epoch average log-loss: 0.03835272883630491\n",
      "In Epoch: 64, val_loss: 0.04036161641407351, best_val_loss: 0.040002458294543956, best_auc: 0.9885077247716612\n",
      "Epoch: 65 Batch: 0 Log-loss: 0.036800313740968704\n",
      "Epoch: 65 Batch: 40 Log-loss: 0.03869720175862312\n",
      "Epoch: 65 Batch: 80 Log-loss: 0.021373188123106956\n",
      "Epoch: 65 Batch: 120 Log-loss: 0.029140954837203026\n",
      "Epoch: 65 Batch: 160 Log-loss: 0.0374300479888916\n",
      "Epoch: 65 Batch: 200 Log-loss: 0.0462004728615284\n",
      "Epoch: 65 Batch: 240 Log-loss: 0.03160581365227699\n",
      "Epoch: 65 Batch: 280 Log-loss: 0.04622277989983559\n",
      "Epoch: 65 Batch: 320 Log-loss: 0.045367687940597534\n",
      "Epoch: 65 Batch: 360 Log-loss: 0.04066026210784912\n",
      "Epoch: 65 Batch: 400 Log-loss: 0.037189092487096786\n",
      "Epoch: 65 Batch: 440 Log-loss: 0.034651823341846466\n",
      "Epoch: 65 Batch: 480 Log-loss: 0.03555717691779137\n",
      "Epoch: 65 Batch: 520 Log-loss: 0.02831951342523098\n",
      "Epoch: 65 Batch: 560 Log-loss: 0.03288029134273529\n",
      "Epoch average log-loss: 0.03850765675971551\n",
      "In Epoch: 65, val_loss: 0.04038121024561301, best_val_loss: 0.040002458294543956, best_auc: 0.9885077247716612\n",
      "Epoch: 66 Batch: 0 Log-loss: 0.04985632374882698\n",
      "Epoch: 66 Batch: 40 Log-loss: 0.0472557358443737\n",
      "Epoch: 66 Batch: 80 Log-loss: 0.044340718537569046\n",
      "Epoch: 66 Batch: 120 Log-loss: 0.03487041965126991\n",
      "Epoch: 66 Batch: 160 Log-loss: 0.04098089784383774\n",
      "Epoch: 66 Batch: 200 Log-loss: 0.038779474794864655\n",
      "Epoch: 66 Batch: 240 Log-loss: 0.02783992327749729\n",
      "Epoch: 66 Batch: 280 Log-loss: 0.044517382979393005\n",
      "Epoch: 66 Batch: 320 Log-loss: 0.04460630193352699\n",
      "Epoch: 66 Batch: 360 Log-loss: 0.029162133112549782\n",
      "Epoch: 66 Batch: 400 Log-loss: 0.03833332657814026\n",
      "Epoch: 66 Batch: 440 Log-loss: 0.030098386108875275\n",
      "Epoch: 66 Batch: 480 Log-loss: 0.04731426760554314\n",
      "Epoch: 66 Batch: 520 Log-loss: 0.0418303944170475\n",
      "Epoch: 66 Batch: 560 Log-loss: 0.04356906935572624\n",
      "Epoch average log-loss: 0.03844674109215183\n",
      "In Epoch: 66, val_loss: 0.040427073320999395, best_val_loss: 0.040002458294543956, best_auc: 0.9885077247716612\n",
      "Epoch: 67 Batch: 0 Log-loss: 0.024645298719406128\n",
      "Epoch: 67 Batch: 40 Log-loss: 0.027476845309138298\n",
      "Epoch: 67 Batch: 80 Log-loss: 0.058481600135564804\n",
      "Epoch: 67 Batch: 120 Log-loss: 0.031644951552152634\n",
      "Epoch: 67 Batch: 160 Log-loss: 0.027588829398155212\n",
      "Epoch: 67 Batch: 200 Log-loss: 0.02724306285381317\n",
      "Epoch: 67 Batch: 240 Log-loss: 0.03224703297019005\n",
      "Epoch: 67 Batch: 280 Log-loss: 0.034461360424757004\n",
      "Epoch: 67 Batch: 320 Log-loss: 0.039110854268074036\n",
      "Epoch: 67 Batch: 360 Log-loss: 0.03190899267792702\n",
      "Epoch: 67 Batch: 400 Log-loss: 0.0411752350628376\n",
      "Epoch: 67 Batch: 440 Log-loss: 0.02916577458381653\n",
      "Epoch: 67 Batch: 480 Log-loss: 0.035274334251880646\n",
      "Epoch: 67 Batch: 520 Log-loss: 0.02486506849527359\n",
      "Epoch: 67 Batch: 560 Log-loss: 0.045124348253011703\n",
      "Epoch average log-loss: 0.03845358127062874\n",
      "In Epoch: 67, val_loss: 0.040313316201211376, best_val_loss: 0.040002458294543956, best_auc: 0.9885077247716612\n",
      "Epoch: 68 Batch: 0 Log-loss: 0.04358555004000664\n",
      "Epoch: 68 Batch: 40 Log-loss: 0.04902949929237366\n",
      "Epoch: 68 Batch: 80 Log-loss: 0.04010237380862236\n",
      "Epoch: 68 Batch: 120 Log-loss: 0.03721809387207031\n",
      "Epoch: 68 Batch: 160 Log-loss: 0.05594596266746521\n",
      "Epoch: 68 Batch: 200 Log-loss: 0.04127248004078865\n",
      "Epoch: 68 Batch: 240 Log-loss: 0.033489953726530075\n",
      "Epoch: 68 Batch: 280 Log-loss: 0.03351588919758797\n",
      "Epoch: 68 Batch: 320 Log-loss: 0.03424578532576561\n",
      "Epoch: 68 Batch: 360 Log-loss: 0.03203278407454491\n",
      "Epoch: 68 Batch: 400 Log-loss: 0.034525949507951736\n",
      "Epoch: 68 Batch: 440 Log-loss: 0.02427092008292675\n",
      "Epoch: 68 Batch: 480 Log-loss: 0.04875992238521576\n",
      "Epoch: 68 Batch: 520 Log-loss: 0.03899458423256874\n",
      "Epoch: 68 Batch: 560 Log-loss: 0.03280961886048317\n",
      "Epoch average log-loss: 0.03847503438259342\n",
      "In Epoch: 68, val_loss: 0.04055999763802035, best_val_loss: 0.040002458294543956, best_auc: 0.9885077247716612\n",
      "Epoch: 69 Batch: 0 Log-loss: 0.03690115734934807\n",
      "Epoch: 69 Batch: 40 Log-loss: 0.04149184003472328\n",
      "Epoch: 69 Batch: 80 Log-loss: 0.03170298784971237\n",
      "Epoch: 69 Batch: 120 Log-loss: 0.047587405890226364\n",
      "Epoch: 69 Batch: 160 Log-loss: 0.03861680626869202\n",
      "Epoch: 69 Batch: 200 Log-loss: 0.03886779397726059\n",
      "Epoch: 69 Batch: 240 Log-loss: 0.04012591019272804\n",
      "Epoch: 69 Batch: 280 Log-loss: 0.0409875325858593\n",
      "Epoch: 69 Batch: 320 Log-loss: 0.018043743446469307\n",
      "Epoch: 69 Batch: 360 Log-loss: 0.04956919327378273\n",
      "Epoch: 69 Batch: 400 Log-loss: 0.04884648695588112\n",
      "Epoch: 69 Batch: 440 Log-loss: 0.03127690404653549\n",
      "Epoch: 69 Batch: 480 Log-loss: 0.03979642689228058\n",
      "Epoch: 69 Batch: 520 Log-loss: 0.04302038252353668\n",
      "Epoch: 69 Batch: 560 Log-loss: 0.03898413106799126\n",
      "Epoch average log-loss: 0.0383173195029875\n",
      "In Epoch: 69, val_loss: 0.04008737722133187, best_val_loss: 0.040002458294543956, best_auc: 0.9885077247716612\n",
      "Epoch: 70 Batch: 0 Log-loss: 0.028415730223059654\n",
      "Epoch: 70 Batch: 40 Log-loss: 0.04111693054437637\n",
      "Epoch: 70 Batch: 80 Log-loss: 0.04323764145374298\n",
      "Epoch: 70 Batch: 120 Log-loss: 0.029629021883010864\n",
      "Epoch: 70 Batch: 160 Log-loss: 0.03989941626787186\n",
      "Epoch: 70 Batch: 200 Log-loss: 0.02557181566953659\n",
      "Epoch: 70 Batch: 240 Log-loss: 0.03385208174586296\n",
      "Epoch: 70 Batch: 280 Log-loss: 0.04738562926650047\n",
      "Epoch: 70 Batch: 320 Log-loss: 0.03668051213026047\n",
      "Epoch: 70 Batch: 360 Log-loss: 0.03581437095999718\n",
      "Epoch: 70 Batch: 400 Log-loss: 0.05804825946688652\n",
      "Epoch: 70 Batch: 440 Log-loss: 0.05646290257573128\n",
      "Epoch: 70 Batch: 480 Log-loss: 0.03127029165625572\n",
      "Epoch: 70 Batch: 520 Log-loss: 0.03847174718976021\n",
      "Epoch: 70 Batch: 560 Log-loss: 0.04272721707820892\n",
      "Epoch average log-loss: 0.03829249456258757\n",
      "In Epoch: 70, val_loss: 0.04060845991868959, best_val_loss: 0.040002458294543956, best_auc: 0.9885077247716612\n",
      "Epoch: 71 Batch: 0 Log-loss: 0.03260590508580208\n",
      "Epoch: 71 Batch: 40 Log-loss: 0.0528535395860672\n",
      "Epoch: 71 Batch: 80 Log-loss: 0.03986181318759918\n",
      "Epoch: 71 Batch: 120 Log-loss: 0.03641793876886368\n",
      "Epoch: 71 Batch: 160 Log-loss: 0.04922858253121376\n",
      "Epoch: 71 Batch: 200 Log-loss: 0.038919638842344284\n",
      "Epoch: 71 Batch: 240 Log-loss: 0.03294549509882927\n",
      "Epoch: 71 Batch: 280 Log-loss: 0.04624614119529724\n",
      "Epoch: 71 Batch: 320 Log-loss: 0.04698766767978668\n",
      "Epoch: 71 Batch: 360 Log-loss: 0.049715373665094376\n",
      "Epoch: 71 Batch: 400 Log-loss: 0.03386181965470314\n",
      "Epoch: 71 Batch: 440 Log-loss: 0.06183615326881409\n",
      "Epoch: 71 Batch: 480 Log-loss: 0.054217930883169174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71 Batch: 520 Log-loss: 0.027956413105130196\n",
      "Epoch: 71 Batch: 560 Log-loss: 0.039534032344818115\n",
      "Epoch average log-loss: 0.03840566971838208\n",
      "In Epoch: 71, val_loss: 0.04042383342511236, best_val_loss: 0.040002458294543956, best_auc: 0.9885077247716612\n",
      "Epoch: 72 Batch: 0 Log-loss: 0.043388232588768005\n",
      "Epoch: 72 Batch: 40 Log-loss: 0.04257170855998993\n",
      "Epoch: 72 Batch: 80 Log-loss: 0.03889037296175957\n",
      "Epoch: 72 Batch: 120 Log-loss: 0.05236242711544037\n",
      "Epoch: 72 Batch: 160 Log-loss: 0.03927769139409065\n",
      "Epoch: 72 Batch: 200 Log-loss: 0.04462650045752525\n",
      "Epoch: 72 Batch: 240 Log-loss: 0.03997233137488365\n",
      "Epoch: 72 Batch: 280 Log-loss: 0.023599199950695038\n",
      "Epoch: 72 Batch: 320 Log-loss: 0.05646069347858429\n",
      "Epoch: 72 Batch: 360 Log-loss: 0.03739509359002113\n",
      "Epoch: 72 Batch: 400 Log-loss: 0.03624780476093292\n",
      "Epoch: 72 Batch: 440 Log-loss: 0.03445405140519142\n",
      "Epoch: 72 Batch: 480 Log-loss: 0.02888009138405323\n",
      "Epoch: 72 Batch: 520 Log-loss: 0.036881472915410995\n",
      "Epoch: 72 Batch: 560 Log-loss: 0.046146586537361145\n",
      "Epoch average log-loss: 0.038341744553430805\n",
      "In Epoch: 72, val_loss: 0.04060743210522642, best_val_loss: 0.040002458294543956, best_auc: 0.9885077247716612\n",
      "Epoch: 73 Batch: 0 Log-loss: 0.03681091591715813\n",
      "Epoch: 73 Batch: 40 Log-loss: 0.028203638270497322\n",
      "Epoch: 73 Batch: 80 Log-loss: 0.04350126162171364\n",
      "Epoch: 73 Batch: 120 Log-loss: 0.04072774574160576\n",
      "Epoch: 73 Batch: 160 Log-loss: 0.054171372205019\n",
      "Epoch: 73 Batch: 200 Log-loss: 0.04148923605680466\n",
      "Epoch: 73 Batch: 240 Log-loss: 0.03620713949203491\n",
      "Epoch: 73 Batch: 280 Log-loss: 0.04338604211807251\n",
      "Epoch: 73 Batch: 320 Log-loss: 0.04761041700839996\n",
      "Epoch: 73 Batch: 360 Log-loss: 0.0456194169819355\n",
      "Epoch: 73 Batch: 400 Log-loss: 0.05785146355628967\n",
      "Epoch: 73 Batch: 440 Log-loss: 0.031878892332315445\n",
      "Epoch: 73 Batch: 480 Log-loss: 0.027688106521964073\n",
      "Epoch: 73 Batch: 520 Log-loss: 0.03687930479645729\n",
      "Epoch: 73 Batch: 560 Log-loss: 0.029610399156808853\n",
      "Epoch average log-loss: 0.03829055259370112\n",
      "In Epoch: 73, val_loss: 0.040418851297360565, best_val_loss: 0.040002458294543956, best_auc: 0.9885077247716612\n",
      "Epoch: 74 Batch: 0 Log-loss: 0.06215443089604378\n",
      "Epoch: 74 Batch: 40 Log-loss: 0.04044635221362114\n",
      "Epoch: 74 Batch: 80 Log-loss: 0.034429509192705154\n",
      "Epoch: 74 Batch: 120 Log-loss: 0.04139954224228859\n",
      "Epoch: 74 Batch: 160 Log-loss: 0.052344050258398056\n",
      "Epoch: 74 Batch: 200 Log-loss: 0.03146898373961449\n",
      "Epoch: 74 Batch: 240 Log-loss: 0.046034153550863266\n",
      "Epoch: 74 Batch: 280 Log-loss: 0.03141225129365921\n",
      "Epoch: 74 Batch: 320 Log-loss: 0.05631755292415619\n",
      "Epoch: 74 Batch: 360 Log-loss: 0.03816406801342964\n",
      "Epoch: 74 Batch: 400 Log-loss: 0.04502221569418907\n",
      "Epoch: 74 Batch: 440 Log-loss: 0.034768637269735336\n",
      "Epoch: 74 Batch: 480 Log-loss: 0.04774417355656624\n",
      "Epoch: 74 Batch: 520 Log-loss: 0.03438900038599968\n",
      "Epoch: 74 Batch: 560 Log-loss: 0.03849048912525177\n",
      "Epoch average log-loss: 0.03836167841551027\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 74, val_loss: 0.03997769121831931, best_val_loss: 0.03997769121831931, best_auc: 0.988435859517228\n",
      "Epoch: 75 Batch: 0 Log-loss: 0.03279002010822296\n",
      "Epoch: 75 Batch: 40 Log-loss: 0.04515042528510094\n",
      "Epoch: 75 Batch: 80 Log-loss: 0.04918736591935158\n",
      "Epoch: 75 Batch: 120 Log-loss: 0.044074252247810364\n",
      "Epoch: 75 Batch: 160 Log-loss: 0.04033690690994263\n",
      "Epoch: 75 Batch: 200 Log-loss: 0.045352350920438766\n",
      "Epoch: 75 Batch: 240 Log-loss: 0.03744334354996681\n",
      "Epoch: 75 Batch: 280 Log-loss: 0.0319269523024559\n",
      "Epoch: 75 Batch: 320 Log-loss: 0.032981086522340775\n",
      "Epoch: 75 Batch: 360 Log-loss: 0.0680992528796196\n",
      "Epoch: 75 Batch: 400 Log-loss: 0.04464029148221016\n",
      "Epoch: 75 Batch: 440 Log-loss: 0.055847447365522385\n",
      "Epoch: 75 Batch: 480 Log-loss: 0.03345457836985588\n",
      "Epoch: 75 Batch: 520 Log-loss: 0.029905224218964577\n",
      "Epoch: 75 Batch: 560 Log-loss: 0.037681106477975845\n",
      "Epoch average log-loss: 0.03842867858475074\n",
      "In Epoch: 75, val_loss: 0.0404988766651736, best_val_loss: 0.03997769121831931, best_auc: 0.988435859517228\n",
      "Epoch: 76 Batch: 0 Log-loss: 0.03747574985027313\n",
      "Epoch: 76 Batch: 40 Log-loss: 0.03008960373699665\n",
      "Epoch: 76 Batch: 80 Log-loss: 0.03814326599240303\n",
      "Epoch: 76 Batch: 120 Log-loss: 0.037232693284749985\n",
      "Epoch: 76 Batch: 160 Log-loss: 0.04893961548805237\n",
      "Epoch: 76 Batch: 200 Log-loss: 0.04282642528414726\n",
      "Epoch: 76 Batch: 240 Log-loss: 0.023559628054499626\n",
      "Epoch: 76 Batch: 280 Log-loss: 0.03746994212269783\n",
      "Epoch: 76 Batch: 320 Log-loss: 0.034933432936668396\n",
      "Epoch: 76 Batch: 360 Log-loss: 0.051500897854566574\n",
      "Epoch: 76 Batch: 400 Log-loss: 0.038325000554323196\n",
      "Epoch: 76 Batch: 440 Log-loss: 0.033764202147722244\n",
      "Epoch: 76 Batch: 480 Log-loss: 0.03151431679725647\n",
      "Epoch: 76 Batch: 520 Log-loss: 0.0442609041929245\n",
      "Epoch: 76 Batch: 560 Log-loss: 0.02927045337855816\n",
      "Epoch average log-loss: 0.038372258762163776\n",
      "In Epoch: 76, val_loss: 0.0406868666055047, best_val_loss: 0.03997769121831931, best_auc: 0.988435859517228\n",
      "Epoch: 77 Batch: 0 Log-loss: 0.0332571379840374\n",
      "Epoch: 77 Batch: 40 Log-loss: 0.02975430153310299\n",
      "Epoch: 77 Batch: 80 Log-loss: 0.03644195571541786\n",
      "Epoch: 77 Batch: 120 Log-loss: 0.029931342229247093\n",
      "Epoch: 77 Batch: 160 Log-loss: 0.043497830629348755\n",
      "Epoch: 77 Batch: 200 Log-loss: 0.03236741945147514\n",
      "Epoch: 77 Batch: 240 Log-loss: 0.03429378941655159\n",
      "Epoch: 77 Batch: 280 Log-loss: 0.039160966873168945\n",
      "Epoch: 77 Batch: 320 Log-loss: 0.03357841446995735\n",
      "Epoch: 77 Batch: 360 Log-loss: 0.030968837440013885\n",
      "Epoch: 77 Batch: 400 Log-loss: 0.0453980527818203\n",
      "Epoch: 77 Batch: 440 Log-loss: 0.031757939606904984\n",
      "Epoch: 77 Batch: 480 Log-loss: 0.03520098701119423\n",
      "Epoch: 77 Batch: 520 Log-loss: 0.03872789815068245\n",
      "Epoch: 77 Batch: 560 Log-loss: 0.03442956507205963\n",
      "Epoch average log-loss: 0.03844488167337009\n",
      "In Epoch: 77, val_loss: 0.04026501881981529, best_val_loss: 0.03997769121831931, best_auc: 0.988435859517228\n",
      "Epoch: 78 Batch: 0 Log-loss: 0.030107790604233742\n",
      "Epoch: 78 Batch: 40 Log-loss: 0.043073128908872604\n",
      "Epoch: 78 Batch: 80 Log-loss: 0.04404780641198158\n",
      "Epoch: 78 Batch: 120 Log-loss: 0.04734284058213234\n",
      "Epoch: 78 Batch: 160 Log-loss: 0.046106111258268356\n",
      "Epoch: 78 Batch: 200 Log-loss: 0.027593307197093964\n",
      "Epoch: 78 Batch: 240 Log-loss: 0.034531913697719574\n",
      "Epoch: 78 Batch: 280 Log-loss: 0.02777152694761753\n",
      "Epoch: 78 Batch: 320 Log-loss: 0.02671969123184681\n",
      "Epoch: 78 Batch: 360 Log-loss: 0.039329010993242264\n",
      "Epoch: 78 Batch: 400 Log-loss: 0.034400034695863724\n",
      "Epoch: 78 Batch: 440 Log-loss: 0.03242474049329758\n",
      "Epoch: 78 Batch: 480 Log-loss: 0.044710759073495865\n",
      "Epoch: 78 Batch: 520 Log-loss: 0.03913820534944534\n",
      "Epoch: 78 Batch: 560 Log-loss: 0.03693534433841705\n",
      "Epoch average log-loss: 0.038474658876657485\n",
      "In Epoch: 78, val_loss: 0.040510154461335615, best_val_loss: 0.03997769121831931, best_auc: 0.988435859517228\n",
      "Epoch: 79 Batch: 0 Log-loss: 0.0330592505633831\n",
      "Epoch: 79 Batch: 40 Log-loss: 0.02679554931819439\n",
      "Epoch: 79 Batch: 80 Log-loss: 0.030983708798885345\n",
      "Epoch: 79 Batch: 120 Log-loss: 0.03556433320045471\n",
      "Epoch: 79 Batch: 160 Log-loss: 0.0391538105905056\n",
      "Epoch: 79 Batch: 200 Log-loss: 0.03321607783436775\n",
      "Epoch: 79 Batch: 240 Log-loss: 0.02844497747719288\n",
      "Epoch: 79 Batch: 280 Log-loss: 0.0353357158601284\n",
      "Epoch: 79 Batch: 320 Log-loss: 0.03809383511543274\n",
      "Epoch: 79 Batch: 360 Log-loss: 0.03108600527048111\n",
      "Epoch: 79 Batch: 400 Log-loss: 0.03572681173682213\n",
      "Epoch: 79 Batch: 440 Log-loss: 0.019141340628266335\n",
      "Epoch: 79 Batch: 480 Log-loss: 0.03670784458518028\n",
      "Epoch: 79 Batch: 520 Log-loss: 0.061439450830221176\n",
      "Epoch: 79 Batch: 560 Log-loss: 0.042842015624046326\n",
      "Epoch average log-loss: 0.03838732887358804\n",
      "In Epoch: 79, val_loss: 0.040448276306180586, best_val_loss: 0.03997769121831931, best_auc: 0.988435859517228\n",
      "Epoch: 80 Batch: 0 Log-loss: 0.05456290766596794\n",
      "Epoch: 80 Batch: 40 Log-loss: 0.031466517597436905\n",
      "Epoch: 80 Batch: 80 Log-loss: 0.027012072503566742\n",
      "Epoch: 80 Batch: 120 Log-loss: 0.02989868074655533\n",
      "Epoch: 80 Batch: 160 Log-loss: 0.043216217309236526\n",
      "Epoch: 80 Batch: 200 Log-loss: 0.03563957288861275\n",
      "Epoch: 80 Batch: 240 Log-loss: 0.03780065104365349\n",
      "Epoch: 80 Batch: 280 Log-loss: 0.03606187924742699\n",
      "Epoch: 80 Batch: 320 Log-loss: 0.030272556468844414\n",
      "Epoch: 80 Batch: 360 Log-loss: 0.04699985682964325\n",
      "Epoch: 80 Batch: 400 Log-loss: 0.06333606690168381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80 Batch: 440 Log-loss: 0.035078734159469604\n",
      "Epoch: 80 Batch: 480 Log-loss: 0.03405631706118584\n",
      "Epoch: 80 Batch: 520 Log-loss: 0.025001270696520805\n",
      "Epoch: 80 Batch: 560 Log-loss: 0.05431564897298813\n",
      "Epoch average log-loss: 0.038385085548673356\n",
      "In Epoch: 80, val_loss: 0.04040217150289287, best_val_loss: 0.03997769121831931, best_auc: 0.988435859517228\n",
      "Epoch: 81 Batch: 0 Log-loss: 0.03548869118094444\n",
      "Epoch: 81 Batch: 40 Log-loss: 0.03781747445464134\n",
      "Epoch: 81 Batch: 80 Log-loss: 0.049095768481492996\n",
      "Epoch: 81 Batch: 120 Log-loss: 0.059041690081357956\n",
      "Epoch: 81 Batch: 160 Log-loss: 0.05756339803338051\n",
      "Epoch: 81 Batch: 200 Log-loss: 0.04720650985836983\n",
      "Epoch: 81 Batch: 240 Log-loss: 0.0412043072283268\n",
      "Epoch: 81 Batch: 280 Log-loss: 0.041975826025009155\n",
      "Epoch: 81 Batch: 320 Log-loss: 0.030223965644836426\n",
      "Epoch: 81 Batch: 360 Log-loss: 0.0456855408847332\n",
      "Epoch: 81 Batch: 400 Log-loss: 0.044119179248809814\n",
      "Epoch: 81 Batch: 440 Log-loss: 0.03884464129805565\n",
      "Epoch: 81 Batch: 480 Log-loss: 0.04023044928908348\n",
      "Epoch: 81 Batch: 520 Log-loss: 0.04484807327389717\n",
      "Epoch: 81 Batch: 560 Log-loss: 0.030346376821398735\n",
      "Epoch average log-loss: 0.038345737612274076\n",
      "In Epoch: 81, val_loss: 0.04053186665590327, best_val_loss: 0.03997769121831931, best_auc: 0.988435859517228\n",
      "Epoch: 82 Batch: 0 Log-loss: 0.0378275103867054\n",
      "Epoch: 82 Batch: 40 Log-loss: 0.034193966537714005\n",
      "Epoch: 82 Batch: 80 Log-loss: 0.03654943034052849\n",
      "Epoch: 82 Batch: 120 Log-loss: 0.029649658128619194\n",
      "Epoch: 82 Batch: 160 Log-loss: 0.04511344060301781\n",
      "Epoch: 82 Batch: 200 Log-loss: 0.029669946059584618\n",
      "Epoch: 82 Batch: 240 Log-loss: 0.042826827615499496\n",
      "Epoch: 82 Batch: 280 Log-loss: 0.03310472518205643\n",
      "Epoch: 82 Batch: 320 Log-loss: 0.03649165481328964\n",
      "Epoch: 82 Batch: 360 Log-loss: 0.02624196745455265\n",
      "Epoch: 82 Batch: 400 Log-loss: 0.028045572340488434\n",
      "Epoch: 82 Batch: 440 Log-loss: 0.05048072710633278\n",
      "Epoch: 82 Batch: 480 Log-loss: 0.04218896105885506\n",
      "Epoch: 82 Batch: 520 Log-loss: 0.036401551216840744\n",
      "Epoch: 82 Batch: 560 Log-loss: 0.03180448338389397\n",
      "Epoch average log-loss: 0.03832237948996148\n",
      "In Epoch: 82, val_loss: 0.040082421615439436, best_val_loss: 0.03997769121831931, best_auc: 0.988435859517228\n",
      "Epoch: 83 Batch: 0 Log-loss: 0.03700277954339981\n",
      "Epoch: 83 Batch: 40 Log-loss: 0.05030444636940956\n",
      "Epoch: 83 Batch: 80 Log-loss: 0.040147650986909866\n",
      "Epoch: 83 Batch: 120 Log-loss: 0.037745509296655655\n",
      "Epoch: 83 Batch: 160 Log-loss: 0.03563810884952545\n",
      "Epoch: 83 Batch: 200 Log-loss: 0.0450967438519001\n",
      "Epoch: 83 Batch: 240 Log-loss: 0.03407825157046318\n",
      "Epoch: 83 Batch: 280 Log-loss: 0.038301680237054825\n",
      "Epoch: 83 Batch: 320 Log-loss: 0.04502459987998009\n",
      "Epoch: 83 Batch: 360 Log-loss: 0.030262812972068787\n",
      "Epoch: 83 Batch: 400 Log-loss: 0.039410728961229324\n",
      "Epoch: 83 Batch: 440 Log-loss: 0.0318109467625618\n",
      "Epoch: 83 Batch: 480 Log-loss: 0.03082110546529293\n",
      "Epoch: 83 Batch: 520 Log-loss: 0.03294101357460022\n",
      "Epoch: 83 Batch: 560 Log-loss: 0.041520919650793076\n",
      "Epoch average log-loss: 0.0385055088205263\n",
      "In Epoch: 83, val_loss: 0.04006923738009794, best_val_loss: 0.03997769121831931, best_auc: 0.988435859517228\n",
      "Epoch: 84 Batch: 0 Log-loss: 0.0338171124458313\n",
      "Epoch: 84 Batch: 40 Log-loss: 0.03694893792271614\n",
      "Epoch: 84 Batch: 80 Log-loss: 0.04873419925570488\n",
      "Epoch: 84 Batch: 120 Log-loss: 0.03636886179447174\n",
      "Epoch: 84 Batch: 160 Log-loss: 0.052444249391555786\n",
      "Epoch: 84 Batch: 200 Log-loss: 0.05039405822753906\n",
      "Epoch: 84 Batch: 240 Log-loss: 0.030877158045768738\n",
      "Epoch: 84 Batch: 280 Log-loss: 0.04460359737277031\n",
      "Epoch: 84 Batch: 320 Log-loss: 0.025649547576904297\n",
      "Epoch: 84 Batch: 360 Log-loss: 0.028196698054671288\n",
      "Epoch: 84 Batch: 400 Log-loss: 0.03390531241893768\n",
      "Epoch: 84 Batch: 440 Log-loss: 0.029414353892207146\n",
      "Epoch: 84 Batch: 480 Log-loss: 0.03497990593314171\n",
      "Epoch: 84 Batch: 520 Log-loss: 0.029961779713630676\n",
      "Epoch: 84 Batch: 560 Log-loss: 0.029071442782878876\n",
      "Epoch average log-loss: 0.03844054246188274\n",
      "In Epoch: 84, val_loss: 0.04031308177559945, best_val_loss: 0.03997769121831931, best_auc: 0.988435859517228\n",
      "Epoch: 85 Batch: 0 Log-loss: 0.04039442911744118\n",
      "Epoch: 85 Batch: 40 Log-loss: 0.04567998647689819\n",
      "Epoch: 85 Batch: 80 Log-loss: 0.04301876947283745\n",
      "Epoch: 85 Batch: 120 Log-loss: 0.05854058265686035\n",
      "Epoch: 85 Batch: 160 Log-loss: 0.049059975892305374\n",
      "Epoch: 85 Batch: 200 Log-loss: 0.029058508574962616\n",
      "Epoch: 85 Batch: 240 Log-loss: 0.03151845559477806\n",
      "Epoch: 85 Batch: 280 Log-loss: 0.037248607724905014\n",
      "Epoch: 85 Batch: 320 Log-loss: 0.03681477531790733\n",
      "Epoch: 85 Batch: 360 Log-loss: 0.033543333411216736\n",
      "Epoch: 85 Batch: 400 Log-loss: 0.0424077995121479\n",
      "Epoch: 85 Batch: 440 Log-loss: 0.03605607524514198\n",
      "Epoch: 85 Batch: 480 Log-loss: 0.027112117037177086\n",
      "Epoch: 85 Batch: 520 Log-loss: 0.039353687316179276\n",
      "Epoch: 85 Batch: 560 Log-loss: 0.05928229168057442\n",
      "Epoch average log-loss: 0.038520081025282184\n",
      "In Epoch: 85, val_loss: 0.040307251928041195, best_val_loss: 0.03997769121831931, best_auc: 0.988435859517228\n",
      "Epoch: 86 Batch: 0 Log-loss: 0.04370725154876709\n",
      "Epoch: 86 Batch: 40 Log-loss: 0.045458998531103134\n",
      "Epoch: 86 Batch: 80 Log-loss: 0.036503877490758896\n",
      "Epoch: 86 Batch: 120 Log-loss: 0.0379565991461277\n",
      "Epoch: 86 Batch: 160 Log-loss: 0.03959536924958229\n",
      "Epoch: 86 Batch: 200 Log-loss: 0.04025217518210411\n",
      "Epoch: 86 Batch: 240 Log-loss: 0.038825202733278275\n",
      "Epoch: 86 Batch: 280 Log-loss: 0.03782576322555542\n",
      "Epoch: 86 Batch: 320 Log-loss: 0.04004940018057823\n",
      "Epoch: 86 Batch: 360 Log-loss: 0.04654712975025177\n",
      "Epoch: 86 Batch: 400 Log-loss: 0.04346137121319771\n",
      "Epoch: 86 Batch: 440 Log-loss: 0.05679960176348686\n",
      "Epoch: 86 Batch: 480 Log-loss: 0.06296655535697937\n",
      "Epoch: 86 Batch: 520 Log-loss: 0.04227479174733162\n",
      "Epoch: 86 Batch: 560 Log-loss: 0.022732000797986984\n",
      "Epoch average log-loss: 0.03828737111096936\n",
      "A pre-trained model at model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt has been loaded.\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn6.pt.\n",
      "\n",
      "Choose the torch base model.\n",
      "## Training on fold 7 ##\n",
      "Epoch: 1 Batch: 0 Log-loss: 0.6961352825164795\n",
      "Epoch: 1 Batch: 40 Log-loss: 0.16760046780109406\n",
      "Epoch: 1 Batch: 80 Log-loss: 0.13744638860225677\n",
      "Epoch: 1 Batch: 120 Log-loss: 0.08707606792449951\n",
      "Epoch: 1 Batch: 160 Log-loss: 0.07678434997797012\n",
      "Epoch: 1 Batch: 200 Log-loss: 0.07423495501279831\n",
      "Epoch: 1 Batch: 240 Log-loss: 0.04153409227728844\n",
      "Epoch: 1 Batch: 280 Log-loss: 0.06888309121131897\n",
      "Epoch: 1 Batch: 320 Log-loss: 0.03131278604269028\n",
      "Epoch: 1 Batch: 360 Log-loss: 0.06942328810691833\n",
      "Epoch: 1 Batch: 400 Log-loss: 0.04186024144291878\n",
      "Epoch: 1 Batch: 440 Log-loss: 0.0586048848927021\n",
      "Epoch: 1 Batch: 480 Log-loss: 0.045400943607091904\n",
      "Epoch: 1 Batch: 520 Log-loss: 0.059491485357284546\n",
      "Epoch: 1 Batch: 560 Log-loss: 0.048640068620443344\n",
      "Epoch average log-loss: 0.08723161458037794\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 1, val_loss: 0.05073384010894901, best_val_loss: 0.05073384010894901, best_auc: 0.9714793302752764\n",
      "Epoch: 2 Batch: 0 Log-loss: 0.05396575108170509\n",
      "Epoch: 2 Batch: 40 Log-loss: 0.06637658923864365\n",
      "Epoch: 2 Batch: 80 Log-loss: 0.05933601036667824\n",
      "Epoch: 2 Batch: 120 Log-loss: 0.05643831565976143\n",
      "Epoch: 2 Batch: 160 Log-loss: 0.043018683791160583\n",
      "Epoch: 2 Batch: 200 Log-loss: 0.07082519680261612\n",
      "Epoch: 2 Batch: 240 Log-loss: 0.05551956593990326\n",
      "Epoch: 2 Batch: 280 Log-loss: 0.0520760677754879\n",
      "Epoch: 2 Batch: 320 Log-loss: 0.05527384579181671\n",
      "Epoch: 2 Batch: 360 Log-loss: 0.03999977186322212\n",
      "Epoch: 2 Batch: 400 Log-loss: 0.05091620981693268\n",
      "Epoch: 2 Batch: 440 Log-loss: 0.06060430780053139\n",
      "Epoch: 2 Batch: 480 Log-loss: 0.0474821962416172\n",
      "Epoch: 2 Batch: 520 Log-loss: 0.040552519261837006\n",
      "Epoch: 2 Batch: 560 Log-loss: 0.043901413679122925\n",
      "Epoch average log-loss: 0.05118144399686051\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 2, val_loss: 0.047998423383923294, best_val_loss: 0.047998423383923294, best_auc: 0.976008408114192\n",
      "Epoch: 3 Batch: 0 Log-loss: 0.04764839634299278\n",
      "Epoch: 3 Batch: 40 Log-loss: 0.04457537457346916\n",
      "Epoch: 3 Batch: 80 Log-loss: 0.0719127207994461\n",
      "Epoch: 3 Batch: 120 Log-loss: 0.053904589265584946\n",
      "Epoch: 3 Batch: 160 Log-loss: 0.04199786111712456\n",
      "Epoch: 3 Batch: 200 Log-loss: 0.04670020565390587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Batch: 240 Log-loss: 0.050572603940963745\n",
      "Epoch: 3 Batch: 280 Log-loss: 0.055014465004205704\n",
      "Epoch: 3 Batch: 320 Log-loss: 0.06919942796230316\n",
      "Epoch: 3 Batch: 360 Log-loss: 0.054632220417261124\n",
      "Epoch: 3 Batch: 400 Log-loss: 0.04156980291008949\n",
      "Epoch: 3 Batch: 440 Log-loss: 0.03298483416438103\n",
      "Epoch: 3 Batch: 480 Log-loss: 0.06467358022928238\n",
      "Epoch: 3 Batch: 520 Log-loss: 0.04641348496079445\n",
      "Epoch: 3 Batch: 560 Log-loss: 0.029278375208377838\n",
      "Epoch average log-loss: 0.04843877702618816\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 3, val_loss: 0.04614800242663497, best_val_loss: 0.04614800242663497, best_auc: 0.9797320860707441\n",
      "Epoch: 4 Batch: 0 Log-loss: 0.031952887773513794\n",
      "Epoch: 4 Batch: 40 Log-loss: 0.040962833911180496\n",
      "Epoch: 4 Batch: 80 Log-loss: 0.04062600061297417\n",
      "Epoch: 4 Batch: 120 Log-loss: 0.044870760291814804\n",
      "Epoch: 4 Batch: 160 Log-loss: 0.043198361992836\n",
      "Epoch: 4 Batch: 200 Log-loss: 0.04755878075957298\n",
      "Epoch: 4 Batch: 240 Log-loss: 0.0672004297375679\n",
      "Epoch: 4 Batch: 280 Log-loss: 0.04295644536614418\n",
      "Epoch: 4 Batch: 320 Log-loss: 0.049017537385225296\n",
      "Epoch: 4 Batch: 360 Log-loss: 0.03560413420200348\n",
      "Epoch: 4 Batch: 400 Log-loss: 0.05136323347687721\n",
      "Epoch: 4 Batch: 440 Log-loss: 0.036876436322927475\n",
      "Epoch: 4 Batch: 480 Log-loss: 0.031588200479745865\n",
      "Epoch: 4 Batch: 520 Log-loss: 0.03813009709119797\n",
      "Epoch: 4 Batch: 560 Log-loss: 0.0635858103632927\n",
      "Epoch average log-loss: 0.04649020603059658\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 4, val_loss: 0.04450848001707736, best_val_loss: 0.04450848001707736, best_auc: 0.9837450430652352\n",
      "Epoch: 5 Batch: 0 Log-loss: 0.054429326206445694\n",
      "Epoch: 5 Batch: 40 Log-loss: 0.05221004784107208\n",
      "Epoch: 5 Batch: 80 Log-loss: 0.05546962097287178\n",
      "Epoch: 5 Batch: 120 Log-loss: 0.04100804775953293\n",
      "Epoch: 5 Batch: 160 Log-loss: 0.03901372104883194\n",
      "Epoch: 5 Batch: 200 Log-loss: 0.04098518565297127\n",
      "Epoch: 5 Batch: 240 Log-loss: 0.058962803333997726\n",
      "Epoch: 5 Batch: 280 Log-loss: 0.02808900736272335\n",
      "Epoch: 5 Batch: 320 Log-loss: 0.06255616992712021\n",
      "Epoch: 5 Batch: 360 Log-loss: 0.05187152326107025\n",
      "Epoch: 5 Batch: 400 Log-loss: 0.04437297582626343\n",
      "Epoch: 5 Batch: 440 Log-loss: 0.03822629526257515\n",
      "Epoch: 5 Batch: 480 Log-loss: 0.050562191754579544\n",
      "Epoch: 5 Batch: 520 Log-loss: 0.04658554866909981\n",
      "Epoch: 5 Batch: 560 Log-loss: 0.0380387008190155\n",
      "Epoch average log-loss: 0.04563007295863437\n",
      "In Epoch: 5, val_loss: 0.04457913976667249, best_val_loss: 0.04450848001707736, best_auc: 0.9837450430652352\n",
      "Epoch: 6 Batch: 0 Log-loss: 0.035246167331933975\n",
      "Epoch: 6 Batch: 40 Log-loss: 0.036720048636198044\n",
      "Epoch: 6 Batch: 80 Log-loss: 0.049153607338666916\n",
      "Epoch: 6 Batch: 120 Log-loss: 0.045428451150655746\n",
      "Epoch: 6 Batch: 160 Log-loss: 0.05574891343712807\n",
      "Epoch: 6 Batch: 200 Log-loss: 0.055414363741874695\n",
      "Epoch: 6 Batch: 240 Log-loss: 0.04603654518723488\n",
      "Epoch: 6 Batch: 280 Log-loss: 0.04065783694386482\n",
      "Epoch: 6 Batch: 320 Log-loss: 0.03573063760995865\n",
      "Epoch: 6 Batch: 360 Log-loss: 0.03160461410880089\n",
      "Epoch: 6 Batch: 400 Log-loss: 0.04068177193403244\n",
      "Epoch: 6 Batch: 440 Log-loss: 0.04825611039996147\n",
      "Epoch: 6 Batch: 480 Log-loss: 0.04885686933994293\n",
      "Epoch: 6 Batch: 520 Log-loss: 0.04887963831424713\n",
      "Epoch: 6 Batch: 560 Log-loss: 0.03976523131132126\n",
      "Epoch average log-loss: 0.044965708332269316\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 6, val_loss: 0.04360340402021546, best_val_loss: 0.04360340402021546, best_auc: 0.9839649345938883\n",
      "Epoch: 7 Batch: 0 Log-loss: 0.04405435919761658\n",
      "Epoch: 7 Batch: 40 Log-loss: 0.040136560797691345\n",
      "Epoch: 7 Batch: 80 Log-loss: 0.030491778627038002\n",
      "Epoch: 7 Batch: 120 Log-loss: 0.03859294578433037\n",
      "Epoch: 7 Batch: 160 Log-loss: 0.03881356492638588\n",
      "Epoch: 7 Batch: 200 Log-loss: 0.04498680308461189\n",
      "Epoch: 7 Batch: 240 Log-loss: 0.04387456178665161\n",
      "Epoch: 7 Batch: 280 Log-loss: 0.03418845310807228\n",
      "Epoch: 7 Batch: 320 Log-loss: 0.044818952679634094\n",
      "Epoch: 7 Batch: 360 Log-loss: 0.041148796677589417\n",
      "Epoch: 7 Batch: 400 Log-loss: 0.036333151161670685\n",
      "Epoch: 7 Batch: 440 Log-loss: 0.06226929649710655\n",
      "Epoch: 7 Batch: 480 Log-loss: 0.03963640704751015\n",
      "Epoch: 7 Batch: 520 Log-loss: 0.037699244916439056\n",
      "Epoch: 7 Batch: 560 Log-loss: 0.03676620125770569\n",
      "Epoch average log-loss: 0.04414782161558313\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 7, val_loss: 0.04292663814998674, best_val_loss: 0.04292663814998674, best_auc: 0.9856326923926421\n",
      "Epoch: 8 Batch: 0 Log-loss: 0.054562654346227646\n",
      "Epoch: 8 Batch: 40 Log-loss: 0.04962858185172081\n",
      "Epoch: 8 Batch: 80 Log-loss: 0.02929753065109253\n",
      "Epoch: 8 Batch: 120 Log-loss: 0.048931851983070374\n",
      "Epoch: 8 Batch: 160 Log-loss: 0.037631627172231674\n",
      "Epoch: 8 Batch: 200 Log-loss: 0.040302474051713943\n",
      "Epoch: 8 Batch: 240 Log-loss: 0.051665011793375015\n",
      "Epoch: 8 Batch: 280 Log-loss: 0.04476289451122284\n",
      "Epoch: 8 Batch: 320 Log-loss: 0.04025690630078316\n",
      "Epoch: 8 Batch: 360 Log-loss: 0.06050046905875206\n",
      "Epoch: 8 Batch: 400 Log-loss: 0.03264882043004036\n",
      "Epoch: 8 Batch: 440 Log-loss: 0.04990507289767265\n",
      "Epoch: 8 Batch: 480 Log-loss: 0.04159935563802719\n",
      "Epoch: 8 Batch: 520 Log-loss: 0.05184181407094002\n",
      "Epoch: 8 Batch: 560 Log-loss: 0.04612213373184204\n",
      "Epoch average log-loss: 0.043488736131361554\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 8, val_loss: 0.0426346080539503, best_val_loss: 0.0426346080539503, best_auc: 0.9863773513346051\n",
      "Epoch: 9 Batch: 0 Log-loss: 0.04800887033343315\n",
      "Epoch: 9 Batch: 40 Log-loss: 0.059747714549303055\n",
      "Epoch: 9 Batch: 80 Log-loss: 0.030439769849181175\n",
      "Epoch: 9 Batch: 120 Log-loss: 0.042643722146749496\n",
      "Epoch: 9 Batch: 160 Log-loss: 0.03837611526250839\n",
      "Epoch: 9 Batch: 200 Log-loss: 0.038569774478673935\n",
      "Epoch: 9 Batch: 240 Log-loss: 0.04650978371500969\n",
      "Epoch: 9 Batch: 280 Log-loss: 0.0399567075073719\n",
      "Epoch: 9 Batch: 320 Log-loss: 0.034860897809267044\n",
      "Epoch: 9 Batch: 360 Log-loss: 0.046382855623960495\n",
      "Epoch: 9 Batch: 400 Log-loss: 0.031179672107100487\n",
      "Epoch: 9 Batch: 440 Log-loss: 0.048149485141038895\n",
      "Epoch: 9 Batch: 480 Log-loss: 0.04720883071422577\n",
      "Epoch: 9 Batch: 520 Log-loss: 0.043193962424993515\n",
      "Epoch: 9 Batch: 560 Log-loss: 0.06333737820386887\n",
      "Epoch average log-loss: 0.04305865041512464\n",
      "In Epoch: 9, val_loss: 0.042837045152397994, best_val_loss: 0.0426346080539503, best_auc: 0.9863773513346051\n",
      "Epoch: 10 Batch: 0 Log-loss: 0.05098230019211769\n",
      "Epoch: 10 Batch: 40 Log-loss: 0.050091881304979324\n",
      "Epoch: 10 Batch: 80 Log-loss: 0.046381786465644836\n",
      "Epoch: 10 Batch: 120 Log-loss: 0.03877799212932587\n",
      "Epoch: 10 Batch: 160 Log-loss: 0.04523928835988045\n",
      "Epoch: 10 Batch: 200 Log-loss: 0.030718861147761345\n",
      "Epoch: 10 Batch: 240 Log-loss: 0.040292732417583466\n",
      "Epoch: 10 Batch: 280 Log-loss: 0.04949210584163666\n",
      "Epoch: 10 Batch: 320 Log-loss: 0.06132734194397926\n",
      "Epoch: 10 Batch: 360 Log-loss: 0.04756108298897743\n",
      "Epoch: 10 Batch: 400 Log-loss: 0.038246359676122665\n",
      "Epoch: 10 Batch: 440 Log-loss: 0.05072445049881935\n",
      "Epoch: 10 Batch: 480 Log-loss: 0.057025253772735596\n",
      "Epoch: 10 Batch: 520 Log-loss: 0.06057155132293701\n",
      "Epoch: 10 Batch: 560 Log-loss: 0.04510650411248207\n",
      "Epoch average log-loss: 0.042477523758342225\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 10, val_loss: 0.04224544904938834, best_val_loss: 0.04224544904938834, best_auc: 0.9863885012355368\n",
      "Epoch: 11 Batch: 0 Log-loss: 0.039859309792518616\n",
      "Epoch: 11 Batch: 40 Log-loss: 0.04148876294493675\n",
      "Epoch: 11 Batch: 80 Log-loss: 0.0323740690946579\n",
      "Epoch: 11 Batch: 120 Log-loss: 0.04280770197510719\n",
      "Epoch: 11 Batch: 160 Log-loss: 0.03519059345126152\n",
      "Epoch: 11 Batch: 200 Log-loss: 0.04014738276600838\n",
      "Epoch: 11 Batch: 240 Log-loss: 0.03749905154109001\n",
      "Epoch: 11 Batch: 280 Log-loss: 0.033477868884801865\n",
      "Epoch: 11 Batch: 320 Log-loss: 0.04025517776608467\n",
      "Epoch: 11 Batch: 360 Log-loss: 0.038798194378614426\n",
      "Epoch: 11 Batch: 400 Log-loss: 0.042947351932525635\n",
      "Epoch: 11 Batch: 440 Log-loss: 0.04049494117498398\n",
      "Epoch: 11 Batch: 480 Log-loss: 0.03201575577259064\n",
      "Epoch: 11 Batch: 520 Log-loss: 0.04713694751262665\n",
      "Epoch: 11 Batch: 560 Log-loss: 0.05120750889182091\n",
      "Epoch average log-loss: 0.04212063408589789\n",
      "In Epoch: 11, val_loss: 0.04226893722726976, best_val_loss: 0.04224544904938834, best_auc: 0.9863885012355368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 Batch: 0 Log-loss: 0.022738443687558174\n",
      "Epoch: 12 Batch: 40 Log-loss: 0.0653163343667984\n",
      "Epoch: 12 Batch: 80 Log-loss: 0.04289734363555908\n",
      "Epoch: 12 Batch: 120 Log-loss: 0.04228740930557251\n",
      "Epoch: 12 Batch: 160 Log-loss: 0.042702335864305496\n",
      "Epoch: 12 Batch: 200 Log-loss: 0.03556590899825096\n",
      "Epoch: 12 Batch: 240 Log-loss: 0.04150617495179176\n",
      "Epoch: 12 Batch: 280 Log-loss: 0.037033479660749435\n",
      "Epoch: 12 Batch: 320 Log-loss: 0.02882070280611515\n",
      "Epoch: 12 Batch: 360 Log-loss: 0.0343131385743618\n",
      "Epoch: 12 Batch: 400 Log-loss: 0.03597128763794899\n",
      "Epoch: 12 Batch: 440 Log-loss: 0.03851523995399475\n",
      "Epoch: 12 Batch: 480 Log-loss: 0.021456904709339142\n",
      "Epoch: 12 Batch: 520 Log-loss: 0.06273675709962845\n",
      "Epoch: 12 Batch: 560 Log-loss: 0.046096064150333405\n",
      "Epoch average log-loss: 0.04211072962997215\n",
      "In Epoch: 12, val_loss: 0.04309883768748272, best_val_loss: 0.04224544904938834, best_auc: 0.9863885012355368\n",
      "Epoch: 13 Batch: 0 Log-loss: 0.06609100848436356\n",
      "Epoch: 13 Batch: 40 Log-loss: 0.04018435999751091\n",
      "Epoch: 13 Batch: 80 Log-loss: 0.02356383018195629\n",
      "Epoch: 13 Batch: 120 Log-loss: 0.03548315167427063\n",
      "Epoch: 13 Batch: 160 Log-loss: 0.039396628737449646\n",
      "Epoch: 13 Batch: 200 Log-loss: 0.05511857569217682\n",
      "Epoch: 13 Batch: 240 Log-loss: 0.026087114587426186\n",
      "Epoch: 13 Batch: 280 Log-loss: 0.053784776479005814\n",
      "Epoch: 13 Batch: 320 Log-loss: 0.031183311715722084\n",
      "Epoch: 13 Batch: 360 Log-loss: 0.04724046215415001\n",
      "Epoch: 13 Batch: 400 Log-loss: 0.030258268117904663\n",
      "Epoch: 13 Batch: 440 Log-loss: 0.030617622658610344\n",
      "Epoch: 13 Batch: 480 Log-loss: 0.03479035571217537\n",
      "Epoch: 13 Batch: 520 Log-loss: 0.05436426401138306\n",
      "Epoch: 13 Batch: 560 Log-loss: 0.04257892817258835\n",
      "Epoch average log-loss: 0.04173683707070138\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 13, val_loss: 0.04222403519447978, best_val_loss: 0.04222403519447978, best_auc: 0.9862019738017809\n",
      "Epoch: 14 Batch: 0 Log-loss: 0.028608374297618866\n",
      "Epoch: 14 Batch: 40 Log-loss: 0.04991190508008003\n",
      "Epoch: 14 Batch: 80 Log-loss: 0.03717175871133804\n",
      "Epoch: 14 Batch: 120 Log-loss: 0.04682673141360283\n",
      "Epoch: 14 Batch: 160 Log-loss: 0.037072937935590744\n",
      "Epoch: 14 Batch: 200 Log-loss: 0.03834139555692673\n",
      "Epoch: 14 Batch: 240 Log-loss: 0.048778656870126724\n",
      "Epoch: 14 Batch: 280 Log-loss: 0.04298713430762291\n",
      "Epoch: 14 Batch: 320 Log-loss: 0.05106749013066292\n",
      "Epoch: 14 Batch: 360 Log-loss: 0.030489712953567505\n",
      "Epoch: 14 Batch: 400 Log-loss: 0.042627740651369095\n",
      "Epoch: 14 Batch: 440 Log-loss: 0.036396730691194534\n",
      "Epoch: 14 Batch: 480 Log-loss: 0.028322355821728706\n",
      "Epoch: 14 Batch: 520 Log-loss: 0.060076430439949036\n",
      "Epoch: 14 Batch: 560 Log-loss: 0.041367996484041214\n",
      "Epoch average log-loss: 0.041187453822099734\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 14, val_loss: 0.042180647586795676, best_val_loss: 0.042180647586795676, best_auc: 0.9870210285743316\n",
      "Epoch: 15 Batch: 0 Log-loss: 0.0460163913667202\n",
      "Epoch: 15 Batch: 40 Log-loss: 0.035337645560503006\n",
      "Epoch: 15 Batch: 80 Log-loss: 0.03662363812327385\n",
      "Epoch: 15 Batch: 120 Log-loss: 0.03436577692627907\n",
      "Epoch: 15 Batch: 160 Log-loss: 0.05784536898136139\n",
      "Epoch: 15 Batch: 200 Log-loss: 0.03565321862697601\n",
      "Epoch: 15 Batch: 240 Log-loss: 0.04534222185611725\n",
      "Epoch: 15 Batch: 280 Log-loss: 0.04421444237232208\n",
      "Epoch: 15 Batch: 320 Log-loss: 0.03946889564394951\n",
      "Epoch: 15 Batch: 360 Log-loss: 0.05374268814921379\n",
      "Epoch: 15 Batch: 400 Log-loss: 0.04263518750667572\n",
      "Epoch: 15 Batch: 440 Log-loss: 0.035701680928468704\n",
      "Epoch: 15 Batch: 480 Log-loss: 0.024805597960948944\n",
      "Epoch: 15 Batch: 520 Log-loss: 0.0538640171289444\n",
      "Epoch: 15 Batch: 560 Log-loss: 0.04973054304718971\n",
      "Epoch average log-loss: 0.041383909793304544\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 15, val_loss: 0.042061948318937094, best_val_loss: 0.042061948318937094, best_auc: 0.9869919950077563\n",
      "Epoch: 16 Batch: 0 Log-loss: 0.041663479059934616\n",
      "Epoch: 16 Batch: 40 Log-loss: 0.03588403761386871\n",
      "Epoch: 16 Batch: 80 Log-loss: 0.019449956715106964\n",
      "Epoch: 16 Batch: 120 Log-loss: 0.03471038118004799\n",
      "Epoch: 16 Batch: 160 Log-loss: 0.04506179317831993\n",
      "Epoch: 16 Batch: 200 Log-loss: 0.03809070959687233\n",
      "Epoch: 16 Batch: 240 Log-loss: 0.027465330436825752\n",
      "Epoch: 16 Batch: 280 Log-loss: 0.05218764767050743\n",
      "Epoch: 16 Batch: 320 Log-loss: 0.04166926443576813\n",
      "Epoch: 16 Batch: 360 Log-loss: 0.03377653285861015\n",
      "Epoch: 16 Batch: 400 Log-loss: 0.0266072079539299\n",
      "Epoch: 16 Batch: 440 Log-loss: 0.028686081990599632\n",
      "Epoch: 16 Batch: 480 Log-loss: 0.029527263715863228\n",
      "Epoch: 16 Batch: 520 Log-loss: 0.03763292729854584\n",
      "Epoch: 16 Batch: 560 Log-loss: 0.05933660641312599\n",
      "Epoch average log-loss: 0.040970391766833404\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 16, val_loss: 0.041994783015547106, best_val_loss: 0.041994783015547106, best_auc: 0.9867677982684436\n",
      "Epoch: 17 Batch: 0 Log-loss: 0.046967554837465286\n",
      "Epoch: 17 Batch: 40 Log-loss: 0.023223526775836945\n",
      "Epoch: 17 Batch: 80 Log-loss: 0.05553651973605156\n",
      "Epoch: 17 Batch: 120 Log-loss: 0.035124730318784714\n",
      "Epoch: 17 Batch: 160 Log-loss: 0.03248561918735504\n",
      "Epoch: 17 Batch: 200 Log-loss: 0.041867583990097046\n",
      "Epoch: 17 Batch: 240 Log-loss: 0.047795820981264114\n",
      "Epoch: 17 Batch: 280 Log-loss: 0.034066975116729736\n",
      "Epoch: 17 Batch: 320 Log-loss: 0.04440809413790703\n",
      "Epoch: 17 Batch: 360 Log-loss: 0.05201183632016182\n",
      "Epoch: 17 Batch: 400 Log-loss: 0.05212761089205742\n",
      "Epoch: 17 Batch: 440 Log-loss: 0.03652311488986015\n",
      "Epoch: 17 Batch: 480 Log-loss: 0.04587985575199127\n",
      "Epoch: 17 Batch: 520 Log-loss: 0.034536853432655334\n",
      "Epoch: 17 Batch: 560 Log-loss: 0.0401456393301487\n",
      "Epoch average log-loss: 0.0408363395370543\n",
      "In Epoch: 17, val_loss: 0.04215189031940724, best_val_loss: 0.041994783015547106, best_auc: 0.9867677982684436\n",
      "Epoch: 18 Batch: 0 Log-loss: 0.045962926000356674\n",
      "Epoch: 18 Batch: 40 Log-loss: 0.03381698951125145\n",
      "Epoch: 18 Batch: 80 Log-loss: 0.037027157843112946\n",
      "Epoch: 18 Batch: 120 Log-loss: 0.03661937639117241\n",
      "Epoch: 18 Batch: 160 Log-loss: 0.03414338827133179\n",
      "Epoch: 18 Batch: 200 Log-loss: 0.034439388662576675\n",
      "Epoch: 18 Batch: 240 Log-loss: 0.03955867514014244\n",
      "Epoch: 18 Batch: 280 Log-loss: 0.028862351551651955\n",
      "Epoch: 18 Batch: 320 Log-loss: 0.04781348630785942\n",
      "Epoch: 18 Batch: 360 Log-loss: 0.04806826636195183\n",
      "Epoch: 18 Batch: 400 Log-loss: 0.04168501868844032\n",
      "Epoch: 18 Batch: 440 Log-loss: 0.031076043844223022\n",
      "Epoch: 18 Batch: 480 Log-loss: 0.039442360401153564\n",
      "Epoch: 18 Batch: 520 Log-loss: 0.050707776099443436\n",
      "Epoch: 18 Batch: 560 Log-loss: 0.0415625162422657\n",
      "Epoch average log-loss: 0.040765155752056415\n",
      "In Epoch: 18, val_loss: 0.04208522315277443, best_val_loss: 0.041994783015547106, best_auc: 0.9867677982684436\n",
      "Epoch: 19 Batch: 0 Log-loss: 0.041947681456804276\n",
      "Epoch: 19 Batch: 40 Log-loss: 0.05946837738156319\n",
      "Epoch: 19 Batch: 80 Log-loss: 0.04662869870662689\n",
      "Epoch: 19 Batch: 120 Log-loss: 0.053474605083465576\n",
      "Epoch: 19 Batch: 160 Log-loss: 0.05781194567680359\n",
      "Epoch: 19 Batch: 200 Log-loss: 0.033047113567590714\n",
      "Epoch: 19 Batch: 240 Log-loss: 0.03576859086751938\n",
      "Epoch: 19 Batch: 280 Log-loss: 0.04184413328766823\n",
      "Epoch: 19 Batch: 320 Log-loss: 0.04438889026641846\n",
      "Epoch: 19 Batch: 360 Log-loss: 0.03503629192709923\n",
      "Epoch: 19 Batch: 400 Log-loss: 0.025047563016414642\n",
      "Epoch: 19 Batch: 440 Log-loss: 0.045590054243803024\n",
      "Epoch: 19 Batch: 480 Log-loss: 0.042886268347501755\n",
      "Epoch: 19 Batch: 520 Log-loss: 0.05013459920883179\n",
      "Epoch: 19 Batch: 560 Log-loss: 0.02763286791741848\n",
      "Epoch average log-loss: 0.0403613105482821\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 19, val_loss: 0.04192741301144812, best_val_loss: 0.04192741301144812, best_auc: 0.9877488701787103\n",
      "Epoch: 20 Batch: 0 Log-loss: 0.03884812816977501\n",
      "Epoch: 20 Batch: 40 Log-loss: 0.03734642639756203\n",
      "Epoch: 20 Batch: 80 Log-loss: 0.04284907877445221\n",
      "Epoch: 20 Batch: 120 Log-loss: 0.04749619960784912\n",
      "Epoch: 20 Batch: 160 Log-loss: 0.0407717265188694\n",
      "Epoch: 20 Batch: 200 Log-loss: 0.03790701553225517\n",
      "Epoch: 20 Batch: 240 Log-loss: 0.04945322498679161\n",
      "Epoch: 20 Batch: 280 Log-loss: 0.037662047892808914\n",
      "Epoch: 20 Batch: 320 Log-loss: 0.03727057948708534\n",
      "Epoch: 20 Batch: 360 Log-loss: 0.02911709062755108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 Batch: 400 Log-loss: 0.040024496614933014\n",
      "Epoch: 20 Batch: 440 Log-loss: 0.03727440536022186\n",
      "Epoch: 20 Batch: 480 Log-loss: 0.038656119257211685\n",
      "Epoch: 20 Batch: 520 Log-loss: 0.028499729931354523\n",
      "Epoch: 20 Batch: 560 Log-loss: 0.04096108675003052\n",
      "Epoch average log-loss: 0.04034778165764042\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 20, val_loss: 0.04152159129044746, best_val_loss: 0.04152159129044746, best_auc: 0.9875796939669147\n",
      "Epoch: 21 Batch: 0 Log-loss: 0.030038869008421898\n",
      "Epoch: 21 Batch: 40 Log-loss: 0.04503236711025238\n",
      "Epoch: 21 Batch: 80 Log-loss: 0.0461677610874176\n",
      "Epoch: 21 Batch: 120 Log-loss: 0.025918984785676003\n",
      "Epoch: 21 Batch: 160 Log-loss: 0.051742956042289734\n",
      "Epoch: 21 Batch: 200 Log-loss: 0.04208174720406532\n",
      "Epoch: 21 Batch: 240 Log-loss: 0.034745845943689346\n",
      "Epoch: 21 Batch: 280 Log-loss: 0.03198826685547829\n",
      "Epoch: 21 Batch: 320 Log-loss: 0.03341188654303551\n",
      "Epoch: 21 Batch: 360 Log-loss: 0.0373055525124073\n",
      "Epoch: 21 Batch: 400 Log-loss: 0.01922440528869629\n",
      "Epoch: 21 Batch: 440 Log-loss: 0.03459322452545166\n",
      "Epoch: 21 Batch: 480 Log-loss: 0.05329170823097229\n",
      "Epoch: 21 Batch: 520 Log-loss: 0.032419200986623764\n",
      "Epoch: 21 Batch: 560 Log-loss: 0.047523073852062225\n",
      "Epoch average log-loss: 0.04028242053464055\n",
      "In Epoch: 21, val_loss: 0.04166341550072913, best_val_loss: 0.04152159129044746, best_auc: 0.9875796939669147\n",
      "Epoch: 22 Batch: 0 Log-loss: 0.04253716394305229\n",
      "Epoch: 22 Batch: 40 Log-loss: 0.03325442597270012\n",
      "Epoch: 22 Batch: 80 Log-loss: 0.04269114509224892\n",
      "Epoch: 22 Batch: 120 Log-loss: 0.05086137726902962\n",
      "Epoch: 22 Batch: 160 Log-loss: 0.04915597662329674\n",
      "Epoch: 22 Batch: 200 Log-loss: 0.0528833270072937\n",
      "Epoch: 22 Batch: 240 Log-loss: 0.03746834769845009\n",
      "Epoch: 22 Batch: 280 Log-loss: 0.04772259667515755\n",
      "Epoch: 22 Batch: 320 Log-loss: 0.04623233154416084\n",
      "Epoch: 22 Batch: 360 Log-loss: 0.04611281678080559\n",
      "Epoch: 22 Batch: 400 Log-loss: 0.036654163151979446\n",
      "Epoch: 22 Batch: 440 Log-loss: 0.028067516162991524\n",
      "Epoch: 22 Batch: 480 Log-loss: 0.04058130830526352\n",
      "Epoch: 22 Batch: 520 Log-loss: 0.04264942184090614\n",
      "Epoch: 22 Batch: 560 Log-loss: 0.0395192988216877\n",
      "Epoch average log-loss: 0.04007639306198273\n",
      "In Epoch: 22, val_loss: 0.04223299118227986, best_val_loss: 0.04152159129044746, best_auc: 0.9875796939669147\n",
      "Epoch: 23 Batch: 0 Log-loss: 0.02531595341861248\n",
      "Epoch: 23 Batch: 40 Log-loss: 0.04255661368370056\n",
      "Epoch: 23 Batch: 80 Log-loss: 0.0378141850233078\n",
      "Epoch: 23 Batch: 120 Log-loss: 0.04039572551846504\n",
      "Epoch: 23 Batch: 160 Log-loss: 0.033363763242959976\n",
      "Epoch: 23 Batch: 200 Log-loss: 0.044101815670728683\n",
      "Epoch: 23 Batch: 240 Log-loss: 0.03240978345274925\n",
      "Epoch: 23 Batch: 280 Log-loss: 0.046017006039619446\n",
      "Epoch: 23 Batch: 320 Log-loss: 0.04236902296543121\n",
      "Epoch: 23 Batch: 360 Log-loss: 0.0318506620824337\n",
      "Epoch: 23 Batch: 400 Log-loss: 0.043230194598436356\n",
      "Epoch: 23 Batch: 440 Log-loss: 0.034260720014572144\n",
      "Epoch: 23 Batch: 480 Log-loss: 0.0340057797729969\n",
      "Epoch: 23 Batch: 520 Log-loss: 0.03682595118880272\n",
      "Epoch: 23 Batch: 560 Log-loss: 0.034960903227329254\n",
      "Epoch average log-loss: 0.03979941230167502\n",
      "In Epoch: 23, val_loss: 0.04194340749791306, best_val_loss: 0.04152159129044746, best_auc: 0.9875796939669147\n",
      "Epoch: 24 Batch: 0 Log-loss: 0.04903942346572876\n",
      "Epoch: 24 Batch: 40 Log-loss: 0.05616528168320656\n",
      "Epoch: 24 Batch: 80 Log-loss: 0.043677885085344315\n",
      "Epoch: 24 Batch: 120 Log-loss: 0.03138824179768562\n",
      "Epoch: 24 Batch: 160 Log-loss: 0.040996912866830826\n",
      "Epoch: 24 Batch: 200 Log-loss: 0.03249267116189003\n",
      "Epoch: 24 Batch: 240 Log-loss: 0.044020846486091614\n",
      "Epoch: 24 Batch: 280 Log-loss: 0.029608115553855896\n",
      "Epoch: 24 Batch: 320 Log-loss: 0.044069793075323105\n",
      "Epoch: 24 Batch: 360 Log-loss: 0.03193923830986023\n",
      "Epoch: 24 Batch: 400 Log-loss: 0.026412056758999825\n",
      "Epoch: 24 Batch: 440 Log-loss: 0.03322102129459381\n",
      "Epoch: 24 Batch: 480 Log-loss: 0.041375111788511276\n",
      "Epoch: 24 Batch: 520 Log-loss: 0.03815751150250435\n",
      "Epoch: 24 Batch: 560 Log-loss: 0.03909001871943474\n",
      "Epoch average log-loss: 0.039895285396570605\n",
      "In Epoch: 24, val_loss: 0.04221952233573822, best_val_loss: 0.04152159129044746, best_auc: 0.9875796939669147\n",
      "Epoch: 25 Batch: 0 Log-loss: 0.03635412082076073\n",
      "Epoch: 25 Batch: 40 Log-loss: 0.032915618270635605\n",
      "Epoch: 25 Batch: 80 Log-loss: 0.03942570835351944\n",
      "Epoch: 25 Batch: 120 Log-loss: 0.039929505437612534\n",
      "Epoch: 25 Batch: 160 Log-loss: 0.04279451444745064\n",
      "Epoch: 25 Batch: 200 Log-loss: 0.02442624419927597\n",
      "Epoch: 25 Batch: 240 Log-loss: 0.028949975967407227\n",
      "Epoch: 25 Batch: 280 Log-loss: 0.03731761500239372\n",
      "Epoch: 25 Batch: 320 Log-loss: 0.046843111515045166\n",
      "Epoch: 25 Batch: 360 Log-loss: 0.030656693503260612\n",
      "Epoch: 25 Batch: 400 Log-loss: 0.04373324289917946\n",
      "Epoch: 25 Batch: 440 Log-loss: 0.03847985342144966\n",
      "Epoch: 25 Batch: 480 Log-loss: 0.053075507283210754\n",
      "Epoch: 25 Batch: 520 Log-loss: 0.03496932610869408\n",
      "Epoch: 25 Batch: 560 Log-loss: 0.04578038305044174\n",
      "Epoch average log-loss: 0.03983372501097619\n",
      "In Epoch: 25, val_loss: 0.04189113562494423, best_val_loss: 0.04152159129044746, best_auc: 0.9875796939669147\n",
      "Epoch: 26 Batch: 0 Log-loss: 0.037422068417072296\n",
      "Epoch: 26 Batch: 40 Log-loss: 0.0512397326529026\n",
      "Epoch: 26 Batch: 80 Log-loss: 0.03594524413347244\n",
      "Epoch: 26 Batch: 120 Log-loss: 0.034012988209724426\n",
      "Epoch: 26 Batch: 160 Log-loss: 0.04894254729151726\n",
      "Epoch: 26 Batch: 200 Log-loss: 0.02580670826137066\n",
      "Epoch: 26 Batch: 240 Log-loss: 0.0487806461751461\n",
      "Epoch: 26 Batch: 280 Log-loss: 0.055774156004190445\n",
      "Epoch: 26 Batch: 320 Log-loss: 0.04244331642985344\n",
      "Epoch: 26 Batch: 360 Log-loss: 0.04748259857296944\n",
      "Epoch: 26 Batch: 400 Log-loss: 0.04524543508887291\n",
      "Epoch: 26 Batch: 440 Log-loss: 0.044260766357183456\n",
      "Epoch: 26 Batch: 480 Log-loss: 0.041872259229421616\n",
      "Epoch: 26 Batch: 520 Log-loss: 0.026240011677145958\n",
      "Epoch: 26 Batch: 560 Log-loss: 0.04849180206656456\n",
      "Epoch average log-loss: 0.03951196086792541\n",
      "In Epoch: 26, val_loss: 0.04203760244442605, best_val_loss: 0.04152159129044746, best_auc: 0.9875796939669147\n",
      "Epoch: 27 Batch: 0 Log-loss: 0.030173351988196373\n",
      "Epoch: 27 Batch: 40 Log-loss: 0.05025084316730499\n",
      "Epoch: 27 Batch: 80 Log-loss: 0.05517388880252838\n",
      "Epoch: 27 Batch: 120 Log-loss: 0.03612726181745529\n",
      "Epoch: 27 Batch: 160 Log-loss: 0.03154450282454491\n",
      "Epoch: 27 Batch: 200 Log-loss: 0.04650406539440155\n",
      "Epoch: 27 Batch: 240 Log-loss: 0.03450686112046242\n",
      "Epoch: 27 Batch: 280 Log-loss: 0.036692582070827484\n",
      "Epoch: 27 Batch: 320 Log-loss: 0.044867437332868576\n",
      "Epoch: 27 Batch: 360 Log-loss: 0.04530328884720802\n",
      "Epoch: 27 Batch: 400 Log-loss: 0.041529614478349686\n",
      "Epoch: 27 Batch: 440 Log-loss: 0.04002578184008598\n",
      "Epoch: 27 Batch: 480 Log-loss: 0.04706906899809837\n",
      "Epoch: 27 Batch: 520 Log-loss: 0.04180527105927467\n",
      "Epoch: 27 Batch: 560 Log-loss: 0.03931362181901932\n",
      "Epoch average log-loss: 0.03958988296134131\n",
      "In Epoch: 27, val_loss: 0.04174429614322229, best_val_loss: 0.04152159129044746, best_auc: 0.9875796939669147\n",
      "Epoch: 28 Batch: 0 Log-loss: 0.049682751297950745\n",
      "Epoch: 28 Batch: 40 Log-loss: 0.04358227178454399\n",
      "Epoch: 28 Batch: 80 Log-loss: 0.04607504606246948\n",
      "Epoch: 28 Batch: 120 Log-loss: 0.06265310198068619\n",
      "Epoch: 28 Batch: 160 Log-loss: 0.04246753826737404\n",
      "Epoch: 28 Batch: 200 Log-loss: 0.04405154660344124\n",
      "Epoch: 28 Batch: 240 Log-loss: 0.03806803748011589\n",
      "Epoch: 28 Batch: 280 Log-loss: 0.03445854410529137\n",
      "Epoch: 28 Batch: 320 Log-loss: 0.04523824527859688\n",
      "Epoch: 28 Batch: 360 Log-loss: 0.05113467574119568\n",
      "Epoch: 28 Batch: 400 Log-loss: 0.06456530094146729\n",
      "Epoch: 28 Batch: 440 Log-loss: 0.02413884550333023\n",
      "Epoch: 28 Batch: 480 Log-loss: 0.048430267721414566\n",
      "Epoch: 28 Batch: 520 Log-loss: 0.026093019172549248\n",
      "Epoch: 28 Batch: 560 Log-loss: 0.03319375216960907\n",
      "Epoch average log-loss: 0.039617290371097624\n",
      "In Epoch: 28, val_loss: 0.04157277636489681, best_val_loss: 0.04152159129044746, best_auc: 0.9875796939669147\n",
      "Epoch: 29 Batch: 0 Log-loss: 0.03702666610479355\n",
      "Epoch: 29 Batch: 40 Log-loss: 0.03322890028357506\n",
      "Epoch: 29 Batch: 80 Log-loss: 0.02944624423980713\n",
      "Epoch: 29 Batch: 120 Log-loss: 0.03230854868888855\n",
      "Epoch: 29 Batch: 160 Log-loss: 0.027668893337249756\n",
      "Epoch: 29 Batch: 200 Log-loss: 0.027735866606235504\n",
      "Epoch: 29 Batch: 240 Log-loss: 0.036380406469106674\n",
      "Epoch: 29 Batch: 280 Log-loss: 0.04713490605354309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 Batch: 320 Log-loss: 0.03630012273788452\n",
      "Epoch: 29 Batch: 360 Log-loss: 0.031387750059366226\n",
      "Epoch: 29 Batch: 400 Log-loss: 0.058537017554044724\n",
      "Epoch: 29 Batch: 440 Log-loss: 0.034526847302913666\n",
      "Epoch: 29 Batch: 480 Log-loss: 0.03541191294789314\n",
      "Epoch: 29 Batch: 520 Log-loss: 0.02692347764968872\n",
      "Epoch: 29 Batch: 560 Log-loss: 0.03639565780758858\n",
      "Epoch average log-loss: 0.03928831862618348\n",
      "In Epoch: 29, val_loss: 0.041781023667074785, best_val_loss: 0.04152159129044746, best_auc: 0.9875796939669147\n",
      "Epoch: 30 Batch: 0 Log-loss: 0.04013305529952049\n",
      "Epoch: 30 Batch: 40 Log-loss: 0.04646116495132446\n",
      "Epoch: 30 Batch: 80 Log-loss: 0.04831978306174278\n",
      "Epoch: 30 Batch: 120 Log-loss: 0.02642126940190792\n",
      "Epoch: 30 Batch: 160 Log-loss: 0.030945828184485435\n",
      "Epoch: 30 Batch: 200 Log-loss: 0.04967764392495155\n",
      "Epoch: 30 Batch: 240 Log-loss: 0.04990263655781746\n",
      "Epoch: 30 Batch: 280 Log-loss: 0.03926141932606697\n",
      "Epoch: 30 Batch: 320 Log-loss: 0.04467589035630226\n",
      "Epoch: 30 Batch: 360 Log-loss: 0.02990279532968998\n",
      "Epoch: 30 Batch: 400 Log-loss: 0.043194279074668884\n",
      "Epoch: 30 Batch: 440 Log-loss: 0.05506293103098869\n",
      "Epoch: 30 Batch: 480 Log-loss: 0.03975653275847435\n",
      "Epoch: 30 Batch: 520 Log-loss: 0.04861128330230713\n",
      "Epoch: 30 Batch: 560 Log-loss: 0.05134217068552971\n",
      "Epoch average log-loss: 0.03913040434376203\n",
      "In Epoch: 30, val_loss: 0.04188491146705545, best_val_loss: 0.04152159129044746, best_auc: 0.9875796939669147\n",
      "Epoch: 31 Batch: 0 Log-loss: 0.030644141137599945\n",
      "Epoch: 31 Batch: 40 Log-loss: 0.049447789788246155\n",
      "Epoch: 31 Batch: 80 Log-loss: 0.03765104338526726\n",
      "Epoch: 31 Batch: 120 Log-loss: 0.05274845287203789\n",
      "Epoch: 31 Batch: 160 Log-loss: 0.02987595833837986\n",
      "Epoch: 31 Batch: 200 Log-loss: 0.02505282126367092\n",
      "Epoch: 31 Batch: 240 Log-loss: 0.03633120283484459\n",
      "Epoch: 31 Batch: 280 Log-loss: 0.03455030173063278\n",
      "Epoch: 31 Batch: 320 Log-loss: 0.0356324128806591\n",
      "Epoch: 31 Batch: 360 Log-loss: 0.04249630495905876\n",
      "Epoch: 31 Batch: 400 Log-loss: 0.03430447354912758\n",
      "Epoch: 31 Batch: 440 Log-loss: 0.031168460845947266\n",
      "Epoch: 31 Batch: 480 Log-loss: 0.04464646056294441\n",
      "Epoch: 31 Batch: 520 Log-loss: 0.02780604176223278\n",
      "Epoch: 31 Batch: 560 Log-loss: 0.028809649869799614\n",
      "Epoch average log-loss: 0.039350218445594824\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 31, val_loss: 0.04124207907387364, best_val_loss: 0.04124207907387364, best_auc: 0.9874693744633388\n",
      "Epoch: 32 Batch: 0 Log-loss: 0.02813946269452572\n",
      "Epoch: 32 Batch: 40 Log-loss: 0.052521660923957825\n",
      "Epoch: 32 Batch: 80 Log-loss: 0.03656547889113426\n",
      "Epoch: 32 Batch: 120 Log-loss: 0.03292052820324898\n",
      "Epoch: 32 Batch: 160 Log-loss: 0.04107873514294624\n",
      "Epoch: 32 Batch: 200 Log-loss: 0.05058617517352104\n",
      "Epoch: 32 Batch: 240 Log-loss: 0.04488359019160271\n",
      "Epoch: 32 Batch: 280 Log-loss: 0.04437923803925514\n",
      "Epoch: 32 Batch: 320 Log-loss: 0.03048638440668583\n",
      "Epoch: 32 Batch: 360 Log-loss: 0.053936827927827835\n",
      "Epoch: 32 Batch: 400 Log-loss: 0.0414775088429451\n",
      "Epoch: 32 Batch: 440 Log-loss: 0.01732628606259823\n",
      "Epoch: 32 Batch: 480 Log-loss: 0.03818811476230621\n",
      "Epoch: 32 Batch: 520 Log-loss: 0.03972673788666725\n",
      "Epoch: 32 Batch: 560 Log-loss: 0.039858393371105194\n",
      "Epoch average log-loss: 0.03938779193974499\n",
      "In Epoch: 32, val_loss: 0.04157283367673171, best_val_loss: 0.04124207907387364, best_auc: 0.9874693744633388\n",
      "Epoch: 33 Batch: 0 Log-loss: 0.0476948581635952\n",
      "Epoch: 33 Batch: 40 Log-loss: 0.04196189343929291\n",
      "Epoch: 33 Batch: 80 Log-loss: 0.04254734143614769\n",
      "Epoch: 33 Batch: 120 Log-loss: 0.02761404775083065\n",
      "Epoch: 33 Batch: 160 Log-loss: 0.02969743125140667\n",
      "Epoch: 33 Batch: 200 Log-loss: 0.04099530354142189\n",
      "Epoch: 33 Batch: 240 Log-loss: 0.04849562421441078\n",
      "Epoch: 33 Batch: 280 Log-loss: 0.038412291556596756\n",
      "Epoch: 33 Batch: 320 Log-loss: 0.03499947860836983\n",
      "Epoch: 33 Batch: 360 Log-loss: 0.06177328899502754\n",
      "Epoch: 33 Batch: 400 Log-loss: 0.034523334354162216\n",
      "Epoch: 33 Batch: 440 Log-loss: 0.024172836914658546\n",
      "Epoch: 33 Batch: 480 Log-loss: 0.059404466301202774\n",
      "Epoch: 33 Batch: 520 Log-loss: 0.02927442453801632\n",
      "Epoch: 33 Batch: 560 Log-loss: 0.028918161988258362\n",
      "Epoch average log-loss: 0.03933199191399451\n",
      "In Epoch: 33, val_loss: 0.041571725144600065, best_val_loss: 0.04124207907387364, best_auc: 0.9874693744633388\n",
      "Epoch: 34 Batch: 0 Log-loss: 0.03571658954024315\n",
      "Epoch: 34 Batch: 40 Log-loss: 0.045689817517995834\n",
      "Epoch: 34 Batch: 80 Log-loss: 0.03048519976437092\n",
      "Epoch: 34 Batch: 120 Log-loss: 0.028980202972888947\n",
      "Epoch: 34 Batch: 160 Log-loss: 0.02997571974992752\n",
      "Epoch: 34 Batch: 200 Log-loss: 0.04401716589927673\n",
      "Epoch: 34 Batch: 240 Log-loss: 0.04715559259057045\n",
      "Epoch: 34 Batch: 280 Log-loss: 0.041872452944517136\n",
      "Epoch: 34 Batch: 320 Log-loss: 0.029688159003853798\n",
      "Epoch: 34 Batch: 360 Log-loss: 0.014124009758234024\n",
      "Epoch: 34 Batch: 400 Log-loss: 0.033029671758413315\n",
      "Epoch: 34 Batch: 440 Log-loss: 0.04633258283138275\n",
      "Epoch: 34 Batch: 480 Log-loss: 0.04115063324570656\n",
      "Epoch: 34 Batch: 520 Log-loss: 0.03517956659197807\n",
      "Epoch: 34 Batch: 560 Log-loss: 0.027177825570106506\n",
      "Epoch average log-loss: 0.03920771814882755\n",
      "In Epoch: 34, val_loss: 0.04189980962615262, best_val_loss: 0.04124207907387364, best_auc: 0.9874693744633388\n",
      "Epoch: 35 Batch: 0 Log-loss: 0.06136921048164368\n",
      "Epoch: 35 Batch: 40 Log-loss: 0.03873037174344063\n",
      "Epoch: 35 Batch: 80 Log-loss: 0.049595341086387634\n",
      "Epoch: 35 Batch: 120 Log-loss: 0.03897998109459877\n",
      "Epoch: 35 Batch: 160 Log-loss: 0.03838762640953064\n",
      "Epoch: 35 Batch: 200 Log-loss: 0.03748256713151932\n",
      "Epoch: 35 Batch: 240 Log-loss: 0.0430946983397007\n",
      "Epoch: 35 Batch: 280 Log-loss: 0.028625240549445152\n",
      "Epoch: 35 Batch: 320 Log-loss: 0.03550077974796295\n",
      "Epoch: 35 Batch: 360 Log-loss: 0.06053805723786354\n",
      "Epoch: 35 Batch: 400 Log-loss: 0.03830545023083687\n",
      "Epoch: 35 Batch: 440 Log-loss: 0.04545728489756584\n",
      "Epoch: 35 Batch: 480 Log-loss: 0.03318062424659729\n",
      "Epoch: 35 Batch: 520 Log-loss: 0.036376144737005234\n",
      "Epoch: 35 Batch: 560 Log-loss: 0.04409775510430336\n",
      "Epoch average log-loss: 0.03929697149433196\n",
      "In Epoch: 35, val_loss: 0.041624016060906814, best_val_loss: 0.04124207907387364, best_auc: 0.9874693744633388\n",
      "Epoch: 36 Batch: 0 Log-loss: 0.0413835234940052\n",
      "Epoch: 36 Batch: 40 Log-loss: 0.04770559445023537\n",
      "Epoch: 36 Batch: 80 Log-loss: 0.03805753216147423\n",
      "Epoch: 36 Batch: 120 Log-loss: 0.04029243066906929\n",
      "Epoch: 36 Batch: 160 Log-loss: 0.0313488133251667\n",
      "Epoch: 36 Batch: 200 Log-loss: 0.051280826330184937\n",
      "Epoch: 36 Batch: 240 Log-loss: 0.053891733288764954\n",
      "Epoch: 36 Batch: 280 Log-loss: 0.04126312583684921\n",
      "Epoch: 36 Batch: 320 Log-loss: 0.0211931262165308\n",
      "Epoch: 36 Batch: 360 Log-loss: 0.03437136486172676\n",
      "Epoch: 36 Batch: 400 Log-loss: 0.030587494373321533\n",
      "Epoch: 36 Batch: 440 Log-loss: 0.051469508558511734\n",
      "Epoch: 36 Batch: 480 Log-loss: 0.04502760246396065\n",
      "Epoch: 36 Batch: 520 Log-loss: 0.04240133985877037\n",
      "Epoch: 36 Batch: 560 Log-loss: 0.04022764414548874\n",
      "Epoch average log-loss: 0.0389874655620328\n",
      "In Epoch: 36, val_loss: 0.04175244787803509, best_val_loss: 0.04124207907387364, best_auc: 0.9874693744633388\n",
      "Epoch: 37 Batch: 0 Log-loss: 0.02933575212955475\n",
      "Epoch: 37 Batch: 40 Log-loss: 0.035367149859666824\n",
      "Epoch: 37 Batch: 80 Log-loss: 0.03626761585474014\n",
      "Epoch: 37 Batch: 120 Log-loss: 0.03820614144206047\n",
      "Epoch: 37 Batch: 160 Log-loss: 0.04566694796085358\n",
      "Epoch: 37 Batch: 200 Log-loss: 0.032098352909088135\n",
      "Epoch: 37 Batch: 240 Log-loss: 0.03811464458703995\n",
      "Epoch: 37 Batch: 280 Log-loss: 0.03996933251619339\n",
      "Epoch: 37 Batch: 320 Log-loss: 0.049952924251556396\n",
      "Epoch: 37 Batch: 360 Log-loss: 0.040741030126810074\n",
      "Epoch: 37 Batch: 400 Log-loss: 0.03495362773537636\n",
      "Epoch: 37 Batch: 440 Log-loss: 0.028758376836776733\n",
      "Epoch: 37 Batch: 480 Log-loss: 0.044895779341459274\n",
      "Epoch: 37 Batch: 520 Log-loss: 0.032697975635528564\n",
      "Epoch: 37 Batch: 560 Log-loss: 0.03748133406043053\n",
      "Epoch average log-loss: 0.03904656985375498\n",
      "In Epoch: 37, val_loss: 0.04135420029546331, best_val_loss: 0.04124207907387364, best_auc: 0.9874693744633388\n",
      "Epoch: 38 Batch: 0 Log-loss: 0.03541646525263786\n",
      "Epoch: 38 Batch: 40 Log-loss: 0.04588804766535759\n",
      "Epoch: 38 Batch: 80 Log-loss: 0.03485088422894478\n",
      "Epoch: 38 Batch: 120 Log-loss: 0.040056031197309494\n",
      "Epoch: 38 Batch: 160 Log-loss: 0.0463278628885746\n",
      "Epoch: 38 Batch: 200 Log-loss: 0.04390767216682434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 Batch: 240 Log-loss: 0.030341118574142456\n",
      "Epoch: 38 Batch: 280 Log-loss: 0.05550031736493111\n",
      "Epoch: 38 Batch: 320 Log-loss: 0.03902146592736244\n",
      "Epoch: 38 Batch: 360 Log-loss: 0.039430782198905945\n",
      "Epoch: 38 Batch: 400 Log-loss: 0.040364932268857956\n",
      "Epoch: 38 Batch: 440 Log-loss: 0.02870199829339981\n",
      "Epoch: 38 Batch: 480 Log-loss: 0.0351371243596077\n",
      "Epoch: 38 Batch: 520 Log-loss: 0.0452023446559906\n",
      "Epoch: 38 Batch: 560 Log-loss: 0.03655380755662918\n",
      "Epoch average log-loss: 0.03915527346543968\n",
      "In Epoch: 38, val_loss: 0.04190331217903309, best_val_loss: 0.04124207907387364, best_auc: 0.9874693744633388\n",
      "Epoch: 39 Batch: 0 Log-loss: 0.036014676094055176\n",
      "Epoch: 39 Batch: 40 Log-loss: 0.05332806333899498\n",
      "Epoch: 39 Batch: 80 Log-loss: 0.05382205918431282\n",
      "Epoch: 39 Batch: 120 Log-loss: 0.03557989373803139\n",
      "Epoch: 39 Batch: 160 Log-loss: 0.02984657883644104\n",
      "Epoch: 39 Batch: 200 Log-loss: 0.02779170125722885\n",
      "Epoch: 39 Batch: 240 Log-loss: 0.03147390857338905\n",
      "Epoch: 39 Batch: 280 Log-loss: 0.047934070229530334\n",
      "Epoch: 39 Batch: 320 Log-loss: 0.04823104664683342\n",
      "Epoch: 39 Batch: 360 Log-loss: 0.028974143788218498\n",
      "Epoch: 39 Batch: 400 Log-loss: 0.046274226158857346\n",
      "Epoch: 39 Batch: 440 Log-loss: 0.03854590281844139\n",
      "Epoch: 39 Batch: 480 Log-loss: 0.040544990450143814\n",
      "Epoch: 39 Batch: 520 Log-loss: 0.017970914021134377\n",
      "Epoch: 39 Batch: 560 Log-loss: 0.03785473108291626\n",
      "Epoch average log-loss: 0.03899347276227283\n",
      "In Epoch: 39, val_loss: 0.04159981464332345, best_val_loss: 0.04124207907387364, best_auc: 0.9874693744633388\n",
      "Epoch: 40 Batch: 0 Log-loss: 0.05180126801133156\n",
      "Epoch: 40 Batch: 40 Log-loss: 0.04793762043118477\n",
      "Epoch: 40 Batch: 80 Log-loss: 0.04809267446398735\n",
      "Epoch: 40 Batch: 120 Log-loss: 0.04822773113846779\n",
      "Epoch: 40 Batch: 160 Log-loss: 0.034333355724811554\n",
      "Epoch: 40 Batch: 200 Log-loss: 0.05229322239756584\n",
      "Epoch: 40 Batch: 240 Log-loss: 0.035200607031583786\n",
      "Epoch: 40 Batch: 280 Log-loss: 0.05903596803545952\n",
      "Epoch: 40 Batch: 320 Log-loss: 0.038107000291347504\n",
      "Epoch: 40 Batch: 360 Log-loss: 0.03678864613175392\n",
      "Epoch: 40 Batch: 400 Log-loss: 0.03285108134150505\n",
      "Epoch: 40 Batch: 440 Log-loss: 0.046616315841674805\n",
      "Epoch: 40 Batch: 480 Log-loss: 0.04966472089290619\n",
      "Epoch: 40 Batch: 520 Log-loss: 0.03825153782963753\n",
      "Epoch: 40 Batch: 560 Log-loss: 0.03955094516277313\n",
      "Epoch average log-loss: 0.03906060511445893\n",
      "In Epoch: 40, val_loss: 0.04190673260408181, best_val_loss: 0.04124207907387364, best_auc: 0.9874693744633388\n",
      "Epoch: 41 Batch: 0 Log-loss: 0.027831830084323883\n",
      "Epoch: 41 Batch: 40 Log-loss: 0.030039792880415916\n",
      "Epoch: 41 Batch: 80 Log-loss: 0.03757281228899956\n",
      "Epoch: 41 Batch: 120 Log-loss: 0.045065511018037796\n",
      "Epoch: 41 Batch: 160 Log-loss: 0.029455387964844704\n",
      "Epoch: 41 Batch: 200 Log-loss: 0.03386978432536125\n",
      "Epoch: 41 Batch: 240 Log-loss: 0.027068952098488808\n",
      "Epoch: 41 Batch: 280 Log-loss: 0.056196168065071106\n",
      "Epoch: 41 Batch: 320 Log-loss: 0.041334282606840134\n",
      "Epoch: 41 Batch: 360 Log-loss: 0.03362603485584259\n",
      "Epoch: 41 Batch: 400 Log-loss: 0.031091617420315742\n",
      "Epoch: 41 Batch: 440 Log-loss: 0.02649587392807007\n",
      "Epoch: 41 Batch: 480 Log-loss: 0.04941253736615181\n",
      "Epoch: 41 Batch: 520 Log-loss: 0.045800164341926575\n",
      "Epoch: 41 Batch: 560 Log-loss: 0.03883960843086243\n",
      "Epoch average log-loss: 0.038895691159580435\n",
      "In Epoch: 41, val_loss: 0.04201517409360614, best_val_loss: 0.04124207907387364, best_auc: 0.9874693744633388\n",
      "Epoch: 42 Batch: 0 Log-loss: 0.02863248623907566\n",
      "Epoch: 42 Batch: 40 Log-loss: 0.03960280865430832\n",
      "Epoch: 42 Batch: 80 Log-loss: 0.029644759371876717\n",
      "Epoch: 42 Batch: 120 Log-loss: 0.03896818682551384\n",
      "Epoch: 42 Batch: 160 Log-loss: 0.04062690958380699\n",
      "Epoch: 42 Batch: 200 Log-loss: 0.031114554032683372\n",
      "Epoch: 42 Batch: 240 Log-loss: 0.033198606222867966\n",
      "Epoch: 42 Batch: 280 Log-loss: 0.03560807183384895\n",
      "Epoch: 42 Batch: 320 Log-loss: 0.046590521931648254\n",
      "Epoch: 42 Batch: 360 Log-loss: 0.059745997190475464\n",
      "Epoch: 42 Batch: 400 Log-loss: 0.03984557464718819\n",
      "Epoch: 42 Batch: 440 Log-loss: 0.03829417750239372\n",
      "Epoch: 42 Batch: 480 Log-loss: 0.035810668021440506\n",
      "Epoch: 42 Batch: 520 Log-loss: 0.027896301820874214\n",
      "Epoch: 42 Batch: 560 Log-loss: 0.03578981012105942\n",
      "Epoch average log-loss: 0.03888323830573687\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 42, val_loss: 0.04121824086557969, best_val_loss: 0.04121824086557969, best_auc: 0.987068847244755\n",
      "Epoch: 43 Batch: 0 Log-loss: 0.05945344269275665\n",
      "Epoch: 43 Batch: 40 Log-loss: 0.049942418932914734\n",
      "Epoch: 43 Batch: 80 Log-loss: 0.04056262597441673\n",
      "Epoch: 43 Batch: 120 Log-loss: 0.03980858623981476\n",
      "Epoch: 43 Batch: 160 Log-loss: 0.04364542290568352\n",
      "Epoch: 43 Batch: 200 Log-loss: 0.04129045084118843\n",
      "Epoch: 43 Batch: 240 Log-loss: 0.04399137198925018\n",
      "Epoch: 43 Batch: 280 Log-loss: 0.042134881019592285\n",
      "Epoch: 43 Batch: 320 Log-loss: 0.04433629289269447\n",
      "Epoch: 43 Batch: 360 Log-loss: 0.04047717526555061\n",
      "Epoch: 43 Batch: 400 Log-loss: 0.052020326256752014\n",
      "Epoch: 43 Batch: 440 Log-loss: 0.040295325219631195\n",
      "Epoch: 43 Batch: 480 Log-loss: 0.046633362770080566\n",
      "Epoch: 43 Batch: 520 Log-loss: 0.03187122941017151\n",
      "Epoch: 43 Batch: 560 Log-loss: 0.029302509501576424\n",
      "Epoch average log-loss: 0.038936414952123806\n",
      "In Epoch: 43, val_loss: 0.04182748511145288, best_val_loss: 0.04121824086557969, best_auc: 0.987068847244755\n",
      "Epoch: 44 Batch: 0 Log-loss: 0.03497463837265968\n",
      "Epoch: 44 Batch: 40 Log-loss: 0.03928723931312561\n",
      "Epoch: 44 Batch: 80 Log-loss: 0.03928280249238014\n",
      "Epoch: 44 Batch: 120 Log-loss: 0.033390797674655914\n",
      "Epoch: 44 Batch: 160 Log-loss: 0.039441823959350586\n",
      "Epoch: 44 Batch: 200 Log-loss: 0.031418006867170334\n",
      "Epoch: 44 Batch: 240 Log-loss: 0.03459848091006279\n",
      "Epoch: 44 Batch: 280 Log-loss: 0.03063295967876911\n",
      "Epoch: 44 Batch: 320 Log-loss: 0.03532135859131813\n",
      "Epoch: 44 Batch: 360 Log-loss: 0.037770356982946396\n",
      "Epoch: 44 Batch: 400 Log-loss: 0.04731391742825508\n",
      "Epoch: 44 Batch: 440 Log-loss: 0.03936140611767769\n",
      "Epoch: 44 Batch: 480 Log-loss: 0.0398080088198185\n",
      "Epoch: 44 Batch: 520 Log-loss: 0.027035698294639587\n",
      "Epoch: 44 Batch: 560 Log-loss: 0.04548748582601547\n",
      "Epoch average log-loss: 0.038921194686554375\n",
      "In Epoch: 44, val_loss: 0.041474002993404264, best_val_loss: 0.04121824086557969, best_auc: 0.987068847244755\n",
      "Epoch: 45 Batch: 0 Log-loss: 0.048800278455019\n",
      "Epoch: 45 Batch: 40 Log-loss: 0.04784660041332245\n",
      "Epoch: 45 Batch: 80 Log-loss: 0.03377007693052292\n",
      "Epoch: 45 Batch: 120 Log-loss: 0.044329434633255005\n",
      "Epoch: 45 Batch: 160 Log-loss: 0.03333449736237526\n",
      "Epoch: 45 Batch: 200 Log-loss: 0.04391296207904816\n",
      "Epoch: 45 Batch: 240 Log-loss: 0.04835386574268341\n",
      "Epoch: 45 Batch: 280 Log-loss: 0.04033542051911354\n",
      "Epoch: 45 Batch: 320 Log-loss: 0.03136197850108147\n",
      "Epoch: 45 Batch: 360 Log-loss: 0.0422951839864254\n",
      "Epoch: 45 Batch: 400 Log-loss: 0.0395292229950428\n",
      "Epoch: 45 Batch: 440 Log-loss: 0.039984080940485\n",
      "Epoch: 45 Batch: 480 Log-loss: 0.03339957818388939\n",
      "Epoch: 45 Batch: 520 Log-loss: 0.03215038776397705\n",
      "Epoch: 45 Batch: 560 Log-loss: 0.04137907177209854\n",
      "Epoch average log-loss: 0.03883415381756744\n",
      "In Epoch: 45, val_loss: 0.04133804200311194, best_val_loss: 0.04121824086557969, best_auc: 0.987068847244755\n",
      "Epoch: 46 Batch: 0 Log-loss: 0.041939083486795425\n",
      "Epoch: 46 Batch: 40 Log-loss: 0.042545538395643234\n",
      "Epoch: 46 Batch: 80 Log-loss: 0.04061325266957283\n",
      "Epoch: 46 Batch: 120 Log-loss: 0.06227133795619011\n",
      "Epoch: 46 Batch: 160 Log-loss: 0.030612535774707794\n",
      "Epoch: 46 Batch: 200 Log-loss: 0.04424413666129112\n",
      "Epoch: 46 Batch: 240 Log-loss: 0.036732688546180725\n",
      "Epoch: 46 Batch: 280 Log-loss: 0.03629693016409874\n",
      "Epoch: 46 Batch: 320 Log-loss: 0.0455765537917614\n",
      "Epoch: 46 Batch: 360 Log-loss: 0.03570449724793434\n",
      "Epoch: 46 Batch: 400 Log-loss: 0.0485696978867054\n",
      "Epoch: 46 Batch: 440 Log-loss: 0.02790592797100544\n",
      "Epoch: 46 Batch: 480 Log-loss: 0.034721530973911285\n",
      "Epoch: 46 Batch: 520 Log-loss: 0.04302925243973732\n",
      "Epoch: 46 Batch: 560 Log-loss: 0.03382527828216553\n",
      "Epoch average log-loss: 0.03885797276826841\n",
      "In Epoch: 46, val_loss: 0.041563387954908264, best_val_loss: 0.04121824086557969, best_auc: 0.987068847244755\n",
      "Epoch: 47 Batch: 0 Log-loss: 0.037536706775426865\n",
      "Epoch: 47 Batch: 40 Log-loss: 0.03358225151896477\n",
      "Epoch: 47 Batch: 80 Log-loss: 0.035733144730329514\n",
      "Epoch: 47 Batch: 120 Log-loss: 0.04458869993686676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 Batch: 160 Log-loss: 0.03379477187991142\n",
      "Epoch: 47 Batch: 200 Log-loss: 0.03051295131444931\n",
      "Epoch: 47 Batch: 240 Log-loss: 0.03222961351275444\n",
      "Epoch: 47 Batch: 280 Log-loss: 0.0376417450606823\n",
      "Epoch: 47 Batch: 320 Log-loss: 0.0408441387116909\n",
      "Epoch: 47 Batch: 360 Log-loss: 0.035791344940662384\n",
      "Epoch: 47 Batch: 400 Log-loss: 0.028622793033719063\n",
      "Epoch: 47 Batch: 440 Log-loss: 0.05428829416632652\n",
      "Epoch: 47 Batch: 480 Log-loss: 0.04175570234656334\n",
      "Epoch: 47 Batch: 520 Log-loss: 0.039322543889284134\n",
      "Epoch: 47 Batch: 560 Log-loss: 0.04553065821528435\n",
      "Epoch average log-loss: 0.03871911936106959\n",
      "In Epoch: 47, val_loss: 0.04193246710667498, best_val_loss: 0.04121824086557969, best_auc: 0.987068847244755\n",
      "Epoch: 48 Batch: 0 Log-loss: 0.034154266119003296\n",
      "Epoch: 48 Batch: 40 Log-loss: 0.03722543269395828\n",
      "Epoch: 48 Batch: 80 Log-loss: 0.0395892933011055\n",
      "Epoch: 48 Batch: 120 Log-loss: 0.028329206630587578\n",
      "Epoch: 48 Batch: 160 Log-loss: 0.028646821156144142\n",
      "Epoch: 48 Batch: 200 Log-loss: 0.03556659817695618\n",
      "Epoch: 48 Batch: 240 Log-loss: 0.0393022857606411\n",
      "Epoch: 48 Batch: 280 Log-loss: 0.038978319615125656\n",
      "Epoch: 48 Batch: 320 Log-loss: 0.05679783597588539\n",
      "Epoch: 48 Batch: 360 Log-loss: 0.04714613035321236\n",
      "Epoch: 48 Batch: 400 Log-loss: 0.03347313031554222\n",
      "Epoch: 48 Batch: 440 Log-loss: 0.04006624221801758\n",
      "Epoch: 48 Batch: 480 Log-loss: 0.03691596910357475\n",
      "Epoch: 48 Batch: 520 Log-loss: 0.04902748391032219\n",
      "Epoch: 48 Batch: 560 Log-loss: 0.03580055013298988\n",
      "Epoch average log-loss: 0.03872288831709219\n",
      "In Epoch: 48, val_loss: 0.04154047059800323, best_val_loss: 0.04121824086557969, best_auc: 0.987068847244755\n",
      "Epoch: 49 Batch: 0 Log-loss: 0.03943907096982002\n",
      "Epoch: 49 Batch: 40 Log-loss: 0.038217704743146896\n",
      "Epoch: 49 Batch: 80 Log-loss: 0.03154900670051575\n",
      "Epoch: 49 Batch: 120 Log-loss: 0.045274849981069565\n",
      "Epoch: 49 Batch: 160 Log-loss: 0.03554674610495567\n",
      "Epoch: 49 Batch: 200 Log-loss: 0.03877270594239235\n",
      "Epoch: 49 Batch: 240 Log-loss: 0.04281311109662056\n",
      "Epoch: 49 Batch: 280 Log-loss: 0.027875078842043877\n",
      "Epoch: 49 Batch: 320 Log-loss: 0.026526562869548798\n",
      "Epoch: 49 Batch: 360 Log-loss: 0.03834066167473793\n",
      "Epoch: 49 Batch: 400 Log-loss: 0.03844893351197243\n",
      "Epoch: 49 Batch: 440 Log-loss: 0.033467236906290054\n",
      "Epoch: 49 Batch: 480 Log-loss: 0.0342281237244606\n",
      "Epoch: 49 Batch: 520 Log-loss: 0.029011236503720284\n",
      "Epoch: 49 Batch: 560 Log-loss: 0.030912967398762703\n",
      "Epoch average log-loss: 0.03850320949485259\n",
      "In Epoch: 49, val_loss: 0.04192671067469006, best_val_loss: 0.04121824086557969, best_auc: 0.987068847244755\n",
      "Epoch: 50 Batch: 0 Log-loss: 0.03761937841773033\n",
      "Epoch: 50 Batch: 40 Log-loss: 0.021209092810750008\n",
      "Epoch: 50 Batch: 80 Log-loss: 0.030197827145457268\n",
      "Epoch: 50 Batch: 120 Log-loss: 0.036490920931100845\n",
      "Epoch: 50 Batch: 160 Log-loss: 0.04488367959856987\n",
      "Epoch: 50 Batch: 200 Log-loss: 0.030521007254719734\n",
      "Epoch: 50 Batch: 240 Log-loss: 0.050135139375925064\n",
      "Epoch: 50 Batch: 280 Log-loss: 0.04249085858464241\n",
      "Epoch: 50 Batch: 320 Log-loss: 0.049563851207494736\n",
      "Epoch: 50 Batch: 360 Log-loss: 0.03982203081250191\n",
      "Epoch: 50 Batch: 400 Log-loss: 0.026222877204418182\n",
      "Epoch: 50 Batch: 440 Log-loss: 0.030256040394306183\n",
      "Epoch: 50 Batch: 480 Log-loss: 0.03265189006924629\n",
      "Epoch: 50 Batch: 520 Log-loss: 0.043133288621902466\n",
      "Epoch: 50 Batch: 560 Log-loss: 0.030427075922489166\n",
      "Epoch average log-loss: 0.03875488639543099\n",
      "In Epoch: 50, val_loss: 0.041577109438540136, best_val_loss: 0.04121824086557969, best_auc: 0.987068847244755\n",
      "Epoch: 51 Batch: 0 Log-loss: 0.043240737169981\n",
      "Epoch: 51 Batch: 40 Log-loss: 0.021439405158162117\n",
      "Epoch: 51 Batch: 80 Log-loss: 0.04523494839668274\n",
      "Epoch: 51 Batch: 120 Log-loss: 0.05764816328883171\n",
      "Epoch: 51 Batch: 160 Log-loss: 0.0295381098985672\n",
      "Epoch: 51 Batch: 200 Log-loss: 0.034194204956293106\n",
      "Epoch: 51 Batch: 240 Log-loss: 0.03955354169011116\n",
      "Epoch: 51 Batch: 280 Log-loss: 0.03792883828282356\n",
      "Epoch: 51 Batch: 320 Log-loss: 0.028407782316207886\n",
      "Epoch: 51 Batch: 360 Log-loss: 0.04063684493303299\n",
      "Epoch: 51 Batch: 400 Log-loss: 0.039109837263822556\n",
      "Epoch: 51 Batch: 440 Log-loss: 0.03859998658299446\n",
      "Epoch: 51 Batch: 480 Log-loss: 0.030830057337880135\n",
      "Epoch: 51 Batch: 520 Log-loss: 0.03174907714128494\n",
      "Epoch: 51 Batch: 560 Log-loss: 0.039401210844516754\n",
      "Epoch average log-loss: 0.03868778740787612\n",
      "In Epoch: 51, val_loss: 0.041379922392940006, best_val_loss: 0.04121824086557969, best_auc: 0.987068847244755\n",
      "Epoch: 52 Batch: 0 Log-loss: 0.0287467110902071\n",
      "Epoch: 52 Batch: 40 Log-loss: 0.04541751742362976\n",
      "Epoch: 52 Batch: 80 Log-loss: 0.03880075737833977\n",
      "Epoch: 52 Batch: 120 Log-loss: 0.04989246651530266\n",
      "Epoch: 52 Batch: 160 Log-loss: 0.043790969997644424\n",
      "Epoch: 52 Batch: 200 Log-loss: 0.03883359953761101\n",
      "Epoch: 52 Batch: 240 Log-loss: 0.05289710685610771\n",
      "Epoch: 52 Batch: 280 Log-loss: 0.0491630844771862\n",
      "Epoch: 52 Batch: 320 Log-loss: 0.03284890204668045\n",
      "Epoch: 52 Batch: 360 Log-loss: 0.059528764337301254\n",
      "Epoch: 52 Batch: 400 Log-loss: 0.05046956613659859\n",
      "Epoch: 52 Batch: 440 Log-loss: 0.06627603620290756\n",
      "Epoch: 52 Batch: 480 Log-loss: 0.05113410949707031\n",
      "Epoch: 52 Batch: 520 Log-loss: 0.04108003154397011\n",
      "Epoch: 52 Batch: 560 Log-loss: 0.038477055728435516\n",
      "Epoch average log-loss: 0.03885870511377496\n",
      "In Epoch: 52, val_loss: 0.041658886130688064, best_val_loss: 0.04121824086557969, best_auc: 0.987068847244755\n",
      "Epoch: 53 Batch: 0 Log-loss: 0.03390916436910629\n",
      "Epoch: 53 Batch: 40 Log-loss: 0.032728809863328934\n",
      "Epoch: 53 Batch: 80 Log-loss: 0.02950447052717209\n",
      "Epoch: 53 Batch: 120 Log-loss: 0.04144638404250145\n",
      "Epoch: 53 Batch: 160 Log-loss: 0.0428728349506855\n",
      "Epoch: 53 Batch: 200 Log-loss: 0.040327657014131546\n",
      "Epoch: 53 Batch: 240 Log-loss: 0.038988370448350906\n",
      "Epoch: 53 Batch: 280 Log-loss: 0.03369849920272827\n",
      "Epoch: 53 Batch: 320 Log-loss: 0.05059504881501198\n",
      "Epoch: 53 Batch: 360 Log-loss: 0.030741320922970772\n",
      "Epoch: 53 Batch: 400 Log-loss: 0.03603515028953552\n",
      "Epoch: 53 Batch: 440 Log-loss: 0.03964286670088768\n",
      "Epoch: 53 Batch: 480 Log-loss: 0.04501687362790108\n",
      "Epoch: 53 Batch: 520 Log-loss: 0.044492337852716446\n",
      "Epoch: 53 Batch: 560 Log-loss: 0.033658016473054886\n",
      "Epoch average log-loss: 0.038753860064649155\n",
      "In Epoch: 53, val_loss: 0.04178510331233523, best_val_loss: 0.04121824086557969, best_auc: 0.987068847244755\n",
      "Epoch: 54 Batch: 0 Log-loss: 0.039473939687013626\n",
      "Epoch: 54 Batch: 40 Log-loss: 0.0328049398958683\n",
      "Epoch: 54 Batch: 80 Log-loss: 0.028650930151343346\n",
      "Epoch: 54 Batch: 120 Log-loss: 0.02932414971292019\n",
      "Epoch: 54 Batch: 160 Log-loss: 0.0430263988673687\n",
      "Epoch: 54 Batch: 200 Log-loss: 0.03286842256784439\n",
      "Epoch: 54 Batch: 240 Log-loss: 0.031439196318387985\n",
      "Epoch: 54 Batch: 280 Log-loss: 0.053168073296546936\n",
      "Epoch: 54 Batch: 320 Log-loss: 0.049513209611177444\n",
      "Epoch: 54 Batch: 360 Log-loss: 0.04692317172884941\n",
      "Epoch: 54 Batch: 400 Log-loss: 0.04591688886284828\n",
      "Epoch: 54 Batch: 440 Log-loss: 0.029143719002604485\n",
      "Epoch: 54 Batch: 480 Log-loss: 0.03561875969171524\n",
      "Epoch: 54 Batch: 520 Log-loss: 0.0382385291159153\n",
      "Epoch: 54 Batch: 560 Log-loss: 0.03657329082489014\n",
      "Epoch average log-loss: 0.03874391704199037\n",
      "A pre-trained model at model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt has been loaded.\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn7.pt.\n",
      "\n",
      "Choose the torch base model.\n",
      "## Training on fold 8 ##\n",
      "Epoch: 1 Batch: 0 Log-loss: 0.6952973008155823\n",
      "Epoch: 1 Batch: 40 Log-loss: 0.1590544432401657\n",
      "Epoch: 1 Batch: 80 Log-loss: 0.11848946660757065\n",
      "Epoch: 1 Batch: 120 Log-loss: 0.03427892178297043\n",
      "Epoch: 1 Batch: 160 Log-loss: 0.09465333074331284\n",
      "Epoch: 1 Batch: 200 Log-loss: 0.06499423831701279\n",
      "Epoch: 1 Batch: 240 Log-loss: 0.049646493047475815\n",
      "Epoch: 1 Batch: 280 Log-loss: 0.05579909682273865\n",
      "Epoch: 1 Batch: 320 Log-loss: 0.053866613656282425\n",
      "Epoch: 1 Batch: 360 Log-loss: 0.03630213439464569\n",
      "Epoch: 1 Batch: 400 Log-loss: 0.05262729153037071\n",
      "Epoch: 1 Batch: 440 Log-loss: 0.04600189998745918\n",
      "Epoch: 1 Batch: 480 Log-loss: 0.05829353630542755\n",
      "Epoch: 1 Batch: 520 Log-loss: 0.051445361226797104\n",
      "Epoch: 1 Batch: 560 Log-loss: 0.06752151250839233\n",
      "Epoch average log-loss: 0.08172723670223994\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 1, val_loss: 0.05360535533658589, best_val_loss: 0.05360535533658589, best_auc: 0.9727307821066287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Batch: 0 Log-loss: 0.07638002932071686\n",
      "Epoch: 2 Batch: 40 Log-loss: 0.05059297755360603\n",
      "Epoch: 2 Batch: 80 Log-loss: 0.039266034960746765\n",
      "Epoch: 2 Batch: 120 Log-loss: 0.05587725713849068\n",
      "Epoch: 2 Batch: 160 Log-loss: 0.06839199364185333\n",
      "Epoch: 2 Batch: 200 Log-loss: 0.05092044547200203\n",
      "Epoch: 2 Batch: 240 Log-loss: 0.04532298073172569\n",
      "Epoch: 2 Batch: 280 Log-loss: 0.02517469972372055\n",
      "Epoch: 2 Batch: 320 Log-loss: 0.04865586385130882\n",
      "Epoch: 2 Batch: 360 Log-loss: 0.05257861316204071\n",
      "Epoch: 2 Batch: 400 Log-loss: 0.05393114313483238\n",
      "Epoch: 2 Batch: 440 Log-loss: 0.03961683437228203\n",
      "Epoch: 2 Batch: 480 Log-loss: 0.052094969898462296\n",
      "Epoch: 2 Batch: 520 Log-loss: 0.047183990478515625\n",
      "Epoch: 2 Batch: 560 Log-loss: 0.06203000992536545\n",
      "Epoch average log-loss: 0.05185389698017388\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 2, val_loss: 0.04834407754120119, best_val_loss: 0.04834407754120119, best_auc: 0.9778770611110832\n",
      "Epoch: 3 Batch: 0 Log-loss: 0.05359239876270294\n",
      "Epoch: 3 Batch: 40 Log-loss: 0.0546293742954731\n",
      "Epoch: 3 Batch: 80 Log-loss: 0.04722851887345314\n",
      "Epoch: 3 Batch: 120 Log-loss: 0.05422309413552284\n",
      "Epoch: 3 Batch: 160 Log-loss: 0.03972118720412254\n",
      "Epoch: 3 Batch: 200 Log-loss: 0.050248607993125916\n",
      "Epoch: 3 Batch: 240 Log-loss: 0.039407651871442795\n",
      "Epoch: 3 Batch: 280 Log-loss: 0.04960137978196144\n",
      "Epoch: 3 Batch: 320 Log-loss: 0.04190165922045708\n",
      "Epoch: 3 Batch: 360 Log-loss: 0.05511704459786415\n",
      "Epoch: 3 Batch: 400 Log-loss: 0.0825372114777565\n",
      "Epoch: 3 Batch: 440 Log-loss: 0.05668455362319946\n",
      "Epoch: 3 Batch: 480 Log-loss: 0.061447564512491226\n",
      "Epoch: 3 Batch: 520 Log-loss: 0.04791472852230072\n",
      "Epoch: 3 Batch: 560 Log-loss: 0.04831860214471817\n",
      "Epoch average log-loss: 0.04981194123559232\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 3, val_loss: 0.0467049251236864, best_val_loss: 0.0467049251236864, best_auc: 0.9804208402616535\n",
      "Epoch: 4 Batch: 0 Log-loss: 0.041851311922073364\n",
      "Epoch: 4 Batch: 40 Log-loss: 0.05578024685382843\n",
      "Epoch: 4 Batch: 80 Log-loss: 0.04687991738319397\n",
      "Epoch: 4 Batch: 120 Log-loss: 0.04658907651901245\n",
      "Epoch: 4 Batch: 160 Log-loss: 0.05551958456635475\n",
      "Epoch: 4 Batch: 200 Log-loss: 0.04008612409234047\n",
      "Epoch: 4 Batch: 240 Log-loss: 0.03962232917547226\n",
      "Epoch: 4 Batch: 280 Log-loss: 0.04850172623991966\n",
      "Epoch: 4 Batch: 320 Log-loss: 0.05158388987183571\n",
      "Epoch: 4 Batch: 360 Log-loss: 0.04741598293185234\n",
      "Epoch: 4 Batch: 400 Log-loss: 0.041660238057374954\n",
      "Epoch: 4 Batch: 440 Log-loss: 0.04315483570098877\n",
      "Epoch: 4 Batch: 480 Log-loss: 0.04176958277821541\n",
      "Epoch: 4 Batch: 520 Log-loss: 0.04095982015132904\n",
      "Epoch: 4 Batch: 560 Log-loss: 0.06272374838590622\n",
      "Epoch average log-loss: 0.04740227666084788\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 4, val_loss: 0.044998212181431796, best_val_loss: 0.044998212181431796, best_auc: 0.9821361966366055\n",
      "Epoch: 5 Batch: 0 Log-loss: 0.044457077980041504\n",
      "Epoch: 5 Batch: 40 Log-loss: 0.03861522302031517\n",
      "Epoch: 5 Batch: 80 Log-loss: 0.06722524017095566\n",
      "Epoch: 5 Batch: 120 Log-loss: 0.037554506212472916\n",
      "Epoch: 5 Batch: 160 Log-loss: 0.037567153573036194\n",
      "Epoch: 5 Batch: 200 Log-loss: 0.044031184166669846\n",
      "Epoch: 5 Batch: 240 Log-loss: 0.05270940065383911\n",
      "Epoch: 5 Batch: 280 Log-loss: 0.052810296416282654\n",
      "Epoch: 5 Batch: 320 Log-loss: 0.045205697417259216\n",
      "Epoch: 5 Batch: 360 Log-loss: 0.039505962282419205\n",
      "Epoch: 5 Batch: 400 Log-loss: 0.045865077525377274\n",
      "Epoch: 5 Batch: 440 Log-loss: 0.05112792178988457\n",
      "Epoch: 5 Batch: 480 Log-loss: 0.04378734901547432\n",
      "Epoch: 5 Batch: 520 Log-loss: 0.03862922266125679\n",
      "Epoch: 5 Batch: 560 Log-loss: 0.07245074957609177\n",
      "Epoch average log-loss: 0.04632826417551509\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 5, val_loss: 0.04357058265538105, best_val_loss: 0.04357058265538105, best_auc: 0.9831608604036347\n",
      "Epoch: 6 Batch: 0 Log-loss: 0.04026620462536812\n",
      "Epoch: 6 Batch: 40 Log-loss: 0.04763669893145561\n",
      "Epoch: 6 Batch: 80 Log-loss: 0.028188148513436317\n",
      "Epoch: 6 Batch: 120 Log-loss: 0.032529499381780624\n",
      "Epoch: 6 Batch: 160 Log-loss: 0.0498795211315155\n",
      "Epoch: 6 Batch: 200 Log-loss: 0.040545184165239334\n",
      "Epoch: 6 Batch: 240 Log-loss: 0.037835437804460526\n",
      "Epoch: 6 Batch: 280 Log-loss: 0.05091596767306328\n",
      "Epoch: 6 Batch: 320 Log-loss: 0.03847280517220497\n",
      "Epoch: 6 Batch: 360 Log-loss: 0.038577936589717865\n",
      "Epoch: 6 Batch: 400 Log-loss: 0.047419410198926926\n",
      "Epoch: 6 Batch: 440 Log-loss: 0.051363855600357056\n",
      "Epoch: 6 Batch: 480 Log-loss: 0.03944265469908714\n",
      "Epoch: 6 Batch: 520 Log-loss: 0.046495918184518814\n",
      "Epoch: 6 Batch: 560 Log-loss: 0.03395044431090355\n",
      "Epoch average log-loss: 0.04493037405357297\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 6, val_loss: 0.04249631150926312, best_val_loss: 0.04249631150926312, best_auc: 0.9851628247703341\n",
      "Epoch: 7 Batch: 0 Log-loss: 0.0329965315759182\n",
      "Epoch: 7 Batch: 40 Log-loss: 0.0674540102481842\n",
      "Epoch: 7 Batch: 80 Log-loss: 0.04933510348200798\n",
      "Epoch: 7 Batch: 120 Log-loss: 0.07877106964588165\n",
      "Epoch: 7 Batch: 160 Log-loss: 0.053804051131010056\n",
      "Epoch: 7 Batch: 200 Log-loss: 0.04129733517765999\n",
      "Epoch: 7 Batch: 240 Log-loss: 0.04452047869563103\n",
      "Epoch: 7 Batch: 280 Log-loss: 0.04461178183555603\n",
      "Epoch: 7 Batch: 320 Log-loss: 0.05272967740893364\n",
      "Epoch: 7 Batch: 360 Log-loss: 0.06197747588157654\n",
      "Epoch: 7 Batch: 400 Log-loss: 0.05048171803355217\n",
      "Epoch: 7 Batch: 440 Log-loss: 0.03690645471215248\n",
      "Epoch: 7 Batch: 480 Log-loss: 0.038869328796863556\n",
      "Epoch: 7 Batch: 520 Log-loss: 0.053831666707992554\n",
      "Epoch: 7 Batch: 560 Log-loss: 0.05072655901312828\n",
      "Epoch average log-loss: 0.044128816574811935\n",
      "In Epoch: 7, val_loss: 0.042640280655673345, best_val_loss: 0.04249631150926312, best_auc: 0.9851628247703341\n",
      "Epoch: 8 Batch: 0 Log-loss: 0.04307861998677254\n",
      "Epoch: 8 Batch: 40 Log-loss: 0.032952308654785156\n",
      "Epoch: 8 Batch: 80 Log-loss: 0.04246347025036812\n",
      "Epoch: 8 Batch: 120 Log-loss: 0.025182755663990974\n",
      "Epoch: 8 Batch: 160 Log-loss: 0.059225887060165405\n",
      "Epoch: 8 Batch: 200 Log-loss: 0.039217639714479446\n",
      "Epoch: 8 Batch: 240 Log-loss: 0.053126584738492966\n",
      "Epoch: 8 Batch: 280 Log-loss: 0.047149449586868286\n",
      "Epoch: 8 Batch: 320 Log-loss: 0.04235462471842766\n",
      "Epoch: 8 Batch: 360 Log-loss: 0.03042592853307724\n",
      "Epoch: 8 Batch: 400 Log-loss: 0.043793659657239914\n",
      "Epoch: 8 Batch: 440 Log-loss: 0.04152442142367363\n",
      "Epoch: 8 Batch: 480 Log-loss: 0.06384073942899704\n",
      "Epoch: 8 Batch: 520 Log-loss: 0.038145218044519424\n",
      "Epoch: 8 Batch: 560 Log-loss: 0.057037174701690674\n",
      "Epoch average log-loss: 0.043799509766644665\n",
      "In Epoch: 8, val_loss: 0.04251601988623548, best_val_loss: 0.04249631150926312, best_auc: 0.9851628247703341\n",
      "Epoch: 9 Batch: 0 Log-loss: 0.044394005089998245\n",
      "Epoch: 9 Batch: 40 Log-loss: 0.0443744957447052\n",
      "Epoch: 9 Batch: 80 Log-loss: 0.043326277285814285\n",
      "Epoch: 9 Batch: 120 Log-loss: 0.03355097398161888\n",
      "Epoch: 9 Batch: 160 Log-loss: 0.06188607215881348\n",
      "Epoch: 9 Batch: 200 Log-loss: 0.037413936108350754\n",
      "Epoch: 9 Batch: 240 Log-loss: 0.05586673319339752\n",
      "Epoch: 9 Batch: 280 Log-loss: 0.04342387616634369\n",
      "Epoch: 9 Batch: 320 Log-loss: 0.046804457902908325\n",
      "Epoch: 9 Batch: 360 Log-loss: 0.04015708714723587\n",
      "Epoch: 9 Batch: 400 Log-loss: 0.05344352126121521\n",
      "Epoch: 9 Batch: 440 Log-loss: 0.03781713917851448\n",
      "Epoch: 9 Batch: 480 Log-loss: 0.07323412597179413\n",
      "Epoch: 9 Batch: 520 Log-loss: 0.03381563723087311\n",
      "Epoch: 9 Batch: 560 Log-loss: 0.040291838347911835\n",
      "Epoch average log-loss: 0.04311932023681168\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 9, val_loss: 0.04224367542480973, best_val_loss: 0.04224367542480973, best_auc: 0.9868869472502845\n",
      "Epoch: 10 Batch: 0 Log-loss: 0.043764401227235794\n",
      "Epoch: 10 Batch: 40 Log-loss: 0.04880471155047417\n",
      "Epoch: 10 Batch: 80 Log-loss: 0.045773088932037354\n",
      "Epoch: 10 Batch: 120 Log-loss: 0.03532588481903076\n",
      "Epoch: 10 Batch: 160 Log-loss: 0.038195084780454636\n",
      "Epoch: 10 Batch: 200 Log-loss: 0.032226767390966415\n",
      "Epoch: 10 Batch: 240 Log-loss: 0.04104648157954216\n",
      "Epoch: 10 Batch: 280 Log-loss: 0.05212156102061272\n",
      "Epoch: 10 Batch: 320 Log-loss: 0.03186839446425438\n",
      "Epoch: 10 Batch: 360 Log-loss: 0.026049256324768066\n",
      "Epoch: 10 Batch: 400 Log-loss: 0.0514255054295063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Batch: 440 Log-loss: 0.042495325207710266\n",
      "Epoch: 10 Batch: 480 Log-loss: 0.04940229654312134\n",
      "Epoch: 10 Batch: 520 Log-loss: 0.0287406574934721\n",
      "Epoch: 10 Batch: 560 Log-loss: 0.043612707406282425\n",
      "Epoch average log-loss: 0.0425050803859319\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 10, val_loss: 0.0421839625307055, best_val_loss: 0.0421839625307055, best_auc: 0.9863014890860726\n",
      "Epoch: 11 Batch: 0 Log-loss: 0.03817583620548248\n",
      "Epoch: 11 Batch: 40 Log-loss: 0.043250229209661484\n",
      "Epoch: 11 Batch: 80 Log-loss: 0.03536013141274452\n",
      "Epoch: 11 Batch: 120 Log-loss: 0.05493398383259773\n",
      "Epoch: 11 Batch: 160 Log-loss: 0.041810568422079086\n",
      "Epoch: 11 Batch: 200 Log-loss: 0.02901049517095089\n",
      "Epoch: 11 Batch: 240 Log-loss: 0.0365234799683094\n",
      "Epoch: 11 Batch: 280 Log-loss: 0.038357626646757126\n",
      "Epoch: 11 Batch: 320 Log-loss: 0.04665851220488548\n",
      "Epoch: 11 Batch: 360 Log-loss: 0.039293184876441956\n",
      "Epoch: 11 Batch: 400 Log-loss: 0.032901111990213394\n",
      "Epoch: 11 Batch: 440 Log-loss: 0.06271153688430786\n",
      "Epoch: 11 Batch: 480 Log-loss: 0.04616126790642738\n",
      "Epoch: 11 Batch: 520 Log-loss: 0.045287907123565674\n",
      "Epoch: 11 Batch: 560 Log-loss: 0.0418219268321991\n",
      "Epoch average log-loss: 0.042509607887560766\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 11, val_loss: 0.04136226191905378, best_val_loss: 0.04136226191905378, best_auc: 0.9863328760338543\n",
      "Epoch: 12 Batch: 0 Log-loss: 0.043776869773864746\n",
      "Epoch: 12 Batch: 40 Log-loss: 0.041154827922582626\n",
      "Epoch: 12 Batch: 80 Log-loss: 0.0412675216794014\n",
      "Epoch: 12 Batch: 120 Log-loss: 0.06281241029500961\n",
      "Epoch: 12 Batch: 160 Log-loss: 0.049888432025909424\n",
      "Epoch: 12 Batch: 200 Log-loss: 0.04595173895359039\n",
      "Epoch: 12 Batch: 240 Log-loss: 0.03136153891682625\n",
      "Epoch: 12 Batch: 280 Log-loss: 0.0653248205780983\n",
      "Epoch: 12 Batch: 320 Log-loss: 0.042393192648887634\n",
      "Epoch: 12 Batch: 360 Log-loss: 0.04488080367445946\n",
      "Epoch: 12 Batch: 400 Log-loss: 0.02826622687280178\n",
      "Epoch: 12 Batch: 440 Log-loss: 0.03636263683438301\n",
      "Epoch: 12 Batch: 480 Log-loss: 0.037921082228422165\n",
      "Epoch: 12 Batch: 520 Log-loss: 0.04641992226243019\n",
      "Epoch: 12 Batch: 560 Log-loss: 0.030517244711518288\n",
      "Epoch average log-loss: 0.042213033711803814\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 12, val_loss: 0.04087165802015081, best_val_loss: 0.04087165802015081, best_auc: 0.9886768601175829\n",
      "Epoch: 13 Batch: 0 Log-loss: 0.06399304419755936\n",
      "Epoch: 13 Batch: 40 Log-loss: 0.03636873513460159\n",
      "Epoch: 13 Batch: 80 Log-loss: 0.03586946427822113\n",
      "Epoch: 13 Batch: 120 Log-loss: 0.045957934111356735\n",
      "Epoch: 13 Batch: 160 Log-loss: 0.03530659154057503\n",
      "Epoch: 13 Batch: 200 Log-loss: 0.04289237782359123\n",
      "Epoch: 13 Batch: 240 Log-loss: 0.051689427345991135\n",
      "Epoch: 13 Batch: 280 Log-loss: 0.03689337894320488\n",
      "Epoch: 13 Batch: 320 Log-loss: 0.03819271922111511\n",
      "Epoch: 13 Batch: 360 Log-loss: 0.03939424455165863\n",
      "Epoch: 13 Batch: 400 Log-loss: 0.03336688131093979\n",
      "Epoch: 13 Batch: 440 Log-loss: 0.026957452297210693\n",
      "Epoch: 13 Batch: 480 Log-loss: 0.04657203331589699\n",
      "Epoch: 13 Batch: 520 Log-loss: 0.037176381796598434\n",
      "Epoch: 13 Batch: 560 Log-loss: 0.05287662893533707\n",
      "Epoch average log-loss: 0.04173954192415944\n",
      "In Epoch: 13, val_loss: 0.04109267841997763, best_val_loss: 0.04087165802015081, best_auc: 0.9886768601175829\n",
      "Epoch: 14 Batch: 0 Log-loss: 0.031174832955002785\n",
      "Epoch: 14 Batch: 40 Log-loss: 0.03464045748114586\n",
      "Epoch: 14 Batch: 80 Log-loss: 0.03598867356777191\n",
      "Epoch: 14 Batch: 120 Log-loss: 0.05354316905140877\n",
      "Epoch: 14 Batch: 160 Log-loss: 0.04724377766251564\n",
      "Epoch: 14 Batch: 200 Log-loss: 0.03578975051641464\n",
      "Epoch: 14 Batch: 240 Log-loss: 0.03586149215698242\n",
      "Epoch: 14 Batch: 280 Log-loss: 0.04019222781062126\n",
      "Epoch: 14 Batch: 320 Log-loss: 0.053238626569509506\n",
      "Epoch: 14 Batch: 360 Log-loss: 0.04178231954574585\n",
      "Epoch: 14 Batch: 400 Log-loss: 0.03242071717977524\n",
      "Epoch: 14 Batch: 440 Log-loss: 0.05118032172322273\n",
      "Epoch: 14 Batch: 480 Log-loss: 0.03955572843551636\n",
      "Epoch: 14 Batch: 520 Log-loss: 0.04382559284567833\n",
      "Epoch: 14 Batch: 560 Log-loss: 0.0285466518253088\n",
      "Epoch average log-loss: 0.04132108686358801\n",
      "In Epoch: 14, val_loss: 0.041093770679497675, best_val_loss: 0.04087165802015081, best_auc: 0.9886768601175829\n",
      "Epoch: 15 Batch: 0 Log-loss: 0.04888193681836128\n",
      "Epoch: 15 Batch: 40 Log-loss: 0.043367814272642136\n",
      "Epoch: 15 Batch: 80 Log-loss: 0.04419562220573425\n",
      "Epoch: 15 Batch: 120 Log-loss: 0.03733152896165848\n",
      "Epoch: 15 Batch: 160 Log-loss: 0.04088355973362923\n",
      "Epoch: 15 Batch: 200 Log-loss: 0.033296603709459305\n",
      "Epoch: 15 Batch: 240 Log-loss: 0.04463626816868782\n",
      "Epoch: 15 Batch: 280 Log-loss: 0.03834037110209465\n",
      "Epoch: 15 Batch: 320 Log-loss: 0.04797255992889404\n",
      "Epoch: 15 Batch: 360 Log-loss: 0.046209871768951416\n",
      "Epoch: 15 Batch: 400 Log-loss: 0.04338778927922249\n",
      "Epoch: 15 Batch: 440 Log-loss: 0.04017273336648941\n",
      "Epoch: 15 Batch: 480 Log-loss: 0.04625653848052025\n",
      "Epoch: 15 Batch: 520 Log-loss: 0.045062512159347534\n",
      "Epoch: 15 Batch: 560 Log-loss: 0.03315192088484764\n",
      "Epoch average log-loss: 0.04124054311708148\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 15, val_loss: 0.04086009885564687, best_val_loss: 0.04086009885564687, best_auc: 0.9885763340231488\n",
      "Epoch: 16 Batch: 0 Log-loss: 0.05307566747069359\n",
      "Epoch: 16 Batch: 40 Log-loss: 0.03937775641679764\n",
      "Epoch: 16 Batch: 80 Log-loss: 0.04417205974459648\n",
      "Epoch: 16 Batch: 120 Log-loss: 0.04578313231468201\n",
      "Epoch: 16 Batch: 160 Log-loss: 0.03438430652022362\n",
      "Epoch: 16 Batch: 200 Log-loss: 0.036172445863485336\n",
      "Epoch: 16 Batch: 240 Log-loss: 0.030965732410550117\n",
      "Epoch: 16 Batch: 280 Log-loss: 0.03860532492399216\n",
      "Epoch: 16 Batch: 320 Log-loss: 0.03352486714720726\n",
      "Epoch: 16 Batch: 360 Log-loss: 0.03193514421582222\n",
      "Epoch: 16 Batch: 400 Log-loss: 0.02624516934156418\n",
      "Epoch: 16 Batch: 440 Log-loss: 0.03196306154131889\n",
      "Epoch: 16 Batch: 480 Log-loss: 0.04512539505958557\n",
      "Epoch: 16 Batch: 520 Log-loss: 0.05048024281859398\n",
      "Epoch: 16 Batch: 560 Log-loss: 0.03858096897602081\n",
      "Epoch average log-loss: 0.041027992624523384\n",
      "In Epoch: 16, val_loss: 0.04095363958504563, best_val_loss: 0.04086009885564687, best_auc: 0.9885763340231488\n",
      "Epoch: 17 Batch: 0 Log-loss: 0.03738793358206749\n",
      "Epoch: 17 Batch: 40 Log-loss: 0.039580319076776505\n",
      "Epoch: 17 Batch: 80 Log-loss: 0.03215014562010765\n",
      "Epoch: 17 Batch: 120 Log-loss: 0.03772788122296333\n",
      "Epoch: 17 Batch: 160 Log-loss: 0.03564263880252838\n",
      "Epoch: 17 Batch: 200 Log-loss: 0.03884749487042427\n",
      "Epoch: 17 Batch: 240 Log-loss: 0.044110849499702454\n",
      "Epoch: 17 Batch: 280 Log-loss: 0.047593940049409866\n",
      "Epoch: 17 Batch: 320 Log-loss: 0.0470569021999836\n",
      "Epoch: 17 Batch: 360 Log-loss: 0.028965028002858162\n",
      "Epoch: 17 Batch: 400 Log-loss: 0.023093437775969505\n",
      "Epoch: 17 Batch: 440 Log-loss: 0.0433407723903656\n",
      "Epoch: 17 Batch: 480 Log-loss: 0.03770526498556137\n",
      "Epoch: 17 Batch: 520 Log-loss: 0.05018685385584831\n",
      "Epoch: 17 Batch: 560 Log-loss: 0.03401234745979309\n",
      "Epoch average log-loss: 0.04093129629202719\n",
      "In Epoch: 17, val_loss: 0.04112456554733026, best_val_loss: 0.04086009885564687, best_auc: 0.9885763340231488\n",
      "Epoch: 18 Batch: 0 Log-loss: 0.04370309039950371\n",
      "Epoch: 18 Batch: 40 Log-loss: 0.06600639969110489\n",
      "Epoch: 18 Batch: 80 Log-loss: 0.04442066326737404\n",
      "Epoch: 18 Batch: 120 Log-loss: 0.03065466322004795\n",
      "Epoch: 18 Batch: 160 Log-loss: 0.044094715267419815\n",
      "Epoch: 18 Batch: 200 Log-loss: 0.0445157028734684\n",
      "Epoch: 18 Batch: 240 Log-loss: 0.0456567145884037\n",
      "Epoch: 18 Batch: 280 Log-loss: 0.03930327296257019\n",
      "Epoch: 18 Batch: 320 Log-loss: 0.03844438120722771\n",
      "Epoch: 18 Batch: 360 Log-loss: 0.05324072763323784\n",
      "Epoch: 18 Batch: 400 Log-loss: 0.054815873503685\n",
      "Epoch: 18 Batch: 440 Log-loss: 0.050494421273469925\n",
      "Epoch: 18 Batch: 480 Log-loss: 0.047969479113817215\n",
      "Epoch: 18 Batch: 520 Log-loss: 0.024596871808171272\n",
      "Epoch: 18 Batch: 560 Log-loss: 0.039253901690244675\n",
      "Epoch average log-loss: 0.04071533733180591\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 18, val_loss: 0.04052805196964017, best_val_loss: 0.04052805196964017, best_auc: 0.9885207688225965\n",
      "Epoch: 19 Batch: 0 Log-loss: 0.04027976840734482\n",
      "Epoch: 19 Batch: 40 Log-loss: 0.05895349755883217\n",
      "Epoch: 19 Batch: 80 Log-loss: 0.03241998702287674\n",
      "Epoch: 19 Batch: 120 Log-loss: 0.034534238278865814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 Batch: 160 Log-loss: 0.03569536656141281\n",
      "Epoch: 19 Batch: 200 Log-loss: 0.036494556814432144\n",
      "Epoch: 19 Batch: 240 Log-loss: 0.029886886477470398\n",
      "Epoch: 19 Batch: 280 Log-loss: 0.049888353794813156\n",
      "Epoch: 19 Batch: 320 Log-loss: 0.03292352706193924\n",
      "Epoch: 19 Batch: 360 Log-loss: 0.04140729084610939\n",
      "Epoch: 19 Batch: 400 Log-loss: 0.05428403615951538\n",
      "Epoch: 19 Batch: 440 Log-loss: 0.050015199929475784\n",
      "Epoch: 19 Batch: 480 Log-loss: 0.030075551941990852\n",
      "Epoch: 19 Batch: 520 Log-loss: 0.05124678090214729\n",
      "Epoch: 19 Batch: 560 Log-loss: 0.041234418749809265\n",
      "Epoch average log-loss: 0.04058452345364328\n",
      "In Epoch: 19, val_loss: 0.041257631911711536, best_val_loss: 0.04052805196964017, best_auc: 0.9885207688225965\n",
      "Epoch: 20 Batch: 0 Log-loss: 0.06254526227712631\n",
      "Epoch: 20 Batch: 40 Log-loss: 0.04560096934437752\n",
      "Epoch: 20 Batch: 80 Log-loss: 0.027760453522205353\n",
      "Epoch: 20 Batch: 120 Log-loss: 0.04748222604393959\n",
      "Epoch: 20 Batch: 160 Log-loss: 0.029363468289375305\n",
      "Epoch: 20 Batch: 200 Log-loss: 0.04881390929222107\n",
      "Epoch: 20 Batch: 240 Log-loss: 0.044410835951566696\n",
      "Epoch: 20 Batch: 280 Log-loss: 0.04555916786193848\n",
      "Epoch: 20 Batch: 320 Log-loss: 0.06353691965341568\n",
      "Epoch: 20 Batch: 360 Log-loss: 0.027758972719311714\n",
      "Epoch: 20 Batch: 400 Log-loss: 0.04250134900212288\n",
      "Epoch: 20 Batch: 440 Log-loss: 0.061213184148073196\n",
      "Epoch: 20 Batch: 480 Log-loss: 0.03202517703175545\n",
      "Epoch: 20 Batch: 520 Log-loss: 0.0431547611951828\n",
      "Epoch: 20 Batch: 560 Log-loss: 0.04384154453873634\n",
      "Epoch average log-loss: 0.040496299020014705\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 20, val_loss: 0.040401130614967584, best_val_loss: 0.040401130614967584, best_auc: 0.9890172248723365\n",
      "Epoch: 21 Batch: 0 Log-loss: 0.046269599348306656\n",
      "Epoch: 21 Batch: 40 Log-loss: 0.04120785370469093\n",
      "Epoch: 21 Batch: 80 Log-loss: 0.03594347834587097\n",
      "Epoch: 21 Batch: 120 Log-loss: 0.05168675258755684\n",
      "Epoch: 21 Batch: 160 Log-loss: 0.05025497078895569\n",
      "Epoch: 21 Batch: 200 Log-loss: 0.040108587592840195\n",
      "Epoch: 21 Batch: 240 Log-loss: 0.04577380791306496\n",
      "Epoch: 21 Batch: 280 Log-loss: 0.04879175126552582\n",
      "Epoch: 21 Batch: 320 Log-loss: 0.03713030368089676\n",
      "Epoch: 21 Batch: 360 Log-loss: 0.03974838927388191\n",
      "Epoch: 21 Batch: 400 Log-loss: 0.04631364718079567\n",
      "Epoch: 21 Batch: 440 Log-loss: 0.043977025896310806\n",
      "Epoch: 21 Batch: 480 Log-loss: 0.04368598386645317\n",
      "Epoch: 21 Batch: 520 Log-loss: 0.034511763602495193\n",
      "Epoch: 21 Batch: 560 Log-loss: 0.04239031299948692\n",
      "Epoch average log-loss: 0.040233972375946385\n",
      "In Epoch: 21, val_loss: 0.04069668444271948, best_val_loss: 0.040401130614967584, best_auc: 0.9890172248723365\n",
      "Epoch: 22 Batch: 0 Log-loss: 0.04158573970198631\n",
      "Epoch: 22 Batch: 40 Log-loss: 0.028155209496617317\n",
      "Epoch: 22 Batch: 80 Log-loss: 0.03705273196101189\n",
      "Epoch: 22 Batch: 120 Log-loss: 0.04576250538229942\n",
      "Epoch: 22 Batch: 160 Log-loss: 0.04525326192378998\n",
      "Epoch: 22 Batch: 200 Log-loss: 0.03375636786222458\n",
      "Epoch: 22 Batch: 240 Log-loss: 0.038288604468107224\n",
      "Epoch: 22 Batch: 280 Log-loss: 0.0339861698448658\n",
      "Epoch: 22 Batch: 320 Log-loss: 0.029762668535113335\n",
      "Epoch: 22 Batch: 360 Log-loss: 0.05937536060810089\n",
      "Epoch: 22 Batch: 400 Log-loss: 0.04544820263981819\n",
      "Epoch: 22 Batch: 440 Log-loss: 0.04335569217801094\n",
      "Epoch: 22 Batch: 480 Log-loss: 0.035627420991659164\n",
      "Epoch: 22 Batch: 520 Log-loss: 0.02961253933608532\n",
      "Epoch: 22 Batch: 560 Log-loss: 0.039543021470308304\n",
      "Epoch average log-loss: 0.04023256705009511\n",
      "In Epoch: 22, val_loss: 0.0405320717059264, best_val_loss: 0.040401130614967584, best_auc: 0.9890172248723365\n",
      "Epoch: 23 Batch: 0 Log-loss: 0.0561753511428833\n",
      "Epoch: 23 Batch: 40 Log-loss: 0.034438226372003555\n",
      "Epoch: 23 Batch: 80 Log-loss: 0.03422427549958229\n",
      "Epoch: 23 Batch: 120 Log-loss: 0.029652738943696022\n",
      "Epoch: 23 Batch: 160 Log-loss: 0.04109584912657738\n",
      "Epoch: 23 Batch: 200 Log-loss: 0.040673837065696716\n",
      "Epoch: 23 Batch: 240 Log-loss: 0.030912145972251892\n",
      "Epoch: 23 Batch: 280 Log-loss: 0.03232299163937569\n",
      "Epoch: 23 Batch: 320 Log-loss: 0.04135473445057869\n",
      "Epoch: 23 Batch: 360 Log-loss: 0.05631938576698303\n",
      "Epoch: 23 Batch: 400 Log-loss: 0.03446834906935692\n",
      "Epoch: 23 Batch: 440 Log-loss: 0.03482292965054512\n",
      "Epoch: 23 Batch: 480 Log-loss: 0.041753899306058884\n",
      "Epoch: 23 Batch: 520 Log-loss: 0.03708608075976372\n",
      "Epoch: 23 Batch: 560 Log-loss: 0.04775660112500191\n",
      "Epoch average log-loss: 0.040155795868486166\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 23, val_loss: 0.04020726170346849, best_val_loss: 0.04020726170346849, best_auc: 0.9886804486757409\n",
      "Epoch: 24 Batch: 0 Log-loss: 0.030517226085066795\n",
      "Epoch: 24 Batch: 40 Log-loss: 0.058383453637361526\n",
      "Epoch: 24 Batch: 80 Log-loss: 0.023126840591430664\n",
      "Epoch: 24 Batch: 120 Log-loss: 0.029350504279136658\n",
      "Epoch: 24 Batch: 160 Log-loss: 0.03480681777000427\n",
      "Epoch: 24 Batch: 200 Log-loss: 0.03498421236872673\n",
      "Epoch: 24 Batch: 240 Log-loss: 0.03446577861905098\n",
      "Epoch: 24 Batch: 280 Log-loss: 0.03608617186546326\n",
      "Epoch: 24 Batch: 320 Log-loss: 0.02620510943233967\n",
      "Epoch: 24 Batch: 360 Log-loss: 0.03628705069422722\n",
      "Epoch: 24 Batch: 400 Log-loss: 0.04806901142001152\n",
      "Epoch: 24 Batch: 440 Log-loss: 0.04113147780299187\n",
      "Epoch: 24 Batch: 480 Log-loss: 0.03964316099882126\n",
      "Epoch: 24 Batch: 520 Log-loss: 0.036579713225364685\n",
      "Epoch: 24 Batch: 560 Log-loss: 0.04784032329916954\n",
      "Epoch average log-loss: 0.03984908707705992\n",
      "In Epoch: 24, val_loss: 0.040440099857044214, best_val_loss: 0.04020726170346849, best_auc: 0.9886804486757409\n",
      "Epoch: 25 Batch: 0 Log-loss: 0.043684136122465134\n",
      "Epoch: 25 Batch: 40 Log-loss: 0.029612669721245766\n",
      "Epoch: 25 Batch: 80 Log-loss: 0.028015343472361565\n",
      "Epoch: 25 Batch: 120 Log-loss: 0.03883608430624008\n",
      "Epoch: 25 Batch: 160 Log-loss: 0.05743288993835449\n",
      "Epoch: 25 Batch: 200 Log-loss: 0.031792402267456055\n",
      "Epoch: 25 Batch: 240 Log-loss: 0.0386919304728508\n",
      "Epoch: 25 Batch: 280 Log-loss: 0.05454779788851738\n",
      "Epoch: 25 Batch: 320 Log-loss: 0.028457602486014366\n",
      "Epoch: 25 Batch: 360 Log-loss: 0.04849165678024292\n",
      "Epoch: 25 Batch: 400 Log-loss: 0.036398161202669144\n",
      "Epoch: 25 Batch: 440 Log-loss: 0.02735316753387451\n",
      "Epoch: 25 Batch: 480 Log-loss: 0.044660523533821106\n",
      "Epoch: 25 Batch: 520 Log-loss: 0.03373448923230171\n",
      "Epoch: 25 Batch: 560 Log-loss: 0.03917992115020752\n",
      "Epoch average log-loss: 0.039800799594792935\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 25, val_loss: 0.040072344823061996, best_val_loss: 0.040072344823061996, best_auc: 0.9889285339485134\n",
      "Epoch: 26 Batch: 0 Log-loss: 0.029565801844000816\n",
      "Epoch: 26 Batch: 40 Log-loss: 0.05812191590666771\n",
      "Epoch: 26 Batch: 80 Log-loss: 0.05246405676007271\n",
      "Epoch: 26 Batch: 120 Log-loss: 0.034541767090559006\n",
      "Epoch: 26 Batch: 160 Log-loss: 0.027617983520030975\n",
      "Epoch: 26 Batch: 200 Log-loss: 0.050467658787965775\n",
      "Epoch: 26 Batch: 240 Log-loss: 0.03634253144264221\n",
      "Epoch: 26 Batch: 280 Log-loss: 0.03331625461578369\n",
      "Epoch: 26 Batch: 320 Log-loss: 0.04059232398867607\n",
      "Epoch: 26 Batch: 360 Log-loss: 0.03863219544291496\n",
      "Epoch: 26 Batch: 400 Log-loss: 0.04782838746905327\n",
      "Epoch: 26 Batch: 440 Log-loss: 0.0396185927093029\n",
      "Epoch: 26 Batch: 480 Log-loss: 0.05256746709346771\n",
      "Epoch: 26 Batch: 520 Log-loss: 0.041118551045656204\n",
      "Epoch: 26 Batch: 560 Log-loss: 0.043013740330934525\n",
      "Epoch average log-loss: 0.03976456300754632\n",
      "In Epoch: 26, val_loss: 0.04041790279717211, best_val_loss: 0.040072344823061996, best_auc: 0.9889285339485134\n",
      "Epoch: 27 Batch: 0 Log-loss: 0.042355168610811234\n",
      "Epoch: 27 Batch: 40 Log-loss: 0.03877517953515053\n",
      "Epoch: 27 Batch: 80 Log-loss: 0.04368864372372627\n",
      "Epoch: 27 Batch: 120 Log-loss: 0.03545786812901497\n",
      "Epoch: 27 Batch: 160 Log-loss: 0.045948222279548645\n",
      "Epoch: 27 Batch: 200 Log-loss: 0.03885827958583832\n",
      "Epoch: 27 Batch: 240 Log-loss: 0.03851250186562538\n",
      "Epoch: 27 Batch: 280 Log-loss: 0.039726097136735916\n",
      "Epoch: 27 Batch: 320 Log-loss: 0.03393615409731865\n",
      "Epoch: 27 Batch: 360 Log-loss: 0.035275235772132874\n",
      "Epoch: 27 Batch: 400 Log-loss: 0.027151377871632576\n",
      "Epoch: 27 Batch: 440 Log-loss: 0.035142265260219574\n",
      "Epoch: 27 Batch: 480 Log-loss: 0.044956009835004807\n",
      "Epoch: 27 Batch: 520 Log-loss: 0.04322638735175133\n",
      "Epoch: 27 Batch: 560 Log-loss: 0.0426909476518631\n",
      "Epoch average log-loss: 0.039845823905696826\n",
      "In Epoch: 27, val_loss: 0.040888112304919434, best_val_loss: 0.040072344823061996, best_auc: 0.9889285339485134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 Batch: 0 Log-loss: 0.04569888114929199\n",
      "Epoch: 28 Batch: 40 Log-loss: 0.03954225033521652\n",
      "Epoch: 28 Batch: 80 Log-loss: 0.03710247576236725\n",
      "Epoch: 28 Batch: 120 Log-loss: 0.0352601520717144\n",
      "Epoch: 28 Batch: 160 Log-loss: 0.048035044223070145\n",
      "Epoch: 28 Batch: 200 Log-loss: 0.026302969083189964\n",
      "Epoch: 28 Batch: 240 Log-loss: 0.048927996307611465\n",
      "Epoch: 28 Batch: 280 Log-loss: 0.027175001800060272\n",
      "Epoch: 28 Batch: 320 Log-loss: 0.04799836501479149\n",
      "Epoch: 28 Batch: 360 Log-loss: 0.039019979536533356\n",
      "Epoch: 28 Batch: 400 Log-loss: 0.04688639938831329\n",
      "Epoch: 28 Batch: 440 Log-loss: 0.046519070863723755\n",
      "Epoch: 28 Batch: 480 Log-loss: 0.03836689889431\n",
      "Epoch: 28 Batch: 520 Log-loss: 0.058969441801309586\n",
      "Epoch: 28 Batch: 560 Log-loss: 0.03842558711767197\n",
      "Epoch average log-loss: 0.03957701981001135\n",
      "In Epoch: 28, val_loss: 0.040125366491837226, best_val_loss: 0.040072344823061996, best_auc: 0.9889285339485134\n",
      "Epoch: 29 Batch: 0 Log-loss: 0.04343480244278908\n",
      "Epoch: 29 Batch: 40 Log-loss: 0.035980939865112305\n",
      "Epoch: 29 Batch: 80 Log-loss: 0.05150943621993065\n",
      "Epoch: 29 Batch: 120 Log-loss: 0.02791551500558853\n",
      "Epoch: 29 Batch: 160 Log-loss: 0.04608244076371193\n",
      "Epoch: 29 Batch: 200 Log-loss: 0.029513897374272346\n",
      "Epoch: 29 Batch: 240 Log-loss: 0.04424846172332764\n",
      "Epoch: 29 Batch: 280 Log-loss: 0.04603525996208191\n",
      "Epoch: 29 Batch: 320 Log-loss: 0.046161990612745285\n",
      "Epoch: 29 Batch: 360 Log-loss: 0.040452148765325546\n",
      "Epoch: 29 Batch: 400 Log-loss: 0.054868560284376144\n",
      "Epoch: 29 Batch: 440 Log-loss: 0.044338658452034\n",
      "Epoch: 29 Batch: 480 Log-loss: 0.02805667370557785\n",
      "Epoch: 29 Batch: 520 Log-loss: 0.03368791565299034\n",
      "Epoch: 29 Batch: 560 Log-loss: 0.05384034663438797\n",
      "Epoch average log-loss: 0.03952502322915409\n",
      "In Epoch: 29, val_loss: 0.040671458401506044, best_val_loss: 0.040072344823061996, best_auc: 0.9889285339485134\n",
      "Epoch: 30 Batch: 0 Log-loss: 0.037236202508211136\n",
      "Epoch: 30 Batch: 40 Log-loss: 0.049253497272729874\n",
      "Epoch: 30 Batch: 80 Log-loss: 0.04822194576263428\n",
      "Epoch: 30 Batch: 120 Log-loss: 0.02648100256919861\n",
      "Epoch: 30 Batch: 160 Log-loss: 0.03219758719205856\n",
      "Epoch: 30 Batch: 200 Log-loss: 0.029026908800005913\n",
      "Epoch: 30 Batch: 240 Log-loss: 0.028420357033610344\n",
      "Epoch: 30 Batch: 280 Log-loss: 0.03626507148146629\n",
      "Epoch: 30 Batch: 320 Log-loss: 0.054114412516355515\n",
      "Epoch: 30 Batch: 360 Log-loss: 0.04865405336022377\n",
      "Epoch: 30 Batch: 400 Log-loss: 0.03835327550768852\n",
      "Epoch: 30 Batch: 440 Log-loss: 0.039821699261665344\n",
      "Epoch: 30 Batch: 480 Log-loss: 0.030523011460900307\n",
      "Epoch: 30 Batch: 520 Log-loss: 0.04304872825741768\n",
      "Epoch: 30 Batch: 560 Log-loss: 0.038436535745859146\n",
      "Epoch average log-loss: 0.03942252184996115\n",
      "In Epoch: 30, val_loss: 0.04068950568899019, best_val_loss: 0.040072344823061996, best_auc: 0.9889285339485134\n",
      "Epoch: 31 Batch: 0 Log-loss: 0.05248093232512474\n",
      "Epoch: 31 Batch: 40 Log-loss: 0.03149985894560814\n",
      "Epoch: 31 Batch: 80 Log-loss: 0.03055887669324875\n",
      "Epoch: 31 Batch: 120 Log-loss: 0.03658617287874222\n",
      "Epoch: 31 Batch: 160 Log-loss: 0.05049057677388191\n",
      "Epoch: 31 Batch: 200 Log-loss: 0.031054377555847168\n",
      "Epoch: 31 Batch: 240 Log-loss: 0.031423237174749374\n",
      "Epoch: 31 Batch: 280 Log-loss: 0.05169318616390228\n",
      "Epoch: 31 Batch: 320 Log-loss: 0.033916350454092026\n",
      "Epoch: 31 Batch: 360 Log-loss: 0.031997960060834885\n",
      "Epoch: 31 Batch: 400 Log-loss: 0.04395688697695732\n",
      "Epoch: 31 Batch: 440 Log-loss: 0.026881368830800056\n",
      "Epoch: 31 Batch: 480 Log-loss: 0.035465873777866364\n",
      "Epoch: 31 Batch: 520 Log-loss: 0.03538117557764053\n",
      "Epoch: 31 Batch: 560 Log-loss: 0.038278207182884216\n",
      "Epoch average log-loss: 0.039339337298380475\n",
      "In Epoch: 31, val_loss: 0.04031057990320092, best_val_loss: 0.040072344823061996, best_auc: 0.9889285339485134\n",
      "Epoch: 32 Batch: 0 Log-loss: 0.03763003274798393\n",
      "Epoch: 32 Batch: 40 Log-loss: 0.027808288112282753\n",
      "Epoch: 32 Batch: 80 Log-loss: 0.0405537374317646\n",
      "Epoch: 32 Batch: 120 Log-loss: 0.040877774357795715\n",
      "Epoch: 32 Batch: 160 Log-loss: 0.045584384351968765\n",
      "Epoch: 32 Batch: 200 Log-loss: 0.054546017199754715\n",
      "Epoch: 32 Batch: 240 Log-loss: 0.03919122368097305\n",
      "Epoch: 32 Batch: 280 Log-loss: 0.04585198685526848\n",
      "Epoch: 32 Batch: 320 Log-loss: 0.046590108424425125\n",
      "Epoch: 32 Batch: 360 Log-loss: 0.042483195662498474\n",
      "Epoch: 32 Batch: 400 Log-loss: 0.032820332795381546\n",
      "Epoch: 32 Batch: 440 Log-loss: 0.03480980545282364\n",
      "Epoch: 32 Batch: 480 Log-loss: 0.04340596869587898\n",
      "Epoch: 32 Batch: 520 Log-loss: 0.03571367263793945\n",
      "Epoch: 32 Batch: 560 Log-loss: 0.033408332616090775\n",
      "Epoch average log-loss: 0.03930211810927306\n",
      "In Epoch: 32, val_loss: 0.04052500844014547, best_val_loss: 0.040072344823061996, best_auc: 0.9889285339485134\n",
      "Epoch: 33 Batch: 0 Log-loss: 0.03442782536149025\n",
      "Epoch: 33 Batch: 40 Log-loss: 0.03655193746089935\n",
      "Epoch: 33 Batch: 80 Log-loss: 0.038828298449516296\n",
      "Epoch: 33 Batch: 120 Log-loss: 0.056669026613235474\n",
      "Epoch: 33 Batch: 160 Log-loss: 0.04178549349308014\n",
      "Epoch: 33 Batch: 200 Log-loss: 0.03125922754406929\n",
      "Epoch: 33 Batch: 240 Log-loss: 0.03391364589333534\n",
      "Epoch: 33 Batch: 280 Log-loss: 0.035909321159124374\n",
      "Epoch: 33 Batch: 320 Log-loss: 0.03343560919165611\n",
      "Epoch: 33 Batch: 360 Log-loss: 0.043774351477622986\n",
      "Epoch: 33 Batch: 400 Log-loss: 0.04019660875201225\n",
      "Epoch: 33 Batch: 440 Log-loss: 0.03393161669373512\n",
      "Epoch: 33 Batch: 480 Log-loss: 0.02824465185403824\n",
      "Epoch: 33 Batch: 520 Log-loss: 0.04736129939556122\n",
      "Epoch: 33 Batch: 560 Log-loss: 0.03363124281167984\n",
      "Epoch average log-loss: 0.03941650157129126\n",
      "In Epoch: 33, val_loss: 0.04068979845022775, best_val_loss: 0.040072344823061996, best_auc: 0.9889285339485134\n",
      "Epoch: 34 Batch: 0 Log-loss: 0.043023038655519485\n",
      "Epoch: 34 Batch: 40 Log-loss: 0.0485699325799942\n",
      "Epoch: 34 Batch: 80 Log-loss: 0.04755350574851036\n",
      "Epoch: 34 Batch: 120 Log-loss: 0.04578472301363945\n",
      "Epoch: 34 Batch: 160 Log-loss: 0.05579289793968201\n",
      "Epoch: 34 Batch: 200 Log-loss: 0.03276622295379639\n",
      "Epoch: 34 Batch: 240 Log-loss: 0.030904358252882957\n",
      "Epoch: 34 Batch: 280 Log-loss: 0.02822343446314335\n",
      "Epoch: 34 Batch: 320 Log-loss: 0.03498244285583496\n",
      "Epoch: 34 Batch: 360 Log-loss: 0.0576535202562809\n",
      "Epoch: 34 Batch: 400 Log-loss: 0.040974948555231094\n",
      "Epoch: 34 Batch: 440 Log-loss: 0.031966716051101685\n",
      "Epoch: 34 Batch: 480 Log-loss: 0.048683881759643555\n",
      "Epoch: 34 Batch: 520 Log-loss: 0.03664671257138252\n",
      "Epoch: 34 Batch: 560 Log-loss: 0.0436222180724144\n",
      "Epoch average log-loss: 0.03918156381031232\n",
      "In Epoch: 34, val_loss: 0.040498236316481216, best_val_loss: 0.040072344823061996, best_auc: 0.9889285339485134\n",
      "Epoch: 35 Batch: 0 Log-loss: 0.03593472018837929\n",
      "Epoch: 35 Batch: 40 Log-loss: 0.04797651246190071\n",
      "Epoch: 35 Batch: 80 Log-loss: 0.04139604791998863\n",
      "Epoch: 35 Batch: 120 Log-loss: 0.03660426661372185\n",
      "Epoch: 35 Batch: 160 Log-loss: 0.05019719526171684\n",
      "Epoch: 35 Batch: 200 Log-loss: 0.04523949697613716\n",
      "Epoch: 35 Batch: 240 Log-loss: 0.03426326811313629\n",
      "Epoch: 35 Batch: 280 Log-loss: 0.03151911869645119\n",
      "Epoch: 35 Batch: 320 Log-loss: 0.03230490908026695\n",
      "Epoch: 35 Batch: 360 Log-loss: 0.03517719730734825\n",
      "Epoch: 35 Batch: 400 Log-loss: 0.04347562789916992\n",
      "Epoch: 35 Batch: 440 Log-loss: 0.028132202103734016\n",
      "Epoch: 35 Batch: 480 Log-loss: 0.05055664852261543\n",
      "Epoch: 35 Batch: 520 Log-loss: 0.03311307728290558\n",
      "Epoch: 35 Batch: 560 Log-loss: 0.03397811949253082\n",
      "Epoch average log-loss: 0.039193983801773614\n",
      "In Epoch: 35, val_loss: 0.04016790464947884, best_val_loss: 0.040072344823061996, best_auc: 0.9889285339485134\n",
      "Epoch: 36 Batch: 0 Log-loss: 0.03839588165283203\n",
      "Epoch: 36 Batch: 40 Log-loss: 0.034083444625139236\n",
      "Epoch: 36 Batch: 80 Log-loss: 0.03820078447461128\n",
      "Epoch: 36 Batch: 120 Log-loss: 0.04227544367313385\n",
      "Epoch: 36 Batch: 160 Log-loss: 0.03620021045207977\n",
      "Epoch: 36 Batch: 200 Log-loss: 0.04089311137795448\n",
      "Epoch: 36 Batch: 240 Log-loss: 0.03723504766821861\n",
      "Epoch: 36 Batch: 280 Log-loss: 0.0356903150677681\n",
      "Epoch: 36 Batch: 320 Log-loss: 0.047731880098581314\n",
      "Epoch: 36 Batch: 360 Log-loss: 0.033357929438352585\n",
      "Epoch: 36 Batch: 400 Log-loss: 0.04989026486873627\n",
      "Epoch: 36 Batch: 440 Log-loss: 0.034613776952028275\n",
      "Epoch: 36 Batch: 480 Log-loss: 0.03663237765431404\n",
      "Epoch: 36 Batch: 520 Log-loss: 0.044909585267305374\n",
      "Epoch: 36 Batch: 560 Log-loss: 0.05551201105117798\n",
      "Epoch average log-loss: 0.03912004016871963\n",
      "In Epoch: 36, val_loss: 0.040484783634566736, best_val_loss: 0.040072344823061996, best_auc: 0.9889285339485134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 Batch: 0 Log-loss: 0.025943731889128685\n",
      "Epoch: 37 Batch: 40 Log-loss: 0.03225911781191826\n",
      "Epoch: 37 Batch: 80 Log-loss: 0.04560138285160065\n",
      "Epoch: 37 Batch: 120 Log-loss: 0.041854213923215866\n",
      "Epoch: 37 Batch: 160 Log-loss: 0.04141495004296303\n",
      "Epoch: 37 Batch: 200 Log-loss: 0.05910206213593483\n",
      "Epoch: 37 Batch: 240 Log-loss: 0.02977190911769867\n",
      "Epoch: 37 Batch: 280 Log-loss: 0.03059864602982998\n",
      "Epoch: 37 Batch: 320 Log-loss: 0.03570556640625\n",
      "Epoch: 37 Batch: 360 Log-loss: 0.04414568841457367\n",
      "Epoch: 37 Batch: 400 Log-loss: 0.059603411704301834\n",
      "Epoch: 37 Batch: 440 Log-loss: 0.04640400782227516\n",
      "Epoch: 37 Batch: 480 Log-loss: 0.0462687723338604\n",
      "Epoch: 37 Batch: 520 Log-loss: 0.03475667163729668\n",
      "Epoch: 37 Batch: 560 Log-loss: 0.026855910196900368\n",
      "Epoch average log-loss: 0.039234114644516795\n",
      "A pre-trained model at model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt has been loaded.\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn8.pt.\n",
      "\n",
      "Choose the torch base model.\n",
      "## Training on fold 9 ##\n",
      "Epoch: 1 Batch: 0 Log-loss: 0.6809341311454773\n",
      "Epoch: 1 Batch: 40 Log-loss: 0.19358427822589874\n",
      "Epoch: 1 Batch: 80 Log-loss: 0.08834504336118698\n",
      "Epoch: 1 Batch: 120 Log-loss: 0.0761895477771759\n",
      "Epoch: 1 Batch: 160 Log-loss: 0.08411094546318054\n",
      "Epoch: 1 Batch: 200 Log-loss: 0.04807708039879799\n",
      "Epoch: 1 Batch: 240 Log-loss: 0.05451550707221031\n",
      "Epoch: 1 Batch: 280 Log-loss: 0.054499831050634384\n",
      "Epoch: 1 Batch: 320 Log-loss: 0.05516497790813446\n",
      "Epoch: 1 Batch: 360 Log-loss: 0.05030131712555885\n",
      "Epoch: 1 Batch: 400 Log-loss: 0.03170706704258919\n",
      "Epoch: 1 Batch: 440 Log-loss: 0.057965993881225586\n",
      "Epoch: 1 Batch: 480 Log-loss: 0.04698916897177696\n",
      "Epoch: 1 Batch: 520 Log-loss: 0.047198619693517685\n",
      "Epoch: 1 Batch: 560 Log-loss: 0.06483469158411026\n",
      "Epoch average log-loss: 0.08203362898368921\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 1, val_loss: 0.05349917150196115, best_val_loss: 0.05349917150196115, best_auc: 0.9676653839442567\n",
      "Epoch: 2 Batch: 0 Log-loss: 0.05143869295716286\n",
      "Epoch: 2 Batch: 40 Log-loss: 0.06066445633769035\n",
      "Epoch: 2 Batch: 80 Log-loss: 0.04217210039496422\n",
      "Epoch: 2 Batch: 120 Log-loss: 0.05439459905028343\n",
      "Epoch: 2 Batch: 160 Log-loss: 0.04731711372733116\n",
      "Epoch: 2 Batch: 200 Log-loss: 0.0484626404941082\n",
      "Epoch: 2 Batch: 240 Log-loss: 0.053592029958963394\n",
      "Epoch: 2 Batch: 280 Log-loss: 0.06581433862447739\n",
      "Epoch: 2 Batch: 320 Log-loss: 0.047309160232543945\n",
      "Epoch: 2 Batch: 360 Log-loss: 0.04691474512219429\n",
      "Epoch: 2 Batch: 400 Log-loss: 0.04818001016974449\n",
      "Epoch: 2 Batch: 440 Log-loss: 0.04474135860800743\n",
      "Epoch: 2 Batch: 480 Log-loss: 0.06536506861448288\n",
      "Epoch: 2 Batch: 520 Log-loss: 0.04016580432653427\n",
      "Epoch: 2 Batch: 560 Log-loss: 0.06612157821655273\n",
      "Epoch average log-loss: 0.051255934489225705\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 2, val_loss: 0.049959937106721676, best_val_loss: 0.049959937106721676, best_auc: 0.9731386680714578\n",
      "Epoch: 3 Batch: 0 Log-loss: 0.054770100861787796\n",
      "Epoch: 3 Batch: 40 Log-loss: 0.04359434172511101\n",
      "Epoch: 3 Batch: 80 Log-loss: 0.06336146593093872\n",
      "Epoch: 3 Batch: 120 Log-loss: 0.04855518043041229\n",
      "Epoch: 3 Batch: 160 Log-loss: 0.049681406468153\n",
      "Epoch: 3 Batch: 200 Log-loss: 0.03985656425356865\n",
      "Epoch: 3 Batch: 240 Log-loss: 0.05238308385014534\n",
      "Epoch: 3 Batch: 280 Log-loss: 0.05286920443177223\n",
      "Epoch: 3 Batch: 320 Log-loss: 0.04125024005770683\n",
      "Epoch: 3 Batch: 360 Log-loss: 0.05723455920815468\n",
      "Epoch: 3 Batch: 400 Log-loss: 0.052656009793281555\n",
      "Epoch: 3 Batch: 440 Log-loss: 0.02945701964199543\n",
      "Epoch: 3 Batch: 480 Log-loss: 0.041974786669015884\n",
      "Epoch: 3 Batch: 520 Log-loss: 0.04416760802268982\n",
      "Epoch: 3 Batch: 560 Log-loss: 0.058711759746074677\n",
      "Epoch average log-loss: 0.04871682673027473\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 3, val_loss: 0.047957666014373325, best_val_loss: 0.047957666014373325, best_auc: 0.9772084040979044\n",
      "Epoch: 4 Batch: 0 Log-loss: 0.03974701091647148\n",
      "Epoch: 4 Batch: 40 Log-loss: 0.046411097049713135\n",
      "Epoch: 4 Batch: 80 Log-loss: 0.047039929777383804\n",
      "Epoch: 4 Batch: 120 Log-loss: 0.05888749659061432\n",
      "Epoch: 4 Batch: 160 Log-loss: 0.035448554903268814\n",
      "Epoch: 4 Batch: 200 Log-loss: 0.05402926728129387\n",
      "Epoch: 4 Batch: 240 Log-loss: 0.04405010864138603\n",
      "Epoch: 4 Batch: 280 Log-loss: 0.044471919536590576\n",
      "Epoch: 4 Batch: 320 Log-loss: 0.0366893969476223\n",
      "Epoch: 4 Batch: 360 Log-loss: 0.047722429037094116\n",
      "Epoch: 4 Batch: 400 Log-loss: 0.040717918425798416\n",
      "Epoch: 4 Batch: 440 Log-loss: 0.050038550049066544\n",
      "Epoch: 4 Batch: 480 Log-loss: 0.041070882230997086\n",
      "Epoch: 4 Batch: 520 Log-loss: 0.04226109758019447\n",
      "Epoch: 4 Batch: 560 Log-loss: 0.05336926877498627\n",
      "Epoch average log-loss: 0.04648553827844028\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 4, val_loss: 0.046407435652884486, best_val_loss: 0.046407435652884486, best_auc: 0.9815203263828384\n",
      "Epoch: 5 Batch: 0 Log-loss: 0.06061616539955139\n",
      "Epoch: 5 Batch: 40 Log-loss: 0.03397243469953537\n",
      "Epoch: 5 Batch: 80 Log-loss: 0.04352805018424988\n",
      "Epoch: 5 Batch: 120 Log-loss: 0.06119375303387642\n",
      "Epoch: 5 Batch: 160 Log-loss: 0.036026280373334885\n",
      "Epoch: 5 Batch: 200 Log-loss: 0.042087066918611526\n",
      "Epoch: 5 Batch: 240 Log-loss: 0.04504767060279846\n",
      "Epoch: 5 Batch: 280 Log-loss: 0.05489354208111763\n",
      "Epoch: 5 Batch: 320 Log-loss: 0.040269073098897934\n",
      "Epoch: 5 Batch: 360 Log-loss: 0.053761985152959824\n",
      "Epoch: 5 Batch: 400 Log-loss: 0.047561902552843094\n",
      "Epoch: 5 Batch: 440 Log-loss: 0.05668817088007927\n",
      "Epoch: 5 Batch: 480 Log-loss: 0.044400110840797424\n",
      "Epoch: 5 Batch: 520 Log-loss: 0.04237790033221245\n",
      "Epoch: 5 Batch: 560 Log-loss: 0.05141149461269379\n",
      "Epoch average log-loss: 0.0453267437838284\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 5, val_loss: 0.046265307547352745, best_val_loss: 0.046265307547352745, best_auc: 0.9838147191433726\n",
      "Epoch: 6 Batch: 0 Log-loss: 0.04720543324947357\n",
      "Epoch: 6 Batch: 40 Log-loss: 0.03863583877682686\n",
      "Epoch: 6 Batch: 80 Log-loss: 0.04569616541266441\n",
      "Epoch: 6 Batch: 120 Log-loss: 0.05144830420613289\n",
      "Epoch: 6 Batch: 160 Log-loss: 0.038822732865810394\n",
      "Epoch: 6 Batch: 200 Log-loss: 0.054381731897592545\n",
      "Epoch: 6 Batch: 240 Log-loss: 0.0492284931242466\n",
      "Epoch: 6 Batch: 280 Log-loss: 0.031020067632198334\n",
      "Epoch: 6 Batch: 320 Log-loss: 0.049919743090867996\n",
      "Epoch: 6 Batch: 360 Log-loss: 0.053975433111190796\n",
      "Epoch: 6 Batch: 400 Log-loss: 0.03588741272687912\n",
      "Epoch: 6 Batch: 440 Log-loss: 0.06042531505227089\n",
      "Epoch: 6 Batch: 480 Log-loss: 0.06258279830217361\n",
      "Epoch: 6 Batch: 520 Log-loss: 0.057920124381780624\n",
      "Epoch: 6 Batch: 560 Log-loss: 0.03905869275331497\n",
      "Epoch average log-loss: 0.04453812537249178\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 6, val_loss: 0.04479931446136676, best_val_loss: 0.04479931446136676, best_auc: 0.9836574023272714\n",
      "Epoch: 7 Batch: 0 Log-loss: 0.02679412066936493\n",
      "Epoch: 7 Batch: 40 Log-loss: 0.03500262275338173\n",
      "Epoch: 7 Batch: 80 Log-loss: 0.04760707914829254\n",
      "Epoch: 7 Batch: 120 Log-loss: 0.04357241466641426\n",
      "Epoch: 7 Batch: 160 Log-loss: 0.03323855623602867\n",
      "Epoch: 7 Batch: 200 Log-loss: 0.05374525859951973\n",
      "Epoch: 7 Batch: 240 Log-loss: 0.04064531251788139\n",
      "Epoch: 7 Batch: 280 Log-loss: 0.03141739219427109\n",
      "Epoch: 7 Batch: 320 Log-loss: 0.04217299446463585\n",
      "Epoch: 7 Batch: 360 Log-loss: 0.027100248262286186\n",
      "Epoch: 7 Batch: 400 Log-loss: 0.04781663417816162\n",
      "Epoch: 7 Batch: 440 Log-loss: 0.063088558614254\n",
      "Epoch: 7 Batch: 480 Log-loss: 0.03815096989274025\n",
      "Epoch: 7 Batch: 520 Log-loss: 0.037938568741083145\n",
      "Epoch: 7 Batch: 560 Log-loss: 0.044749438762664795\n",
      "Epoch average log-loss: 0.04378583556307214\n",
      "In Epoch: 7, val_loss: 0.04507984683980251, best_val_loss: 0.04479931446136676, best_auc: 0.9836574023272714\n",
      "Epoch: 8 Batch: 0 Log-loss: 0.057504162192344666\n",
      "Epoch: 8 Batch: 40 Log-loss: 0.03339206054806709\n",
      "Epoch: 8 Batch: 80 Log-loss: 0.04163876548409462\n",
      "Epoch: 8 Batch: 120 Log-loss: 0.040039096027612686\n",
      "Epoch: 8 Batch: 160 Log-loss: 0.044460054486989975\n",
      "Epoch: 8 Batch: 200 Log-loss: 0.04479986056685448\n",
      "Epoch: 8 Batch: 240 Log-loss: 0.0688946545124054\n",
      "Epoch: 8 Batch: 280 Log-loss: 0.03212080895900726\n",
      "Epoch: 8 Batch: 320 Log-loss: 0.027017034590244293\n",
      "Epoch: 8 Batch: 360 Log-loss: 0.059531450271606445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Batch: 400 Log-loss: 0.03382719308137894\n",
      "Epoch: 8 Batch: 440 Log-loss: 0.048083167523145676\n",
      "Epoch: 8 Batch: 480 Log-loss: 0.05418485030531883\n",
      "Epoch: 8 Batch: 520 Log-loss: 0.04740704968571663\n",
      "Epoch: 8 Batch: 560 Log-loss: 0.0408833883702755\n",
      "Epoch average log-loss: 0.04323709427512118\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 8, val_loss: 0.043749109520896945, best_val_loss: 0.043749109520896945, best_auc: 0.9857527683808179\n",
      "Epoch: 9 Batch: 0 Log-loss: 0.04207947850227356\n",
      "Epoch: 9 Batch: 40 Log-loss: 0.03316400572657585\n",
      "Epoch: 9 Batch: 80 Log-loss: 0.04960491880774498\n",
      "Epoch: 9 Batch: 120 Log-loss: 0.04337206482887268\n",
      "Epoch: 9 Batch: 160 Log-loss: 0.04126513749361038\n",
      "Epoch: 9 Batch: 200 Log-loss: 0.04335455223917961\n",
      "Epoch: 9 Batch: 240 Log-loss: 0.03849118947982788\n",
      "Epoch: 9 Batch: 280 Log-loss: 0.04522884264588356\n",
      "Epoch: 9 Batch: 320 Log-loss: 0.033341165632009506\n",
      "Epoch: 9 Batch: 360 Log-loss: 0.05232580378651619\n",
      "Epoch: 9 Batch: 400 Log-loss: 0.04583147168159485\n",
      "Epoch: 9 Batch: 440 Log-loss: 0.03508627042174339\n",
      "Epoch: 9 Batch: 480 Log-loss: 0.048843588680028915\n",
      "Epoch: 9 Batch: 520 Log-loss: 0.03834875673055649\n",
      "Epoch: 9 Batch: 560 Log-loss: 0.03694825991988182\n",
      "Epoch average log-loss: 0.042780413970883405\n",
      "In Epoch: 9, val_loss: 0.0438017208479362, best_val_loss: 0.043749109520896945, best_auc: 0.9857527683808179\n",
      "Epoch: 10 Batch: 0 Log-loss: 0.05358290299773216\n",
      "Epoch: 10 Batch: 40 Log-loss: 0.058227550238370895\n",
      "Epoch: 10 Batch: 80 Log-loss: 0.050779905170202255\n",
      "Epoch: 10 Batch: 120 Log-loss: 0.03987807035446167\n",
      "Epoch: 10 Batch: 160 Log-loss: 0.04319683834910393\n",
      "Epoch: 10 Batch: 200 Log-loss: 0.04065815731883049\n",
      "Epoch: 10 Batch: 240 Log-loss: 0.04105560854077339\n",
      "Epoch: 10 Batch: 280 Log-loss: 0.04264765605330467\n",
      "Epoch: 10 Batch: 320 Log-loss: 0.05350026115775108\n",
      "Epoch: 10 Batch: 360 Log-loss: 0.05738603696227074\n",
      "Epoch: 10 Batch: 400 Log-loss: 0.0424676239490509\n",
      "Epoch: 10 Batch: 440 Log-loss: 0.03660129755735397\n",
      "Epoch: 10 Batch: 480 Log-loss: 0.06216553971171379\n",
      "Epoch: 10 Batch: 520 Log-loss: 0.04016663506627083\n",
      "Epoch: 10 Batch: 560 Log-loss: 0.03866080194711685\n",
      "Epoch average log-loss: 0.04217482326618795\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 10, val_loss: 0.043606407472193714, best_val_loss: 0.043606407472193714, best_auc: 0.9856528896466145\n",
      "Epoch: 11 Batch: 0 Log-loss: 0.036145344376564026\n",
      "Epoch: 11 Batch: 40 Log-loss: 0.05501026287674904\n",
      "Epoch: 11 Batch: 80 Log-loss: 0.04867883026599884\n",
      "Epoch: 11 Batch: 120 Log-loss: 0.04949327930808067\n",
      "Epoch: 11 Batch: 160 Log-loss: 0.027129190042614937\n",
      "Epoch: 11 Batch: 200 Log-loss: 0.048818886280059814\n",
      "Epoch: 11 Batch: 240 Log-loss: 0.03414371982216835\n",
      "Epoch: 11 Batch: 280 Log-loss: 0.056305497884750366\n",
      "Epoch: 11 Batch: 320 Log-loss: 0.03928210958838463\n",
      "Epoch: 11 Batch: 360 Log-loss: 0.046291619539260864\n",
      "Epoch: 11 Batch: 400 Log-loss: 0.03728669136762619\n",
      "Epoch: 11 Batch: 440 Log-loss: 0.03040868043899536\n",
      "Epoch: 11 Batch: 480 Log-loss: 0.04310760274529457\n",
      "Epoch: 11 Batch: 520 Log-loss: 0.036781322211027145\n",
      "Epoch: 11 Batch: 560 Log-loss: 0.055341724306344986\n",
      "Epoch average log-loss: 0.04221343400422484\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 11, val_loss: 0.04355263296422399, best_val_loss: 0.04355263296422399, best_auc: 0.9862503167147109\n",
      "Epoch: 12 Batch: 0 Log-loss: 0.03731494024395943\n",
      "Epoch: 12 Batch: 40 Log-loss: 0.040283024311065674\n",
      "Epoch: 12 Batch: 80 Log-loss: 0.05136258527636528\n",
      "Epoch: 12 Batch: 120 Log-loss: 0.05223428085446358\n",
      "Epoch: 12 Batch: 160 Log-loss: 0.03884805366396904\n",
      "Epoch: 12 Batch: 200 Log-loss: 0.04292936623096466\n",
      "Epoch: 12 Batch: 240 Log-loss: 0.045381829142570496\n",
      "Epoch: 12 Batch: 280 Log-loss: 0.05189427733421326\n",
      "Epoch: 12 Batch: 320 Log-loss: 0.04014265909790993\n",
      "Epoch: 12 Batch: 360 Log-loss: 0.03911210969090462\n",
      "Epoch: 12 Batch: 400 Log-loss: 0.05505840852856636\n",
      "Epoch: 12 Batch: 440 Log-loss: 0.05164389684796333\n",
      "Epoch: 12 Batch: 480 Log-loss: 0.03303918242454529\n",
      "Epoch: 12 Batch: 520 Log-loss: 0.04316513240337372\n",
      "Epoch: 12 Batch: 560 Log-loss: 0.04442431032657623\n",
      "Epoch average log-loss: 0.04164106557145715\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 12, val_loss: 0.042709535963684216, best_val_loss: 0.042709535963684216, best_auc: 0.9868495101684204\n",
      "Epoch: 13 Batch: 0 Log-loss: 0.04815717041492462\n",
      "Epoch: 13 Batch: 40 Log-loss: 0.04223978891968727\n",
      "Epoch: 13 Batch: 80 Log-loss: 0.04800976440310478\n",
      "Epoch: 13 Batch: 120 Log-loss: 0.02352604828774929\n",
      "Epoch: 13 Batch: 160 Log-loss: 0.03844447061419487\n",
      "Epoch: 13 Batch: 200 Log-loss: 0.052053302526474\n",
      "Epoch: 13 Batch: 240 Log-loss: 0.044090013951063156\n",
      "Epoch: 13 Batch: 280 Log-loss: 0.03497632220387459\n",
      "Epoch: 13 Batch: 320 Log-loss: 0.0369756855070591\n",
      "Epoch: 13 Batch: 360 Log-loss: 0.0542147122323513\n",
      "Epoch: 13 Batch: 400 Log-loss: 0.060245539993047714\n",
      "Epoch: 13 Batch: 440 Log-loss: 0.04859383404254913\n",
      "Epoch: 13 Batch: 480 Log-loss: 0.03783772885799408\n",
      "Epoch: 13 Batch: 520 Log-loss: 0.04584938660264015\n",
      "Epoch: 13 Batch: 560 Log-loss: 0.05264581739902496\n",
      "Epoch average log-loss: 0.0412389335521896\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 13, val_loss: 0.04264739444703094, best_val_loss: 0.04264739444703094, best_auc: 0.9874523140181172\n",
      "Epoch: 14 Batch: 0 Log-loss: 0.0444914735853672\n",
      "Epoch: 14 Batch: 40 Log-loss: 0.0479605495929718\n",
      "Epoch: 14 Batch: 80 Log-loss: 0.05200343206524849\n",
      "Epoch: 14 Batch: 120 Log-loss: 0.0486309677362442\n",
      "Epoch: 14 Batch: 160 Log-loss: 0.03227830305695534\n",
      "Epoch: 14 Batch: 200 Log-loss: 0.06327202916145325\n",
      "Epoch: 14 Batch: 240 Log-loss: 0.050338417291641235\n",
      "Epoch: 14 Batch: 280 Log-loss: 0.05156039074063301\n",
      "Epoch: 14 Batch: 320 Log-loss: 0.04299594461917877\n",
      "Epoch: 14 Batch: 360 Log-loss: 0.037263233214616776\n",
      "Epoch: 14 Batch: 400 Log-loss: 0.031644221395254135\n",
      "Epoch: 14 Batch: 440 Log-loss: 0.04282395541667938\n",
      "Epoch: 14 Batch: 480 Log-loss: 0.04738640785217285\n",
      "Epoch: 14 Batch: 520 Log-loss: 0.03777593746781349\n",
      "Epoch: 14 Batch: 560 Log-loss: 0.03523087128996849\n",
      "Epoch average log-loss: 0.04128232192514198\n",
      "In Epoch: 14, val_loss: 0.04277241657654896, best_val_loss: 0.04264739444703094, best_auc: 0.9874523140181172\n",
      "Epoch: 15 Batch: 0 Log-loss: 0.050609346479177475\n",
      "Epoch: 15 Batch: 40 Log-loss: 0.051681485027074814\n",
      "Epoch: 15 Batch: 80 Log-loss: 0.04800262674689293\n",
      "Epoch: 15 Batch: 120 Log-loss: 0.047374311834573746\n",
      "Epoch: 15 Batch: 160 Log-loss: 0.04362886771559715\n",
      "Epoch: 15 Batch: 200 Log-loss: 0.037351466715335846\n",
      "Epoch: 15 Batch: 240 Log-loss: 0.04397697374224663\n",
      "Epoch: 15 Batch: 280 Log-loss: 0.03232384845614433\n",
      "Epoch: 15 Batch: 320 Log-loss: 0.050047606229782104\n",
      "Epoch: 15 Batch: 360 Log-loss: 0.047014687210321426\n",
      "Epoch: 15 Batch: 400 Log-loss: 0.0357184000313282\n",
      "Epoch: 15 Batch: 440 Log-loss: 0.031243862584233284\n",
      "Epoch: 15 Batch: 480 Log-loss: 0.03555586189031601\n",
      "Epoch: 15 Batch: 520 Log-loss: 0.045660600066185\n",
      "Epoch: 15 Batch: 560 Log-loss: 0.04405155032873154\n",
      "Epoch average log-loss: 0.040949226689657996\n",
      "In Epoch: 15, val_loss: 0.043855663335977096, best_val_loss: 0.04264739444703094, best_auc: 0.9874523140181172\n",
      "Epoch: 16 Batch: 0 Log-loss: 0.04034414142370224\n",
      "Epoch: 16 Batch: 40 Log-loss: 0.027583541348576546\n",
      "Epoch: 16 Batch: 80 Log-loss: 0.04167819023132324\n",
      "Epoch: 16 Batch: 120 Log-loss: 0.03135230019688606\n",
      "Epoch: 16 Batch: 160 Log-loss: 0.0468284972012043\n",
      "Epoch: 16 Batch: 200 Log-loss: 0.03749121353030205\n",
      "Epoch: 16 Batch: 240 Log-loss: 0.04503696784377098\n",
      "Epoch: 16 Batch: 280 Log-loss: 0.040752869099378586\n",
      "Epoch: 16 Batch: 320 Log-loss: 0.02961859665811062\n",
      "Epoch: 16 Batch: 360 Log-loss: 0.026762768626213074\n",
      "Epoch: 16 Batch: 400 Log-loss: 0.038247060030698776\n",
      "Epoch: 16 Batch: 440 Log-loss: 0.02723931521177292\n",
      "Epoch: 16 Batch: 480 Log-loss: 0.060808464884757996\n",
      "Epoch: 16 Batch: 520 Log-loss: 0.04481760039925575\n",
      "Epoch: 16 Batch: 560 Log-loss: 0.03144186735153198\n",
      "Epoch average log-loss: 0.04077746101788112\n",
      "In Epoch: 16, val_loss: 0.04304455916789684, best_val_loss: 0.04264739444703094, best_auc: 0.9874523140181172\n",
      "Epoch: 17 Batch: 0 Log-loss: 0.025653747841715813\n",
      "Epoch: 17 Batch: 40 Log-loss: 0.0445149801671505\n",
      "Epoch: 17 Batch: 80 Log-loss: 0.05199294909834862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 Batch: 120 Log-loss: 0.05397900938987732\n",
      "Epoch: 17 Batch: 160 Log-loss: 0.046282440423965454\n",
      "Epoch: 17 Batch: 200 Log-loss: 0.04273634031414986\n",
      "Epoch: 17 Batch: 240 Log-loss: 0.038886964321136475\n",
      "Epoch: 17 Batch: 280 Log-loss: 0.04354344680905342\n",
      "Epoch: 17 Batch: 320 Log-loss: 0.02584017626941204\n",
      "Epoch: 17 Batch: 360 Log-loss: 0.02790120057761669\n",
      "Epoch: 17 Batch: 400 Log-loss: 0.0509486086666584\n",
      "Epoch: 17 Batch: 440 Log-loss: 0.030340520665049553\n",
      "Epoch: 17 Batch: 480 Log-loss: 0.03737952560186386\n",
      "Epoch: 17 Batch: 520 Log-loss: 0.044431690126657486\n",
      "Epoch: 17 Batch: 560 Log-loss: 0.03767431527376175\n",
      "Epoch average log-loss: 0.040437045746615954\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 17, val_loss: 0.042334016915048435, best_val_loss: 0.042334016915048435, best_auc: 0.9878003029687994\n",
      "Epoch: 18 Batch: 0 Log-loss: 0.04020917788147926\n",
      "Epoch: 18 Batch: 40 Log-loss: 0.046628281474113464\n",
      "Epoch: 18 Batch: 80 Log-loss: 0.039797861129045486\n",
      "Epoch: 18 Batch: 120 Log-loss: 0.04586310312151909\n",
      "Epoch: 18 Batch: 160 Log-loss: 0.04409538581967354\n",
      "Epoch: 18 Batch: 200 Log-loss: 0.0446414053440094\n",
      "Epoch: 18 Batch: 240 Log-loss: 0.049381840974092484\n",
      "Epoch: 18 Batch: 280 Log-loss: 0.03508615121245384\n",
      "Epoch: 18 Batch: 320 Log-loss: 0.03029092401266098\n",
      "Epoch: 18 Batch: 360 Log-loss: 0.04231143370270729\n",
      "Epoch: 18 Batch: 400 Log-loss: 0.03753116726875305\n",
      "Epoch: 18 Batch: 440 Log-loss: 0.035053495317697525\n",
      "Epoch: 18 Batch: 480 Log-loss: 0.04400449991226196\n",
      "Epoch: 18 Batch: 520 Log-loss: 0.0382230244576931\n",
      "Epoch: 18 Batch: 560 Log-loss: 0.03479255735874176\n",
      "Epoch average log-loss: 0.040263934287109546\n",
      "In Epoch: 18, val_loss: 0.0425258751626004, best_val_loss: 0.042334016915048435, best_auc: 0.9878003029687994\n",
      "Epoch: 19 Batch: 0 Log-loss: 0.04671810194849968\n",
      "Epoch: 19 Batch: 40 Log-loss: 0.031118763610720634\n",
      "Epoch: 19 Batch: 80 Log-loss: 0.05660031735897064\n",
      "Epoch: 19 Batch: 120 Log-loss: 0.03685646504163742\n",
      "Epoch: 19 Batch: 160 Log-loss: 0.030747398734092712\n",
      "Epoch: 19 Batch: 200 Log-loss: 0.057929784059524536\n",
      "Epoch: 19 Batch: 240 Log-loss: 0.046339523047208786\n",
      "Epoch: 19 Batch: 280 Log-loss: 0.04412365332245827\n",
      "Epoch: 19 Batch: 320 Log-loss: 0.05005916580557823\n",
      "Epoch: 19 Batch: 360 Log-loss: 0.02999856323003769\n",
      "Epoch: 19 Batch: 400 Log-loss: 0.02714725024998188\n",
      "Epoch: 19 Batch: 440 Log-loss: 0.045371901243925095\n",
      "Epoch: 19 Batch: 480 Log-loss: 0.03798731416463852\n",
      "Epoch: 19 Batch: 520 Log-loss: 0.03253727778792381\n",
      "Epoch: 19 Batch: 560 Log-loss: 0.03726116195321083\n",
      "Epoch average log-loss: 0.04036020691772657\n",
      "In Epoch: 19, val_loss: 0.04237226446141532, best_val_loss: 0.042334016915048435, best_auc: 0.9878003029687994\n",
      "Epoch: 20 Batch: 0 Log-loss: 0.04338458180427551\n",
      "Epoch: 20 Batch: 40 Log-loss: 0.032882433384656906\n",
      "Epoch: 20 Batch: 80 Log-loss: 0.03649362176656723\n",
      "Epoch: 20 Batch: 120 Log-loss: 0.02763254940509796\n",
      "Epoch: 20 Batch: 160 Log-loss: 0.041337329894304276\n",
      "Epoch: 20 Batch: 200 Log-loss: 0.057435691356658936\n",
      "Epoch: 20 Batch: 240 Log-loss: 0.05436612665653229\n",
      "Epoch: 20 Batch: 280 Log-loss: 0.045808304101228714\n",
      "Epoch: 20 Batch: 320 Log-loss: 0.04377228021621704\n",
      "Epoch: 20 Batch: 360 Log-loss: 0.03924768418073654\n",
      "Epoch: 20 Batch: 400 Log-loss: 0.044285181909799576\n",
      "Epoch: 20 Batch: 440 Log-loss: 0.04202275350689888\n",
      "Epoch: 20 Batch: 480 Log-loss: 0.04349539801478386\n",
      "Epoch: 20 Batch: 520 Log-loss: 0.029914328828454018\n",
      "Epoch: 20 Batch: 560 Log-loss: 0.03422610089182854\n",
      "Epoch average log-loss: 0.04001135801331007\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 20, val_loss: 0.04207448748068642, best_val_loss: 0.04207448748068642, best_auc: 0.9878801992444403\n",
      "Epoch: 21 Batch: 0 Log-loss: 0.040008433163166046\n",
      "Epoch: 21 Batch: 40 Log-loss: 0.0417911671102047\n",
      "Epoch: 21 Batch: 80 Log-loss: 0.03258899226784706\n",
      "Epoch: 21 Batch: 120 Log-loss: 0.0285495538264513\n",
      "Epoch: 21 Batch: 160 Log-loss: 0.036607272922992706\n",
      "Epoch: 21 Batch: 200 Log-loss: 0.049747273325920105\n",
      "Epoch: 21 Batch: 240 Log-loss: 0.038415029644966125\n",
      "Epoch: 21 Batch: 280 Log-loss: 0.039393652230501175\n",
      "Epoch: 21 Batch: 320 Log-loss: 0.04939905181527138\n",
      "Epoch: 21 Batch: 360 Log-loss: 0.03327656909823418\n",
      "Epoch: 21 Batch: 400 Log-loss: 0.03871369734406471\n",
      "Epoch: 21 Batch: 440 Log-loss: 0.041475143283605576\n",
      "Epoch: 21 Batch: 480 Log-loss: 0.02958410046994686\n",
      "Epoch: 21 Batch: 520 Log-loss: 0.04464520141482353\n",
      "Epoch: 21 Batch: 560 Log-loss: 0.04016965627670288\n",
      "Epoch average log-loss: 0.0397999857452565\n",
      "In Epoch: 21, val_loss: 0.04300539834166157, best_val_loss: 0.04207448748068642, best_auc: 0.9878801992444403\n",
      "Epoch: 22 Batch: 0 Log-loss: 0.03600737825036049\n",
      "Epoch: 22 Batch: 40 Log-loss: 0.03498975560069084\n",
      "Epoch: 22 Batch: 80 Log-loss: 0.046942029148340225\n",
      "Epoch: 22 Batch: 120 Log-loss: 0.043478578329086304\n",
      "Epoch: 22 Batch: 160 Log-loss: 0.03075387515127659\n",
      "Epoch: 22 Batch: 200 Log-loss: 0.04120131954550743\n",
      "Epoch: 22 Batch: 240 Log-loss: 0.046748626977205276\n",
      "Epoch: 22 Batch: 280 Log-loss: 0.04784537851810455\n",
      "Epoch: 22 Batch: 320 Log-loss: 0.035400282591581345\n",
      "Epoch: 22 Batch: 360 Log-loss: 0.057212281972169876\n",
      "Epoch: 22 Batch: 400 Log-loss: 0.03520986810326576\n",
      "Epoch: 22 Batch: 440 Log-loss: 0.03683743625879288\n",
      "Epoch: 22 Batch: 480 Log-loss: 0.041571810841560364\n",
      "Epoch: 22 Batch: 520 Log-loss: 0.03559378907084465\n",
      "Epoch: 22 Batch: 560 Log-loss: 0.042423877865076065\n",
      "Epoch average log-loss: 0.03983400769066066\n",
      "In Epoch: 22, val_loss: 0.04261147080775423, best_val_loss: 0.04207448748068642, best_auc: 0.9878801992444403\n",
      "Epoch: 23 Batch: 0 Log-loss: 0.024419767782092094\n",
      "Epoch: 23 Batch: 40 Log-loss: 0.05738617852330208\n",
      "Epoch: 23 Batch: 80 Log-loss: 0.03437744081020355\n",
      "Epoch: 23 Batch: 120 Log-loss: 0.038401853293180466\n",
      "Epoch: 23 Batch: 160 Log-loss: 0.04437949135899544\n",
      "Epoch: 23 Batch: 200 Log-loss: 0.03314358741044998\n",
      "Epoch: 23 Batch: 240 Log-loss: 0.03323828801512718\n",
      "Epoch: 23 Batch: 280 Log-loss: 0.0317625030875206\n",
      "Epoch: 23 Batch: 320 Log-loss: 0.04059214144945145\n",
      "Epoch: 23 Batch: 360 Log-loss: 0.0383242703974247\n",
      "Epoch: 23 Batch: 400 Log-loss: 0.03651810809969902\n",
      "Epoch: 23 Batch: 440 Log-loss: 0.02871548943221569\n",
      "Epoch: 23 Batch: 480 Log-loss: 0.04311354085803032\n",
      "Epoch: 23 Batch: 520 Log-loss: 0.062257617712020874\n",
      "Epoch: 23 Batch: 560 Log-loss: 0.03855631500482559\n",
      "Epoch average log-loss: 0.039569527183526326\n",
      "In Epoch: 23, val_loss: 0.04243725638036425, best_val_loss: 0.04207448748068642, best_auc: 0.9878801992444403\n",
      "Epoch: 24 Batch: 0 Log-loss: 0.04182858392596245\n",
      "Epoch: 24 Batch: 40 Log-loss: 0.03900207206606865\n",
      "Epoch: 24 Batch: 80 Log-loss: 0.04343000426888466\n",
      "Epoch: 24 Batch: 120 Log-loss: 0.03448581323027611\n",
      "Epoch: 24 Batch: 160 Log-loss: 0.04115744307637215\n",
      "Epoch: 24 Batch: 200 Log-loss: 0.04021881893277168\n",
      "Epoch: 24 Batch: 240 Log-loss: 0.0527682900428772\n",
      "Epoch: 24 Batch: 280 Log-loss: 0.032080475240945816\n",
      "Epoch: 24 Batch: 320 Log-loss: 0.054800957441329956\n",
      "Epoch: 24 Batch: 360 Log-loss: 0.03789367154240608\n",
      "Epoch: 24 Batch: 400 Log-loss: 0.028590722009539604\n",
      "Epoch: 24 Batch: 440 Log-loss: 0.04636409506201744\n",
      "Epoch: 24 Batch: 480 Log-loss: 0.03895476832985878\n",
      "Epoch: 24 Batch: 520 Log-loss: 0.03391791135072708\n",
      "Epoch: 24 Batch: 560 Log-loss: 0.03311470150947571\n",
      "Epoch average log-loss: 0.039557559927925465\n",
      "In Epoch: 24, val_loss: 0.042379973481967786, best_val_loss: 0.04207448748068642, best_auc: 0.9878801992444403\n",
      "Epoch: 25 Batch: 0 Log-loss: 0.048358771950006485\n",
      "Epoch: 25 Batch: 40 Log-loss: 0.041058044880628586\n",
      "Epoch: 25 Batch: 80 Log-loss: 0.042697831988334656\n",
      "Epoch: 25 Batch: 120 Log-loss: 0.05019648000597954\n",
      "Epoch: 25 Batch: 160 Log-loss: 0.04199652373790741\n",
      "Epoch: 25 Batch: 200 Log-loss: 0.020906003192067146\n",
      "Epoch: 25 Batch: 240 Log-loss: 0.033239658921957016\n",
      "Epoch: 25 Batch: 280 Log-loss: 0.035933129489421844\n",
      "Epoch: 25 Batch: 320 Log-loss: 0.043648723512887955\n",
      "Epoch: 25 Batch: 360 Log-loss: 0.03407621011137962\n",
      "Epoch: 25 Batch: 400 Log-loss: 0.045036837458610535\n",
      "Epoch: 25 Batch: 440 Log-loss: 0.03161833435297012\n",
      "Epoch: 25 Batch: 480 Log-loss: 0.04348224401473999\n",
      "Epoch: 25 Batch: 520 Log-loss: 0.049645423889160156\n",
      "Epoch: 25 Batch: 560 Log-loss: 0.04367324337363243\n",
      "Epoch average log-loss: 0.039539439510554074\n",
      "In Epoch: 25, val_loss: 0.04214444019990094, best_val_loss: 0.04207448748068642, best_auc: 0.9878801992444403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 Batch: 0 Log-loss: 0.03883567452430725\n",
      "Epoch: 26 Batch: 40 Log-loss: 0.045384105294942856\n",
      "Epoch: 26 Batch: 80 Log-loss: 0.024999646469950676\n",
      "Epoch: 26 Batch: 120 Log-loss: 0.03816399723291397\n",
      "Epoch: 26 Batch: 160 Log-loss: 0.04577105864882469\n",
      "Epoch: 26 Batch: 200 Log-loss: 0.05936066433787346\n",
      "Epoch: 26 Batch: 240 Log-loss: 0.042776674032211304\n",
      "Epoch: 26 Batch: 280 Log-loss: 0.034909214824438095\n",
      "Epoch: 26 Batch: 320 Log-loss: 0.04442470893263817\n",
      "Epoch: 26 Batch: 360 Log-loss: 0.028683951124548912\n",
      "Epoch: 26 Batch: 400 Log-loss: 0.049075350165367126\n",
      "Epoch: 26 Batch: 440 Log-loss: 0.039184246212244034\n",
      "Epoch: 26 Batch: 480 Log-loss: 0.036265257745981216\n",
      "Epoch: 26 Batch: 520 Log-loss: 0.05032311752438545\n",
      "Epoch: 26 Batch: 560 Log-loss: 0.032920077443122864\n",
      "Epoch average log-loss: 0.03944106190798006\n",
      "In Epoch: 26, val_loss: 0.04262586031051133, best_val_loss: 0.04207448748068642, best_auc: 0.9878801992444403\n",
      "Epoch: 27 Batch: 0 Log-loss: 0.022795936092734337\n",
      "Epoch: 27 Batch: 40 Log-loss: 0.04918985441327095\n",
      "Epoch: 27 Batch: 80 Log-loss: 0.031947147101163864\n",
      "Epoch: 27 Batch: 120 Log-loss: 0.0348387248814106\n",
      "Epoch: 27 Batch: 160 Log-loss: 0.041971176862716675\n",
      "Epoch: 27 Batch: 200 Log-loss: 0.040772538632154465\n",
      "Epoch: 27 Batch: 240 Log-loss: 0.04421889781951904\n",
      "Epoch: 27 Batch: 280 Log-loss: 0.04177128151059151\n",
      "Epoch: 27 Batch: 320 Log-loss: 0.03869320824742317\n",
      "Epoch: 27 Batch: 360 Log-loss: 0.030883489176630974\n",
      "Epoch: 27 Batch: 400 Log-loss: 0.03121747262775898\n",
      "Epoch: 27 Batch: 440 Log-loss: 0.03896268084645271\n",
      "Epoch: 27 Batch: 480 Log-loss: 0.03804440051317215\n",
      "Epoch: 27 Batch: 520 Log-loss: 0.04040120169520378\n",
      "Epoch: 27 Batch: 560 Log-loss: 0.037170033901929855\n",
      "Epoch average log-loss: 0.03935735051719738\n",
      "In Epoch: 27, val_loss: 0.04237315874501185, best_val_loss: 0.04207448748068642, best_auc: 0.9878801992444403\n",
      "Epoch: 28 Batch: 0 Log-loss: 0.035378266125917435\n",
      "Epoch: 28 Batch: 40 Log-loss: 0.02549014985561371\n",
      "Epoch: 28 Batch: 80 Log-loss: 0.04081136733293533\n",
      "Epoch: 28 Batch: 120 Log-loss: 0.051733557134866714\n",
      "Epoch: 28 Batch: 160 Log-loss: 0.03854203224182129\n",
      "Epoch: 28 Batch: 200 Log-loss: 0.04438258707523346\n",
      "Epoch: 28 Batch: 240 Log-loss: 0.041534725576639175\n",
      "Epoch: 28 Batch: 280 Log-loss: 0.036067042499780655\n",
      "Epoch: 28 Batch: 320 Log-loss: 0.03418513759970665\n",
      "Epoch: 28 Batch: 360 Log-loss: 0.052005279809236526\n",
      "Epoch: 28 Batch: 400 Log-loss: 0.04356817528605461\n",
      "Epoch: 28 Batch: 440 Log-loss: 0.0286442581564188\n",
      "Epoch: 28 Batch: 480 Log-loss: 0.04051665961742401\n",
      "Epoch: 28 Batch: 520 Log-loss: 0.03699013590812683\n",
      "Epoch: 28 Batch: 560 Log-loss: 0.04652079939842224\n",
      "Epoch average log-loss: 0.039336098747194875\n",
      "In Epoch: 28, val_loss: 0.04218573464699903, best_val_loss: 0.04207448748068642, best_auc: 0.9878801992444403\n",
      "Epoch: 29 Batch: 0 Log-loss: 0.030260002240538597\n",
      "Epoch: 29 Batch: 40 Log-loss: 0.03643205761909485\n",
      "Epoch: 29 Batch: 80 Log-loss: 0.0332329161465168\n",
      "Epoch: 29 Batch: 120 Log-loss: 0.03163500875234604\n",
      "Epoch: 29 Batch: 160 Log-loss: 0.03533388301730156\n",
      "Epoch: 29 Batch: 200 Log-loss: 0.029187457635998726\n",
      "Epoch: 29 Batch: 240 Log-loss: 0.04328710958361626\n",
      "Epoch: 29 Batch: 280 Log-loss: 0.04572415351867676\n",
      "Epoch: 29 Batch: 320 Log-loss: 0.04559792950749397\n",
      "Epoch: 29 Batch: 360 Log-loss: 0.03970711678266525\n",
      "Epoch: 29 Batch: 400 Log-loss: 0.03867195174098015\n",
      "Epoch: 29 Batch: 440 Log-loss: 0.04228224232792854\n",
      "Epoch: 29 Batch: 480 Log-loss: 0.036142971366643906\n",
      "Epoch: 29 Batch: 520 Log-loss: 0.03782912716269493\n",
      "Epoch: 29 Batch: 560 Log-loss: 0.04172650724649429\n",
      "Epoch average log-loss: 0.03923068284389696\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt.\n",
      "\n",
      "In Epoch: 29, val_loss: 0.04195208273176713, best_val_loss: 0.04195208273176713, best_auc: 0.9874712577696828\n",
      "Epoch: 30 Batch: 0 Log-loss: 0.02370643801987171\n",
      "Epoch: 30 Batch: 40 Log-loss: 0.04513993859291077\n",
      "Epoch: 30 Batch: 80 Log-loss: 0.039673760533332825\n",
      "Epoch: 30 Batch: 120 Log-loss: 0.03266904875636101\n",
      "Epoch: 30 Batch: 160 Log-loss: 0.04027288034558296\n",
      "Epoch: 30 Batch: 200 Log-loss: 0.04025174304842949\n",
      "Epoch: 30 Batch: 240 Log-loss: 0.04662073776125908\n",
      "Epoch: 30 Batch: 280 Log-loss: 0.05301130935549736\n",
      "Epoch: 30 Batch: 320 Log-loss: 0.02577909640967846\n",
      "Epoch: 30 Batch: 360 Log-loss: 0.05228828266263008\n",
      "Epoch: 30 Batch: 400 Log-loss: 0.03595404699444771\n",
      "Epoch: 30 Batch: 440 Log-loss: 0.03918157145380974\n",
      "Epoch: 30 Batch: 480 Log-loss: 0.046513933688402176\n",
      "Epoch: 30 Batch: 520 Log-loss: 0.028114335611462593\n",
      "Epoch: 30 Batch: 560 Log-loss: 0.045589931309223175\n",
      "Epoch average log-loss: 0.03917590455883848\n",
      "In Epoch: 30, val_loss: 0.04271160936796959, best_val_loss: 0.04195208273176713, best_auc: 0.9874712577696828\n",
      "Epoch: 31 Batch: 0 Log-loss: 0.0398075133562088\n",
      "Epoch: 31 Batch: 40 Log-loss: 0.04485092684626579\n",
      "Epoch: 31 Batch: 80 Log-loss: 0.033650752156972885\n",
      "Epoch: 31 Batch: 120 Log-loss: 0.04406876489520073\n",
      "Epoch: 31 Batch: 160 Log-loss: 0.03979875519871712\n",
      "Epoch: 31 Batch: 200 Log-loss: 0.05210428312420845\n",
      "Epoch: 31 Batch: 240 Log-loss: 0.032979611307382584\n",
      "Epoch: 31 Batch: 280 Log-loss: 0.036664124578237534\n",
      "Epoch: 31 Batch: 320 Log-loss: 0.0434727780520916\n",
      "Epoch: 31 Batch: 360 Log-loss: 0.037998367100954056\n",
      "Epoch: 31 Batch: 400 Log-loss: 0.0333324633538723\n",
      "Epoch: 31 Batch: 440 Log-loss: 0.03810477629303932\n",
      "Epoch: 31 Batch: 480 Log-loss: 0.05189121142029762\n",
      "Epoch: 31 Batch: 520 Log-loss: 0.0410451740026474\n",
      "Epoch: 31 Batch: 560 Log-loss: 0.041781164705753326\n",
      "Epoch average log-loss: 0.03923964817076921\n",
      "In Epoch: 31, val_loss: 0.042533605179181845, best_val_loss: 0.04195208273176713, best_auc: 0.9874712577696828\n",
      "Epoch: 32 Batch: 0 Log-loss: 0.04770873114466667\n",
      "Epoch: 32 Batch: 40 Log-loss: 0.04818029701709747\n",
      "Epoch: 32 Batch: 80 Log-loss: 0.054867714643478394\n",
      "Epoch: 32 Batch: 120 Log-loss: 0.04615074023604393\n",
      "Epoch: 32 Batch: 160 Log-loss: 0.03275880590081215\n",
      "Epoch: 32 Batch: 200 Log-loss: 0.046524178236722946\n",
      "Epoch: 32 Batch: 240 Log-loss: 0.037274934351444244\n",
      "Epoch: 32 Batch: 280 Log-loss: 0.026440108194947243\n",
      "Epoch: 32 Batch: 320 Log-loss: 0.056611742824316025\n",
      "Epoch: 32 Batch: 360 Log-loss: 0.03372098505496979\n",
      "Epoch: 32 Batch: 400 Log-loss: 0.043386250734329224\n",
      "Epoch: 32 Batch: 440 Log-loss: 0.039840199053287506\n",
      "Epoch: 32 Batch: 480 Log-loss: 0.022351099178195\n",
      "Epoch: 32 Batch: 520 Log-loss: 0.03299103304743767\n",
      "Epoch: 32 Batch: 560 Log-loss: 0.02472437545657158\n",
      "Epoch average log-loss: 0.038961261031883104\n",
      "In Epoch: 32, val_loss: 0.04267747923849236, best_val_loss: 0.04195208273176713, best_auc: 0.9874712577696828\n",
      "Epoch: 33 Batch: 0 Log-loss: 0.044583600014448166\n",
      "Epoch: 33 Batch: 40 Log-loss: 0.03782704845070839\n",
      "Epoch: 33 Batch: 80 Log-loss: 0.049531061202287674\n",
      "Epoch: 33 Batch: 120 Log-loss: 0.060778092592954636\n",
      "Epoch: 33 Batch: 160 Log-loss: 0.04106739163398743\n",
      "Epoch: 33 Batch: 200 Log-loss: 0.03157828375697136\n",
      "Epoch: 33 Batch: 240 Log-loss: 0.04189079999923706\n",
      "Epoch: 33 Batch: 280 Log-loss: 0.03448707237839699\n",
      "Epoch: 33 Batch: 320 Log-loss: 0.026016099378466606\n",
      "Epoch: 33 Batch: 360 Log-loss: 0.04478082433342934\n",
      "Epoch: 33 Batch: 400 Log-loss: 0.043198153376579285\n",
      "Epoch: 33 Batch: 440 Log-loss: 0.055113598704338074\n",
      "Epoch: 33 Batch: 480 Log-loss: 0.03163473308086395\n",
      "Epoch: 33 Batch: 520 Log-loss: 0.03307966887950897\n",
      "Epoch: 33 Batch: 560 Log-loss: 0.03772559016942978\n",
      "Epoch average log-loss: 0.0389956937504134\n",
      "In Epoch: 33, val_loss: 0.04204198751668973, best_val_loss: 0.04195208273176713, best_auc: 0.9874712577696828\n",
      "Epoch: 34 Batch: 0 Log-loss: 0.04715224727988243\n",
      "Epoch: 34 Batch: 40 Log-loss: 0.043347302824258804\n",
      "Epoch: 34 Batch: 80 Log-loss: 0.036220040172338486\n",
      "Epoch: 34 Batch: 120 Log-loss: 0.03620770946145058\n",
      "Epoch: 34 Batch: 160 Log-loss: 0.04334178566932678\n",
      "Epoch: 34 Batch: 200 Log-loss: 0.03978443518280983\n",
      "Epoch: 34 Batch: 240 Log-loss: 0.04350457713007927\n",
      "Epoch: 34 Batch: 280 Log-loss: 0.038562219589948654\n",
      "Epoch: 34 Batch: 320 Log-loss: 0.038175929337739944\n",
      "Epoch: 34 Batch: 360 Log-loss: 0.036028530448675156\n",
      "Epoch: 34 Batch: 400 Log-loss: 0.02619200386106968\n",
      "Epoch: 34 Batch: 440 Log-loss: 0.03517350181937218\n",
      "Epoch: 34 Batch: 480 Log-loss: 0.03819791600108147\n",
      "Epoch: 34 Batch: 520 Log-loss: 0.05143843963742256\n",
      "Epoch: 34 Batch: 560 Log-loss: 0.024816658347845078\n",
      "Epoch average log-loss: 0.03870512328576296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Epoch: 34, val_loss: 0.042443047534341936, best_val_loss: 0.04195208273176713, best_auc: 0.9874712577696828\n",
      "Epoch: 35 Batch: 0 Log-loss: 0.040370915085077286\n",
      "Epoch: 35 Batch: 40 Log-loss: 0.03420058265328407\n",
      "Epoch: 35 Batch: 80 Log-loss: 0.043225616216659546\n",
      "Epoch: 35 Batch: 120 Log-loss: 0.029145510867238045\n",
      "Epoch: 35 Batch: 160 Log-loss: 0.0430169440805912\n",
      "Epoch: 35 Batch: 200 Log-loss: 0.041503969579935074\n",
      "Epoch: 35 Batch: 240 Log-loss: 0.030428245663642883\n",
      "Epoch: 35 Batch: 280 Log-loss: 0.03583500534296036\n",
      "Epoch: 35 Batch: 320 Log-loss: 0.03483636677265167\n",
      "Epoch: 35 Batch: 360 Log-loss: 0.023957595229148865\n",
      "Epoch: 35 Batch: 400 Log-loss: 0.0330234058201313\n",
      "Epoch: 35 Batch: 440 Log-loss: 0.031945422291755676\n",
      "Epoch: 35 Batch: 480 Log-loss: 0.043881550431251526\n",
      "Epoch: 35 Batch: 520 Log-loss: 0.024249091744422913\n",
      "Epoch: 35 Batch: 560 Log-loss: 0.03303859010338783\n",
      "Epoch average log-loss: 0.03883973265100005\n",
      "In Epoch: 35, val_loss: 0.04225873007797046, best_val_loss: 0.04195208273176713, best_auc: 0.9874712577696828\n",
      "Epoch: 36 Batch: 0 Log-loss: 0.033940013498067856\n",
      "Epoch: 36 Batch: 40 Log-loss: 0.03203163295984268\n",
      "Epoch: 36 Batch: 80 Log-loss: 0.02562742866575718\n",
      "Epoch: 36 Batch: 120 Log-loss: 0.036950450390577316\n",
      "Epoch: 36 Batch: 160 Log-loss: 0.028691472485661507\n",
      "Epoch: 36 Batch: 200 Log-loss: 0.027775272727012634\n",
      "Epoch: 36 Batch: 240 Log-loss: 0.04727717861533165\n",
      "Epoch: 36 Batch: 280 Log-loss: 0.04529445245862007\n",
      "Epoch: 36 Batch: 320 Log-loss: 0.04157637059688568\n",
      "Epoch: 36 Batch: 360 Log-loss: 0.03600384667515755\n",
      "Epoch: 36 Batch: 400 Log-loss: 0.03654260188341141\n",
      "Epoch: 36 Batch: 440 Log-loss: 0.05093899369239807\n",
      "Epoch: 36 Batch: 480 Log-loss: 0.03525962308049202\n",
      "Epoch: 36 Batch: 520 Log-loss: 0.0276659969240427\n",
      "Epoch: 36 Batch: 560 Log-loss: 0.04021668806672096\n",
      "Epoch average log-loss: 0.03874151877659772\n",
      "In Epoch: 36, val_loss: 0.04252275718007791, best_val_loss: 0.04195208273176713, best_auc: 0.9874712577696828\n",
      "Epoch: 37 Batch: 0 Log-loss: 0.045636486262083054\n",
      "Epoch: 37 Batch: 40 Log-loss: 0.03443680331110954\n",
      "Epoch: 37 Batch: 80 Log-loss: 0.0390176959335804\n",
      "Epoch: 37 Batch: 120 Log-loss: 0.032966602593660355\n",
      "Epoch: 37 Batch: 160 Log-loss: 0.03899642452597618\n",
      "Epoch: 37 Batch: 200 Log-loss: 0.037928592413663864\n",
      "Epoch: 37 Batch: 240 Log-loss: 0.04089013859629631\n",
      "Epoch: 37 Batch: 280 Log-loss: 0.03219171240925789\n",
      "Epoch: 37 Batch: 320 Log-loss: 0.03492787852883339\n",
      "Epoch: 37 Batch: 360 Log-loss: 0.04100758954882622\n",
      "Epoch: 37 Batch: 400 Log-loss: 0.03881823644042015\n",
      "Epoch: 37 Batch: 440 Log-loss: 0.04259061813354492\n",
      "Epoch: 37 Batch: 480 Log-loss: 0.03380696848034859\n",
      "Epoch: 37 Batch: 520 Log-loss: 0.037068501114845276\n",
      "Epoch: 37 Batch: 560 Log-loss: 0.031579937785863876\n",
      "Epoch average log-loss: 0.03888243048318795\n",
      "In Epoch: 37, val_loss: 0.04238764662747121, best_val_loss: 0.04195208273176713, best_auc: 0.9874712577696828\n",
      "Epoch: 38 Batch: 0 Log-loss: 0.034956760704517365\n",
      "Epoch: 38 Batch: 40 Log-loss: 0.04624450206756592\n",
      "Epoch: 38 Batch: 80 Log-loss: 0.05047367513179779\n",
      "Epoch: 38 Batch: 120 Log-loss: 0.04287341609597206\n",
      "Epoch: 38 Batch: 160 Log-loss: 0.04722527787089348\n",
      "Epoch: 38 Batch: 200 Log-loss: 0.031527359038591385\n",
      "Epoch: 38 Batch: 240 Log-loss: 0.0280169565230608\n",
      "Epoch: 38 Batch: 280 Log-loss: 0.03135483339428902\n",
      "Epoch: 38 Batch: 320 Log-loss: 0.03054775856435299\n",
      "Epoch: 38 Batch: 360 Log-loss: 0.03810242936015129\n",
      "Epoch: 38 Batch: 400 Log-loss: 0.031484026461839676\n",
      "Epoch: 38 Batch: 440 Log-loss: 0.032568324357271194\n",
      "Epoch: 38 Batch: 480 Log-loss: 0.03747188672423363\n",
      "Epoch: 38 Batch: 520 Log-loss: 0.043008193373680115\n",
      "Epoch: 38 Batch: 560 Log-loss: 0.043129533529281616\n",
      "Epoch average log-loss: 0.038614679989404974\n",
      "In Epoch: 38, val_loss: 0.04236647059157067, best_val_loss: 0.04195208273176713, best_auc: 0.9874712577696828\n",
      "Epoch: 39 Batch: 0 Log-loss: 0.04421760514378548\n",
      "Epoch: 39 Batch: 40 Log-loss: 0.032208386808633804\n",
      "Epoch: 39 Batch: 80 Log-loss: 0.053370073437690735\n",
      "Epoch: 39 Batch: 120 Log-loss: 0.03472647815942764\n",
      "Epoch: 39 Batch: 160 Log-loss: 0.03667066991329193\n",
      "Epoch: 39 Batch: 200 Log-loss: 0.029959946870803833\n",
      "Epoch: 39 Batch: 240 Log-loss: 0.04426513984799385\n",
      "Epoch: 39 Batch: 280 Log-loss: 0.029626324772834778\n",
      "Epoch: 39 Batch: 320 Log-loss: 0.041868094354867935\n",
      "Epoch: 39 Batch: 360 Log-loss: 0.03929492458701134\n",
      "Epoch: 39 Batch: 400 Log-loss: 0.028961682692170143\n",
      "Epoch: 39 Batch: 440 Log-loss: 0.03825773671269417\n",
      "Epoch: 39 Batch: 480 Log-loss: 0.037329625338315964\n",
      "Epoch: 39 Batch: 520 Log-loss: 0.039727404713630676\n",
      "Epoch: 39 Batch: 560 Log-loss: 0.0333167165517807\n",
      "Epoch average log-loss: 0.03867870017753116\n",
      "In Epoch: 39, val_loss: 0.042136345949783695, best_val_loss: 0.04195208273176713, best_auc: 0.9874712577696828\n",
      "Epoch: 40 Batch: 0 Log-loss: 0.0531640462577343\n",
      "Epoch: 40 Batch: 40 Log-loss: 0.0353396050632\n",
      "Epoch: 40 Batch: 80 Log-loss: 0.04218420758843422\n",
      "Epoch: 40 Batch: 120 Log-loss: 0.04147797077894211\n",
      "Epoch: 40 Batch: 160 Log-loss: 0.03188227117061615\n",
      "Epoch: 40 Batch: 200 Log-loss: 0.03687924146652222\n",
      "Epoch: 40 Batch: 240 Log-loss: 0.03925016522407532\n",
      "Epoch: 40 Batch: 280 Log-loss: 0.04483460262417793\n",
      "Epoch: 40 Batch: 320 Log-loss: 0.04414820671081543\n",
      "Epoch: 40 Batch: 360 Log-loss: 0.043800100684165955\n",
      "Epoch: 40 Batch: 400 Log-loss: 0.035222526639699936\n",
      "Epoch: 40 Batch: 440 Log-loss: 0.024192407727241516\n",
      "Epoch: 40 Batch: 480 Log-loss: 0.039888132363557816\n",
      "Epoch: 40 Batch: 520 Log-loss: 0.04717341065406799\n",
      "Epoch: 40 Batch: 560 Log-loss: 0.04437440261244774\n",
      "Epoch average log-loss: 0.038507668261549305\n",
      "In Epoch: 40, val_loss: 0.04208935452651921, best_val_loss: 0.04195208273176713, best_auc: 0.9874712577696828\n",
      "Epoch: 41 Batch: 0 Log-loss: 0.042638156563043594\n",
      "Epoch: 41 Batch: 40 Log-loss: 0.02554631419479847\n",
      "Epoch: 41 Batch: 80 Log-loss: 0.03600512817502022\n",
      "Epoch: 41 Batch: 120 Log-loss: 0.06361851841211319\n",
      "Epoch: 41 Batch: 160 Log-loss: 0.06474557518959045\n",
      "Epoch: 41 Batch: 200 Log-loss: 0.04126768931746483\n",
      "Epoch: 41 Batch: 240 Log-loss: 0.04452551528811455\n",
      "Epoch: 41 Batch: 280 Log-loss: 0.04329053685069084\n",
      "Epoch: 41 Batch: 320 Log-loss: 0.03160775825381279\n",
      "Epoch: 41 Batch: 360 Log-loss: 0.04518003761768341\n",
      "Epoch: 41 Batch: 400 Log-loss: 0.02084789238870144\n",
      "Epoch: 41 Batch: 440 Log-loss: 0.03165895864367485\n",
      "Epoch: 41 Batch: 480 Log-loss: 0.02170487679541111\n",
      "Epoch: 41 Batch: 520 Log-loss: 0.04462334141135216\n",
      "Epoch: 41 Batch: 560 Log-loss: 0.04183833673596382\n",
      "Epoch average log-loss: 0.0385101961903274\n",
      "A pre-trained model at model_pool/Pytorch/rhn/pavel_rhn-TEMP.pt has been loaded.\n",
      "Model has been saved as model_pool/Pytorch/rhn/pavel_rhn9.pt.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models, val_loss, total_auc, fold_predictions = pytorch_train_folds(x=train_sequences, y=train_labels, fold_count=10, batch_size=256, get_model_func=get_recurrent_highway_classifier, skip_fold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pos-Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_path_prefix = 'results/CNN_Based/' + model_name\n",
    "\n",
    "print('Predicting testing results...')\n",
    "test_predicts_list = []\n",
    "for fold_id, model in enumerate(models):\n",
    "    test_data_dict = {'Onehot': test_data, 'POS': pos_test_data}\n",
    "    test_predict = model.predict(test_data_dict, batch_size=BATCH_SIZE, verbose=1)\n",
    "    test_predicts_list.append(test_predict)\n",
    "    np.save('parameters_pool/AVPOSCNN/{}-AV-POS-CNN.npy'.format(fold_id), test_predict)\n",
    "    \n",
    "test_predicts = np.zeros(test_predicts_list[0].shape)\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts += fold_predict\n",
    "test_predicts /= len(test_predicts_list)\n",
    "\n",
    "test_ids = test_df['id'].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "\n",
    "test_predicts = pd.DataFrame(data=test_predicts, columns=list_classes)\n",
    "test_predicts['id'] = test_ids\n",
    "test_predicts = test_predicts[['id'] + list_classes]\n",
    "submit_path = submit_path_prefix + '-L{:4f}-A{:4f}.csv'.format(val_loss, total_auc)\n",
    "test_predicts.to_csv(submit_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General-Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_path_prefix = 'results/rhn/Fasttext-rhn-' + str(MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Predicting testing results...')\n",
    "test_predicts_list = []\n",
    "for fold_id, model in enumerate(models):\n",
    "    test_predict = model.predict(test_sequences, batch_size=BATCH_SIZE, verbose=1)\n",
    "    test_predicts_list.append(test_predict)\n",
    "    \n",
    "test_predicts = np.zeros(test_predicts_list[0].shape)\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts += fold_predict\n",
    "test_predicts /= len(test_predicts_list)\n",
    "\n",
    "test_ids = test_df['id'].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "\n",
    "test_predicts = pd.DataFrame(data=test_predicts, columns=list_classes)\n",
    "test_predicts['id'] = test_ids\n",
    "test_predicts = test_predicts[['id'] + list_classes]\n",
    "submit_path = submit_path_prefix + '-L{:4f}-A{:4f}.csv'.format(val_loss, total_auc)\n",
    "test_predicts.to_csv(submit_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOB (Out-of-Bag) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold_predictions = np.concatenate((fold_predictions), axis=0)\n",
    "train_auc = roc_auc_score(train_labels, train_fold_predictions)\n",
    "print('Training AUC', train_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicting training results...')\n",
    "train_ids = train_df['id'].values\n",
    "train_ids = train_ids.reshape((len(train_ids), 1))\n",
    "\n",
    "train_predicts = pd.DataFrame(data=train_fold_predictions, columns=list_classes)\n",
    "train_predicts['id'] = train_ids\n",
    "train_predicts = train_predicts[['id'] + list_classes]\n",
    "submit_path = submit_path_prefix + '-(Train)-L{:4f}-A{:4f}.csv'.format(val_loss, train_auc)\n",
    "train_predicts.to_csv(submit_path, index=False)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Ensemble (For Test Format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging(arrs, path):\n",
    "    print(\"Doing ensemble on\")\n",
    "    subs = []\n",
    "    for arr in arrs:\n",
    "        print(arr)\n",
    "        subs.append(pd.read_csv(arr))\n",
    "    \n",
    "    for sub in subs[1:]:\n",
    "        for c in list_classes:\n",
    "            subs[0][c] += sub[c]\n",
    "    \n",
    "    for c in list_classes:\n",
    "        subs[0][c] /= len(subs)\n",
    "        \n",
    "    subs[0].to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_corr(arr1, arr2):\n",
    "    res = 0\n",
    "    for col in arr1.columns.values[1:]:\n",
    "        cur = arr1[col].corr(arr2[col])\n",
    "        corr = (arr1[col].rank() / len(arr1)).corr(arr2[col].rank() / len(arr2))\n",
    "        print(col, corr)\n",
    "        res += corr\n",
    "    print(\"Avg Rank: \", res / len(arr1.columns.values[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Estimation (For Train Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = train_df.iloc[:, :8]\n",
    "ground_truth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_prediction = pd.read_csv('results/RNN_Based/fasttext-avrnn-100000vocabulary-350length-(Train)-L0.013377-A0.998050.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_term_pos(train, pred, check_column):\n",
    "    sub_train = train[check_column]\n",
    "    sub_pred = pred[check_column]\n",
    "    sub_pred = sub_pred.round()\n",
    "    diff = (sub_pred != sub_train) & (sub_train == 1)\n",
    "    print('Wrong pos-predictions number is {}\\tWrong rate: {}'.format(diff.sum(), diff.sum() / len(sub_pred)))\n",
    "    pos = pd.DataFrame()\n",
    "    pos['id'] = train[diff]['id']\n",
    "    pos['text'] = train[diff]['comment_text']\n",
    "    pos['pred_val'] = pred[diff][check_column]\n",
    "    pos['label'] = train[diff][check_column]\n",
    "    return pos\n",
    "\n",
    "def get_error_term_neg(train, pred, check_column):\n",
    "    sub_train = train[check_column]\n",
    "    sub_pred = pred[check_column]\n",
    "    sub_pred = sub_pred.round()\n",
    "    diff = (sub_pred != sub_train) & (sub_train == 0)\n",
    "    print('Wrong neg-predictions number is {}\\tWrong rate: {}'.format(diff.sum(), diff.sum() / len(sub_pred)))\n",
    "    neg = pd.DataFrame()\n",
    "    neg['id'] = train[diff]['id']\n",
    "    neg['text'] = train[diff]['comment_text']\n",
    "    neg['pred_val'] = pred[diff][check_column]\n",
    "    neg['label'] = train[diff][check_column]\n",
    "    return neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in list_classes:\n",
    "    print('In term: ', term)\n",
    "    err_neg = get_error_term_neg(train_df, check_prediction, term)\n",
    "    err_pos = get_error_term_pos(train_df, check_prediction, term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
