{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re, csv, codecs, operator, sys, gc\n",
    "from collections import defaultdict, OrderedDict\n",
    "from tqdm import tqdm\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import lightgbm as lgb\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from textblob import TextBlob    # For pos-tagging\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from scipy.special import logit, expit\n",
    "\n",
    "from keras import optimizers, initializers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Lambda, Embedding, Dropout, Activation, SpatialDropout1D, Reshape, \\\n",
    "GlobalAveragePooling1D, merge, Flatten, Bidirectional, CuDNNGRU, add, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'datasets/'\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "FAST_TEXT_EMBEDDING = 'pretrain_embedding/crawl-300d-2M.vec'\n",
    "GLOVE_EMBEDDING = 'pretrain_embedding/glove.840B.300d.txt'\n",
    "CLEAN_WORD_PATH = None\n",
    "TRAIN_DATA_FILE = 'train.csv'\n",
    "TEST_DATA_FILE = 'test.csv'\n",
    "MAX_SEQUENCE_LENGTH = 350\n",
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300\n",
    "FOLD_COUNT = 10\n",
    "BATCH_SIZE = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrain Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrain_embedding(file):\n",
    "    print('Indexing word vectors')\n",
    "    embeddings_index = {}\n",
    "    f = open(file, 'r', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        try:\n",
    "            word = values[0]\n",
    "            coefs = np.array(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        except:\n",
    "            print(\"Error on: \", values[:3])\n",
    "    f.close()\n",
    "    print(\"Total %s word vectors\" % len(embeddings_index))\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Total 2000000 word vectors\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = load_pretrain_embedding(FAST_TEXT_EMBEDDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Error on:  ['.', '.', '.']\n",
      "Error on:  ['at', 'name@domain.com', '0.0061218']\n",
      "Error on:  ['.', '.', '.']\n",
      "Error on:  ['to', 'name@domain.com', '0.33865']\n",
      "Error on:  ['.', '.', '0.035974']\n",
      "Error on:  ['.', '.', '.']\n",
      "Error on:  ['email', 'name@domain.com', '0.33529']\n",
      "Error on:  ['or', 'name@domain.com', '0.48374']\n",
      "Error on:  ['contact', 'name@domain.com', '0.016426']\n",
      "Error on:  ['Email', 'name@domain.com', '0.37344']\n",
      "Error on:  ['on', 'name@domain.com', '0.037295']\n",
      "Error on:  ['At', 'Killerseats.com', '-0.13854']\n",
      "Error on:  ['by', 'name@domain.com', '0.6882']\n",
      "Error on:  ['in', 'mylot.com', '-0.18148']\n",
      "Error on:  ['emailing', 'name@domain.com', '0.39173']\n",
      "Error on:  ['Contact', 'name@domain.com', '0.14933']\n",
      "Error on:  ['at', 'name@domain.com', '0.44321']\n",
      "Error on:  ['•', 'name@domain.com', '-0.13288']\n",
      "Error on:  ['at', 'Amazon.com', '-0.5275']\n",
      "Error on:  ['is', 'name@domain.com', '-0.1197']\n",
      "Total 2195885 word vectors\n"
     ]
    }
   ],
   "source": [
    "glove_embeddings_index = load_pretrain_embedding(GLOVE_EMBEDDING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('datasets/train.csv')\n",
    "test_df = pd.read_csv('datasets/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      "id               159571 non-null object\n",
      "comment_text     159571 non-null object\n",
      "toxic            159571 non-null int64\n",
      "severe_toxic     159571 non-null int64\n",
      "obscene          159571 non-null int64\n",
      "threat           159571 non-null int64\n",
      "insult           159571 non-null int64\n",
      "identity_hate    159571 non-null int64\n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 9.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805  \n",
       "std         0.216627       0.093420  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.308619</td>\n",
       "      <td>0.676515</td>\n",
       "      <td>0.157058</td>\n",
       "      <td>0.647518</td>\n",
       "      <td>0.266009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>severe_toxic</th>\n",
       "      <td>0.308619</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.403014</td>\n",
       "      <td>0.123601</td>\n",
       "      <td>0.375807</td>\n",
       "      <td>0.201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obscene</th>\n",
       "      <td>0.676515</td>\n",
       "      <td>0.403014</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.141179</td>\n",
       "      <td>0.741272</td>\n",
       "      <td>0.286867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threat</th>\n",
       "      <td>0.157058</td>\n",
       "      <td>0.123601</td>\n",
       "      <td>0.141179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.150022</td>\n",
       "      <td>0.115128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insult</th>\n",
       "      <td>0.647518</td>\n",
       "      <td>0.375807</td>\n",
       "      <td>0.741272</td>\n",
       "      <td>0.150022</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.337736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>identity_hate</th>\n",
       "      <td>0.266009</td>\n",
       "      <td>0.201600</td>\n",
       "      <td>0.286867</td>\n",
       "      <td>0.115128</td>\n",
       "      <td>0.337736</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  toxic  severe_toxic   obscene    threat    insult  \\\n",
       "toxic          1.000000      0.308619  0.676515  0.157058  0.647518   \n",
       "severe_toxic   0.308619      1.000000  0.403014  0.123601  0.375807   \n",
       "obscene        0.676515      0.403014  1.000000  0.141179  0.741272   \n",
       "threat         0.157058      0.123601  0.141179  1.000000  0.150022   \n",
       "insult         0.647518      0.375807  0.741272  0.150022  1.000000   \n",
       "identity_hate  0.266009      0.201600  0.286867  0.115128  0.337736   \n",
       "\n",
       "               identity_hate  \n",
       "toxic               0.266009  \n",
       "severe_toxic        0.201600  \n",
       "obscene             0.286867  \n",
       "threat              0.115128  \n",
       "insult              0.337736  \n",
       "identity_hate       1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comments = train_df['comment_text'].values\n",
    "test_comments = test_df['comment_text'].values\n",
    "train_comments_lengths = [len(s) for s in train_comments]\n",
    "test_comments_lengths = [len(s) for s in test_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Train------\n",
      "MAX LENGTH:\t\t 5000\n",
      "AVG LENGTH:\t\t 394.0732213246768\n",
      "MIN LENGTH:\t\t 6\n",
      "STANDARD DIVISION:\t 590.7184309382144\n",
      "RANGE:\t\t\t 6  to  1575.5100832011055\n",
      "------Test------\n",
      "MAX LENGTH:\t\t 5000\n",
      "AVG LENGTH:\t\t 364.8751207855632\n",
      "MIN LENGTH:\t\t 1\n",
      "STANDARD DIVISION:\t 592.4901645516661\n",
      "RANGE:\t\t\t 1  to  1549.8554498888955\n"
     ]
    }
   ],
   "source": [
    "def explore_comments(arr):\n",
    "    print(\"MAX LENGTH:\\t\\t\", np.max(arr))\n",
    "    print(\"AVG LENGTH:\\t\\t\", np.average(arr))\n",
    "    print(\"MIN LENGTH:\\t\\t\", np.min(arr))\n",
    "    print(\"STANDARD DIVISION:\\t\", np.std(arr))\n",
    "    print(\"RANGE:\\t\\t\\t\", np.min(arr), \" to \", np.average(arr) + 2 * np.std(arr))\n",
    "    \n",
    "print(\"------Train------\")\n",
    "explore_comments(train_comments_lengths)\n",
    "\n",
    "print(\"------Test------\")\n",
    "explore_comments(test_comments_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "0    144277\n",
      "1     15294\n",
      "Name: toxic, dtype: int64\n",
      "\n",
      "severe_toxic\n",
      "0    157976\n",
      "1      1595\n",
      "Name: severe_toxic, dtype: int64\n",
      "\n",
      "obscene\n",
      "0    151122\n",
      "1      8449\n",
      "Name: obscene, dtype: int64\n",
      "\n",
      "threat\n",
      "0    159093\n",
      "1       478\n",
      "Name: threat, dtype: int64\n",
      "\n",
      "insult\n",
      "0    151694\n",
      "1      7877\n",
      "Name: insult, dtype: int64\n",
      "\n",
      "identity_hate\n",
      "0    158166\n",
      "1      1405\n",
      "Name: identity_hate, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for class_name in list_classes:\n",
    "    print('{}\\n{}\\n'.format(class_name, train_df[class_name].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_regexp_occ(regexp='', text=None):\n",
    "    \"\"\"\n",
    "    Simple way to calculate the number of occurence of a regex\n",
    "    \"\"\"\n",
    "    if len(text) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(re.findall(regexp, text)) / len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Cleaned Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEAN_WORD_PATH == None:\n",
    "    ignored_words = set(stopwords.words('english'))\n",
    "else:\n",
    "    ignored_words = {}\n",
    "    with open(CLEAN_WORD_PATH, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip('\\n')\n",
    "            typo, correct = line.split(',')\n",
    "            ignored_words[typo] = correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_files = sorted([f for f in listdir('datasets/feature_files/') if isfile(join('datasets/feature_files/', f))])\n",
    "for file in feature_files:\n",
    "    fixed_df = pd.read_csv('datasets/feature_files/' + file)\n",
    "    train_df = train_df.merge(fixed_df, on='id', how='left')\n",
    "    test_df = test_df.merge(fixed_df, on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_data(path):\n",
    "    files = sorted([f for f in listdir(path) if isfile(join(path, f))])\n",
    "    for i, file in enumerate(files):\n",
    "        temp_df = pd.read_csv(path + file)\n",
    "        print('Datasets before ensemble:', temp_df.isnull().sum().sum())\n",
    "        if i == 0:\n",
    "            ensembled = temp_df\n",
    "        else:\n",
    "            ensembled = ensembled.merge(temp_df, on='id', how='left')\n",
    "            print('Datasets after ensemble:', ensembled.isnull().sum().sum())\n",
    "    \n",
    "    column_numbers = ensembled.shape[1]\n",
    "    toxic = ensembled.iloc[:, [i for i in range(2, column_numbers, len(list_classes))]]\n",
    "    severe_toxic = ensembled.iloc[:, [i for i in range(3, column_numbers, len(list_classes))]]\n",
    "    obscene = ensembled.iloc[:, [i for i in range(4, column_numbers, len(list_classes))]]\n",
    "    threat = ensembled.iloc[:, [i for i in range(5, column_numbers, len(list_classes))]]\n",
    "    insult = ensembled.iloc[:, [i for i in range(6, column_numbers, len(list_classes))]]\n",
    "    identity_hate = ensembled.iloc[:, [i for i in range(7, column_numbers, len(list_classes))]]\n",
    "    \n",
    "    return toxic, severe_toxic, obscene, threat, insult, identity_hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets before ensemble: 0\n",
      "Datasets before ensemble: 0\n"
     ]
    }
   ],
   "source": [
    "train_toxic, train_severe_toxic, train_obscene, train_threat, train_insult, train_identity_hate = ensemble_data('datasets/ensemble_files/train/')\n",
    "test_toxic, test_severe_toxic, test_obscene, test_threat, test_insult, test_identity_hate = ensemble_data('datasets/ensemble_files/test/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal = re.compile(r'[^?!.,:a-z\\d ]', re.IGNORECASE)\n",
    "# Regex to remove all numerics\n",
    "replace_numbers = re.compile(r'\\d+', re.IGNORECASE)\n",
    "\n",
    "word_count_dict = defaultdict(int)\n",
    "\n",
    "def clean_datasets(text, remove_stopwords=False, stem_words=False, count_null_words=True, clean_wiki_tokens=True):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \"\", text)\n",
    "    text = re.sub(r\"”\", \"\\\"\", text)\n",
    "    text = re.sub(r\"“\", \"\\\"\", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"i’m\", \"i am\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = replace_numbers.sub('', text)\n",
    "    \n",
    "    if count_null_words:\n",
    "        text = text.split()\n",
    "        for t in text:\n",
    "            word_count_dict[t] += 1\n",
    "        text = \" \".join(text)\n",
    "        \n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_train = train_df['comment_text'].fillna('no comment').values\n",
    "list_sentences_test = test_df['comment_text'].fillna('no comment').values\n",
    "train_labels = train_df[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data cleaning...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "cleaned_train_comments, cleaned_test_comments = [], []\n",
    "print('Processing data cleaning...')\n",
    "\n",
    "for text in list_sentences_train:\n",
    "    cleaned_train_comments.append(clean_datasets(text))\n",
    "for text in list_sentences_test:\n",
    "    cleaned_test_comments.append(clean_datasets(text))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['comment_text_cleaned'] = cleaned_train_comments\n",
    "test_df['comment_text_cleaned'] = cleaned_test_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Meta (Extend) Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unknown_embedding(t, idx):\n",
    "    t = t.split()\n",
    "    res = 0\n",
    "    for word in t:\n",
    "        if word not in idx.keys():\n",
    "            res += 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_meta_feature(df):\n",
    "    df['total_length'] = df['comment_text'].apply(len)\n",
    "    df['capital'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.apply(lambda row: float(row['capital']) / float(row['total_length']), axis=1)\n",
    "    df[\"unknown_glove\"] = df['comment_text_cleaned'].apply(lambda x: count_unknown_embedding(x, glove_embeddings_index))\n",
    "    df[\"unknown_fasttext\"] = df['comment_text_cleaned'].apply(lambda x: count_unknown_embedding(x, embeddings_index))\n",
    "    df[\"unknown_glove_fasttext\"] = df[\"unknown_glove\"] + df[\"unknown_fasttext\"]\n",
    "    # Special Expressions Collection\n",
    "    df['num_exclamation_marks'] = df['comment_text'].apply(lambda comment: comment.count('!'))\n",
    "    df['num_question_marks'] = df['comment_text'].apply(lambda comment: comment.count('?'))\n",
    "    df['num_punctuation'] = df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '.,;:'))\n",
    "    df['num_symbols'] = df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '*&$%'))\n",
    "    df['num_words'] = df['comment_text'].apply(lambda comment: len(comment.split()))\n",
    "    df['num_unique_words'] = df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']\n",
    "    df['num_smilies'] = df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n",
    "    df[\"ant_slash_n\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\n\", x))\n",
    "    df[\"start_with_columns\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"^\\:+\", x))\n",
    "    df[\"has_emphasize_equal\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\={2}.+\\={2}\", x))\n",
    "    df[\"has_emphasize_quotes\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\\"{4}\\S+\\\"{4}\", x))\n",
    "    df[\"has_star\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\*\", x))\n",
    "    ## Check for dates\n",
    "    df[\"has_timestamp\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\d{2}|:\\d{2}\", x))\n",
    "    df[\"has_date_long\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{2}:\\d{2}, \\d{1,2} \\w+ \\d{4}\", x))    # example: 18:44, 8 December 2010\n",
    "    df[\"has_date_short\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{1,2} \\w+ \\d{4}\", x))\n",
    "    ## Check for http links\n",
    "    df[\"has_http\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"http[s]{0,1}://\\S+\", x))\n",
    "    df[\"has_ip\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(\"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}\", x))\n",
    "    ## Check for mail\n",
    "    df[\"has_mail\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', x))\n",
    "    ## Check for image\n",
    "    df[\"has_image\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r'image\\:', x))\n",
    "    # Dirty Languages Collection\n",
    "    df[\"nb_fk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ff]\\S{2}[Kk]\", x))\n",
    "    df[\"nb_sk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ss]\\S{2}[Kk]\", x))\n",
    "    df[\"nb_dk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[dD]ick\", x))\n",
    "    df[\"nb_you\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\W[Yy]ou\\W\", x))\n",
    "    df[\"nb_mother\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wmother\\W\", x))\n",
    "    df[\"nb_ng\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wnigger\\W\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating meta features...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating meta features...\")\n",
    "create_meta_feature(train_df)\n",
    "create_meta_feature(test_df)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['comment_text'], axis=1)\n",
    "test_df = test_df.drop(['comment_text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['total_length', 'capital', 'caps_vs_length', 'unknown_glove',\n",
      "       'unknown_fasttext', 'unknown_glove_fasttext', 'num_exclamation_marks',\n",
      "       'num_question_marks', 'num_punctuation', 'num_symbols', 'num_words',\n",
      "       'num_unique_words', 'words_vs_unique', 'num_smilies', 'ant_slash_n',\n",
      "       'start_with_columns', 'has_emphasize_equal', 'has_emphasize_quotes',\n",
      "       'has_star', 'has_timestamp', 'has_date_long', 'has_date_short',\n",
      "       'has_http', 'has_ip', 'has_mail', 'has_image', 'nb_fk', 'nb_sk',\n",
      "       'nb_dk', 'nb_you', 'nb_mother', 'nb_ng'],\n",
      "      dtype='object')\n",
      "40 32\n"
     ]
    }
   ],
   "source": [
    "train_cols = train_df.columns[8:]\n",
    "print(train_cols)\n",
    "column_numbers = len(train_df.columns)\n",
    "print(column_numbers, len(train_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_cols:\n",
    "    train_toxic[col] = train_df[col]\n",
    "    train_severe_toxic[col] = train_df[col]\n",
    "    train_obscene[col] = train_df[col]\n",
    "    train_threat[col] = train_df[col]\n",
    "    train_insult[col] = train_df[col]\n",
    "    train_identity_hate[col] = train_df[col]\n",
    "for col in train_cols:\n",
    "    test_toxic[col] = test_df[col]\n",
    "    test_severe_toxic[col] = test_df[col]\n",
    "    test_obscene[col] = test_df[col]\n",
    "    test_threat[col] = test_df[col]\n",
    "    test_insult[col] = test_df[col]\n",
    "    test_identity_hate[col] = test_df[col]\n",
    "train_ensembled_data = [train_toxic, train_severe_toxic, train_obscene, train_threat, train_insult, train_identity_hate]\n",
    "test_ensembled_data = [test_toxic, test_severe_toxic, test_obscene, test_threat, test_insult, test_identity_hate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del list_sentences_train, list_sentences_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312735"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_comment_text = pd.concat([train_df['comment_text_cleaned'], test_df['comment_text_cleaned']], axis=0).fillna(\"unknown\")\n",
    "nrow_train = train_df.shape[0]\n",
    "all_comment_text.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312735, 50000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=50000)\n",
    "data = vectorizer.fit_transform(all_comment_text)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312735, 50000)\n"
     ]
    }
   ],
   "source": [
    "norm_data = MaxAbsScaler().fit_transform(data)\n",
    "print(norm_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vectorization process Done!\n"
     ]
    }
   ],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=20000\n",
    ")\n",
    "word_vectorizer.fit(all_comment_text)\n",
    "train_word_features = word_vectorizer.transform(cleaned_train_comments)\n",
    "test_word_features = word_vectorizer.transform(cleaned_test_comments)\n",
    "print('Word vectorization process Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char vectorization process Done!\n"
     ]
    }
   ],
   "source": [
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 6),\n",
    "    max_features=30000\n",
    ")\n",
    "char_vectorizer.fit(all_comment_text)\n",
    "train_char_features = char_vectorizer.transform(cleaned_train_comments)\n",
    "test_char_features = char_vectorizer.transform(cleaned_test_comments)\n",
    "print('Char vectorization process Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del all_comment_text\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf_features = hstack([train_char_features, train_word_features]).tocsr()\n",
    "test_tfidf_features = hstack([test_char_features, test_word_features]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_char_features, train_word_features, test_char_features, test_word_features\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='\"#$%&()+,-./:;<=>@[\\\\]^_`{|}~\\t\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically train vocab & tokenizer...\n",
      "Found 353757 unique tokens\n",
      "Shape of train_data tensor:  (159571, 350)\n",
      "Shape of train_label tensor:  (159571, 6)\n",
      "Shape of test_data tensor:  (153164, 350)\n"
     ]
    }
   ],
   "source": [
    "print('Automatically train vocab & tokenizer...')\n",
    "tokenizer.fit_on_texts(cleaned_train_comments + cleaned_test_comments)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(cleaned_train_comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(cleaned_test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of train_data tensor: ', train_data.shape)\n",
    "print('Shape of train_label tensor: ', train_labels.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor: ', test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pos Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2pos(sentence):\n",
    "    try:\n",
    "        tag = TextBlob(sentence).tags\n",
    "    except:\n",
    "        print(sentence)\n",
    "        \n",
    "    updated_sentence = ' '.join([i[0] for i in tag])\n",
    "    tagged = ' '.join([i[1] for i in tag])\n",
    "    return updated_sentence, tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_word_index = {v: k for k, v in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [09:19<00:00, 285.39it/s]\n",
      "100%|██████████| 153164/153164 [08:42<00:00, 292.94it/s]\n"
     ]
    }
   ],
   "source": [
    "Pos_comments = []\n",
    "Pos_updated_sentence = []\n",
    "for text in tqdm(train_sequences):\n",
    "    text_ = ' '.join([inverse_word_index[word] for word in text])    # convert to word format\n",
    "    if not isinstance(text_, str):\n",
    "        print(text, '\\n', text_)\n",
    "    updated_sentence, tags = sent2pos(text_)\n",
    "    Pos_updated_sentence.append(updated_sentence)\n",
    "    Pos_comments.append(tags)\n",
    "    assert len(updated_sentence.split(' ')) == len(tags.split(' ')), \"T1 {} T2 {}\".format(len(text), len(tags.split()))\n",
    "    \n",
    "Pos_test_comments = []\n",
    "Pos_test_updated_sentence = []\n",
    "for text in tqdm(test_sequences):\n",
    "    text_ = ' '.join([inverse_word_index[word] for word in text])\n",
    "    updated_test_sentence, test_tags = sent2pos(text_)\n",
    "    Pos_test_updated_sentence.append(updated_test_sentence)\n",
    "    Pos_test_comments.append(test_tags)\n",
    "    assert len(updated_test_sentence.split(' ')) == len(test_tags.split(' ')), \"T1 {} T2 {}\".format(len(text), len(test_tags.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tokenizer = Tokenizer(num_words=50, filters='\"#$%&()+,-./:;<=>@[\\\\]^_`{|}~\\t\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically train pos tokenizer...\n",
      "Found 34 unique tokens\n",
      "Shape of data tensor: (159571, 350)\n",
      "Shape of test_data tensor: (153164, 350)\n"
     ]
    }
   ],
   "source": [
    "print('Automatically train pos tokenizer...')\n",
    "pos_tokenizer.fit_on_texts(Pos_comments + Pos_test_comments)\n",
    "\n",
    "train_pos_sequences = pos_tokenizer.texts_to_sequences(Pos_comments)\n",
    "test_pos_sequences = pos_tokenizer.texts_to_sequences(Pos_test_comments)\n",
    "\n",
    "pos_word_index = pos_tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(pos_word_index))\n",
    "\n",
    "pos_train_data = pad_sequences(train_pos_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', pos_train_data.shape)\n",
    "\n",
    "pos_test_data = pad_sequences(test_pos_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', pos_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second time valid for tokenzier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically train pos tokenizer secondly...\n",
      "Found 353757 unique tokens\n",
      "Shape of train_data tensor: (159571, 350)\n",
      "Shape of train_label tensor: (159571, 6)\n",
      "Shape of test_data tensor: (153164, 350)\n"
     ]
    }
   ],
   "source": [
    "print('Automatically train pos tokenizer secondly...')\n",
    "cleaned_train_comments, cleaned_test_comments = [], []\n",
    "for text in Pos_updated_sentence:\n",
    "    cleaned_train_comments.append(clean_datasets(text))\n",
    "for text in Pos_test_updated_sentence:\n",
    "    cleaned_test_comments.append(clean_datasets(text))\n",
    "    \n",
    "train_sequences = tokenizer.texts_to_sequences(cleaned_train_comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(cleaned_test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of train_data tensor:', train_data.shape)\n",
    "print('Shape of train_label tensor:', train_labels.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['cleaned_comment_text'] = cleaned_train_comments\n",
    "test_df['cleaned_comment_text'] = cleaned_test_comments\n",
    "train_df.to_csv(PATH + 'cleaned_train.csv', index=False)\n",
    "test_df.to_csv(PATH + 'cleaned_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del cleaned_train_comments, cleaned_test_comments\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embedding (Build a matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix...\n",
      "Null_word embeddings: 26629\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix...')\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "null_words = open(PATH + 'null_words.txt', 'w', encoding='utf-8')\n",
    "\n",
    "for word, idx in word_index.items():\n",
    "    if idx >= MAX_NB_WORDS:\n",
    "        null_words.write(word + ', ' + str(word_count_dict[word]) + '\\n')\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "    else:\n",
    "        null_words.write(word + ', ' + str(word_count_dict[word]) + '\\n')\n",
    "print('Null_word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting null_words...\n",
      "Sorting operation Done!\n"
     ]
    }
   ],
   "source": [
    "print('Sorting null_words...')\n",
    "null_dict = {}\n",
    "with open(PATH + 'null_words.txt', 'r', encoding='utf-8') as nullword:\n",
    "    for line in nullword:\n",
    "        word, count = line.strip('\\n').split(', ')\n",
    "        null_dict[word] = int(count)\n",
    "\n",
    "null_dict = sorted(null_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "with open(PATH + 'null_words.txt', 'w', encoding='utf-8') as output:\n",
    "    for word, count in null_dict:\n",
    "        output.write(word + ', ' + str(count) + '\\n')\n",
    "print('Sorting operation Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model with Using Tfidf Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=FOLD_COUNT, shuffle=False)\n",
    "tfidf_models = []\n",
    "for i, (train_idx, test_idx) in enumerate(kfold.split(train_tfidf_features)):\n",
    "    print('## In fold {} ##'.format(i + 1))\n",
    "    classifier = LogisticRegression(solver='sag', C=12.0)\n",
    "    \n",
    "    for class_name in list_classes:\n",
    "        print('Processing {} ...'.format(class_name))\n",
    "        train_target = train_df[class_name][train_idx]\n",
    "        \n",
    "        classifier.fit(train_tfidf_features[train_idx], train_target)\n",
    "        y_pred = classifier.predict(train_tfidf_features[train_idx])\n",
    "        \n",
    "        print('Training accuracy is {}'.format(accuracy_score(y_pred, train_target)))\n",
    "        \n",
    "        val_target = train_df[class_name][test_idx]\n",
    "        val_pred = classifier.predict(train_tfidf_features[test_idx])\n",
    "        \n",
    "        print('Validation accuracy is {}'.format(accuracy_score(val_pred, val_target)))\n",
    "        \n",
    "    tfidf_models.append(classifier)\n",
    "print('K-fold cross validation Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(tfidf_models):\n",
    "    print('## In Model {} ##'.format(i + 1))\n",
    "    # predictions = {'id': test_df['id']}\n",
    "    predictions = OrderedDict()\n",
    "    predictions['id'] = test_df['id']\n",
    "    \n",
    "    for class_name in list_classes:\n",
    "        predictions[class_name] = model.predict_proba(test_tfidf_features)[:, 1]\n",
    "        print('Predict the proba for {} Done!'.format(class_name))\n",
    "        print(predictions.keys())\n",
    "    \n",
    "    print(predictions.keys())\n",
    "    submission = pd.DataFrame.from_dict(predictions)\n",
    "    submission.to_csv('results/LR_Based/Logistic_Regression_Submission_{}.csv'.format(i), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = ['results/LR_Based/Logistic_Regression_Submission_{}.csv'.format(i) for i in range(0, FOLD_COUNT)]\n",
    "bagging(result_list, 'results/LR_Based/bagging.csv')\n",
    "print('Bagging operation Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExtraTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "# et_predictions = {'id': test_df['id']}\n",
    "et_predictions = OrderedDict()\n",
    "et_predictions['id'] = test_df['id']\n",
    "\n",
    "for class_name in list_classes:\n",
    "    train_target = train_df[class_name]\n",
    "    classifier = ExtraTreesClassifier(n_estimators=30)\n",
    "    \n",
    "    cv_score = np.mean(cross_val_score(classifier, train_tfidf_features, train_target, cv=10, scoring='roc_auc'))\n",
    "    accs.append(cv_score)\n",
    "    print('CV Score for class {} is {}'.format(class_name, cv_score))\n",
    "    \n",
    "    classifier.fit(train_tfidf_features, train_target)\n",
    "    et_predictions[class_name] = classifier.predict_proba(test_tfidf_features)[:, 1]\n",
    "    \n",
    "submission = pd.DataFrame.from_dict(et_predictions)\n",
    "submission.to_csv('result/LR_Based/ExtraTreesClassifier_Submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light-GBM For Ensembled_Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_every_feature_model(feature_data, label, feature_name, feature_test_data, fold_count, predict=False):\n",
    "    predictions = np.zeros(shape=(len(feature_test_data)))\n",
    "    fold_size = len(feature_data) // fold_count\n",
    "    \n",
    "    print('Feature name is {}'.format(feature_name))\n",
    "    auc_score = 0\n",
    "    for fold_id in range(0, fold_count):\n",
    "        print('## Fold In {} ##'.format(fold_id + 1))\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "        if fold_id == fold_size - 1:\n",
    "            fold_end = len(feature_data)\n",
    "            \n",
    "        train_x = np.concatenate((feature_data[:fold_start], feature_data[fold_end:]))\n",
    "        train_y = np.concatenate((label[:fold_start], label[fold_end:]))\n",
    "        \n",
    "        val_x = feature_data[fold_start:fold_end]\n",
    "        val_y = label[fold_start:fold_end]\n",
    "        \n",
    "        lgb_train = lgb.Dataset(train_x, train_y)\n",
    "        lgb_val = lgb.Dataset(val_x, val_y)\n",
    "        \n",
    "        lgbm_model = lgb.LGBMClassifier(max_depth=5, metric='auc', n_estimators=10000, num_leaves=32, boosting_type='gbdt', \\\n",
    "                                       learning_rate=0.01, feature_fraction=0.3, bagging_fraction=0.8, bagging_freq=5, reg_lambda=0)\n",
    "        lgbm_model.fit(X=train_x, y=train_y, eval_metric=['auc', 'binary_logloss'], eval_set=(val_x, val_y), early_stopping_rounds=1000, verbose=500)\n",
    "        auc_score += lgbm_model.best_score_['valid_0']['auc']\n",
    "        lgb.plot_importance(lgbm_model, max_num_features=30)\n",
    "        plt.show()\n",
    "        if predict:\n",
    "            prediction = lgbm_model.predict_proba(feature_test_data)[:, 1]\n",
    "            predictions += prediction\n",
    "            del lgbm_model\n",
    "    predictions /= fold_count\n",
    "    print('Training LightGBM Done!')\n",
    "    return predictions, auc_score / fold_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, auc_scores = [], []\n",
    "for i, feature_name in enumerate(list_classes):\n",
    "    prediction, auc = fit_every_feature_model(train_ensembled_data[i], train_df[feature_name].values, feature_name, test_ensembled_data[i], 10, predict=True)\n",
    "    auc_scores.append(auc)\n",
    "    predictions.append(prediction)\n",
    "print('Overall AUC Score is {}'.format(sum(auc_scores) / 6))\n",
    "print('For each:'.format(auc_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('datasets/sample_submission.csv')\n",
    "for i, feature_name in enumerate(list_classes):\n",
    "    submission[feature_name] = predictions[i]\n",
    "submission.to_csv('results/LightGBM/LightGBM_with_Meta_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAMP = 'model_pool/av_rnn/pavel_rnn_%.2f_%.2f'%(0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_model_by_auc(model, batch_size, train_x, train_y, val_x, val_y):\n",
    "    best_auc = -1\n",
    "    best_weight = None\n",
    "    best_epoch = 0\n",
    "    current_epoch = 1\n",
    "    \n",
    "    while True:\n",
    "        model.fit(train_x, train_y, batch_size=batch_size, epoch=1, validation_data=[val_x, val_y])\n",
    "        y_pred = model.predict(val_x, batch_size=batch_size)\n",
    "        current_auc = roc_auc_score(val_y, y_pred)\n",
    "        print('Epoch {} auc {:.6f} best_auc {:.6f}'.format(current_epoch, current_auc, best_auc))\n",
    "        current_epoch += 1\n",
    "        if best_auc < current_auc or best_auc == -1:\n",
    "            best_auc = current_auc\n",
    "            best_weight = model.get_weights()\n",
    "            best_epoch = current_epoch\n",
    "        else:\n",
    "            # early stop\n",
    "            if current_epoch - best_epoch == 5:\n",
    "                break\n",
    "                \n",
    "    model.set_weights(best_weights)\n",
    "    return model, best_auc\n",
    "\n",
    "def _train_model_by_logloss(model, batch_size, train_x, pos_train_x, train_y, val_x, pos_val_x, val_y, fold_id):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=7)\n",
    "    best_model_path = STAMP + str(fold_id) + '.h5'\n",
    "    model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n",
    "    train_data = {'Onehot': train_x, 'POS': pos_train_x}\n",
    "    val_data = {'Onehot': val_x, 'POS': pos_val_x}\n",
    "    \n",
    "    hist = model.fit(train_data, train_y, validation_data=(val_data, val_y), epochs=50, batch_size=batch_size, shuffle=True, callbacks=[early_stopping, model_checkpoint])\n",
    "    best_val_score = min(hist.history['val_loss'])\n",
    "    predictions = model.predict(val_data)\n",
    "    auc = roc_auc_score(val_y, predictions)\n",
    "    print('AUC Score', auc)\n",
    "    return model, best_val_score, auc, predictions\n",
    "\n",
    "def train_folds(x, pos_x, y, fold_count, batch_size, get_model_func):\n",
    "    fold_size = len(x) // fold_count\n",
    "    models = []\n",
    "    fold_predictions = []\n",
    "    score, total_auc = 0, 0\n",
    "    for fold_id in range(0, fold_count):\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "        \n",
    "        if fold_id == fold_count - 1:\n",
    "            fold_end = len(x)\n",
    "            \n",
    "        train_x = np.concatenate((x[:fold_start], x[fold_end:]))\n",
    "        train_y = np.concatenate((y[:fold_start], y[fold_end:]))\n",
    "        \n",
    "        val_x = x[fold_start: fold_end]\n",
    "        val_y = y[fold_start: fold_end]\n",
    "        \n",
    "        pos_train_x = np.concatenate((pos_x[:fold_start], pos_x[fold_end:]))\n",
    "        pos_val_x = pos_x[fold_start: fold_end]\n",
    "        print('## In fold {} ## : '.format(fold_id + 1))\n",
    "        model, best_val_score, auc, fold_prediction = _train_model_by_logloss(get_model_func, batch_size, train_x, pos_train_x, train_y, val_x, pos_val_x, val_y, fold_id)\n",
    "        \n",
    "        score += best_val_score\n",
    "        total_auc += auc\n",
    "        fold_predictions.append(fold_prediction)\n",
    "        models.append(model)\n",
    "    return models, score / fold_count, total_auc / fold_count , fold_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Model for computing a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter per channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "        \n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        \n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        attn_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(attn_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, attn_weights]\n",
    "        return result\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "    \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMaxPooling(Layer):\n",
    "    \"\"\"\n",
    "    K-max pooling layer that extracts the k-highest activations from a sequence (2nd dimension).\n",
    "    With TensorFlow Backend.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_spec = InputSpec(ndim=3)\n",
    "        self.k = k\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[2] * self.k)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # top_k function can only be applied along the last dimension\n",
    "        shifted_input = tf.transpose(inputs, [0, 2, 1])\n",
    "        top_k = tf.nn.top_k(shifted_input, self.k, True, None)[0]\n",
    "        return Flatten()(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_av_cnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    filter_nums = 300\n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.25)(embedding_sequences)\n",
    "    \n",
    "    conv_0 = Conv1D(filter_nums, 1, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_1 = Conv1D(filter_nums, 2, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_2 = Conv1D(filter_nums, 3, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_3 = Conv1D(filter_nums, 4, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    \n",
    "    attn_0 = AttentionWeightedAverage()(conv_0)\n",
    "    avg_0 = GlobalAveragePooling1D()(conv_0)\n",
    "    maxpool_0 = GlobalMaxPooling1D()(conv_0)\n",
    "    \n",
    "    attn_1 = AttentionWeightedAverage()(conv_1)\n",
    "    avg_1 = GlobalAveragePooling1D()(conv_1)\n",
    "    maxpool_1 = GlobalMaxPooling1D()(conv_1)\n",
    "    \n",
    "    attn_2 = AttentionWeightedAverage()(conv_2)\n",
    "    avg_2 = GlobalAveragePooling1D()(conv_2)\n",
    "    maxpool_2 = GlobalMaxPooling1D()(conv_2)\n",
    "    \n",
    "    attn_3 = AttentionWeightedAverage()(conv_3)\n",
    "    avg_3 = GlobalAveragePooling1D()(conv_3)\n",
    "    maxpool_3 = GlobalMaxPooling1D()(conv_3)\n",
    "    \n",
    "    merged_maxpool = merge([maxpool_0, maxpool_1, maxpool_2, maxpool_3], mode='concat', concat_axis=1)\n",
    "    merged_attn = merge([attn_0, attn_1, attn_2, attn_3], mode='concat', concat_axis=1)\n",
    "    merged_avg = merge([avg_0, avg_1, avg_2, avg_3], mode='concat', concat_axis=1)\n",
    "    merged_tensor = merge([merged_maxpool, merged_attn, merged_avg], mode='concat', concat_axis=1)\n",
    "    \n",
    "    output = Dropout(0.7)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_av_pos_cnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words, \n",
    "                                embedding_dim, \n",
    "                                weights=[embedding_matrix], \n",
    "                                input_length=max_sequence_length, \n",
    "                                trainable=False)\n",
    "    \n",
    "    pos_embedding_layer = Embedding(50,\n",
    "                                    30,\n",
    "                                    input_length=max_sequence_length,\n",
    "                                    trainable=True)\n",
    "    \n",
    "    filter_nums = 325\n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    pos_input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='POS')\n",
    "    \n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    pos_sequences = pos_embedding_layer(pos_input_layer)\n",
    "    merged_embedding_layer = concatenate([embedding_sequences, pos_sequences])\n",
    "    final_embedding_sequences = SpatialDropout1D(0.25)(embedding_layer)\n",
    "    \n",
    "    conv_0 = Conv1D(filter_nums, 1, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_1 = Conv1D(filter_nums, 2, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_2 = Conv1D(filter_nums, 3, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_3 = Conv1D(filter_nums, 4, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "\n",
    "    attn_0 = AttentionWeightedAverage()(conv_0)\n",
    "    avg_0 = GlobalAveragePooling1D()(conv_0)\n",
    "    maxpool_0 = GlobalMaxPooling1D()(conv_0)\n",
    "    \n",
    "    attn_1 = AttentionWeightedAverage()(conv_1)\n",
    "    avg_1 = GlobalAveragePooling1D()(conv_1)\n",
    "    maxpool_1 = GlobalMaxPooling1D()(conv_1)\n",
    "    \n",
    "    attn_2 = AttentionWeightedAverage()(conv_2)\n",
    "    avg_2 = GlobalAveragePooling1D()(conv_2)\n",
    "    maxpool_2 = GlobalMaxPooling1D()(conv_2)\n",
    "    \n",
    "    attn_3 = AttentionWeightedAverage()(conv_3)\n",
    "    avg_3 = GlobalAveragePooling1D()(conv_3)\n",
    "    maxpool_3 = GlobalMaxPooling1D()(conv_3)\n",
    "    \n",
    "    merged_tensor_maxpool = merge([maxpool_0, maxpool_1, maxpool_2, maxpool_3], mode='concat', concat_axis=1)\n",
    "    merged_tensor_attn = merge([attn_0, attn_1, attn_2, attn_3], mode='concat', concat_axis=1)\n",
    "    merged_tensor_avg = merge([avg_0, avg_1, avg_2, avg_3], mode='concat', concat_axis=1)\n",
    "    merged_tensor = merge([merged_tensor_maxpool, merged_tensor_attn, merged_tensor_avg], mode='concat', concat_axis=1)\n",
    "    \n",
    "    output = Dropout(0.7)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=[input_layer, pos_input_layer], outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n",
    "    model.compile(loss='binary_corssentropy', optimizer=adam_optimizer, matrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmax_text_cnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    filter_nums = 180\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(embedding_sequences)\n",
    "    \n",
    "    conv_0 = Conv1D(filter_nums, 1, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_1 = Conv1D(filter_nums, 2, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_2 = Conv1D(filter_nums, 3, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_3 = Conv1D(filter_nums, 4, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    \n",
    "    maxpool_0 = KMaxPooling(k=3)(conv_0)\n",
    "    maxpool_1 = KMaxPooling(k=3)(conv_1)\n",
    "    maxpool_2 = KMaxPooling(k=3)(conv_2)\n",
    "    maxpool_3 = KMaxPooling(k=3)(conv_3)\n",
    "    \n",
    "    merged_tensor = merge([maxpool_0, maxpool_1, maxpool_2, maxpool_3], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.6)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rcnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    recurrent_units = 64\n",
    "    filter_nums = 128\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.2)(embedding_sequences)\n",
    "    \n",
    "    rnn_layer = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(final_embedding_sequences)\n",
    "    conv_layer = Conv1D(filter_nums, 2, kernel_initializer='normal', padding='valid', activation='relu', strides=1)(rnn_layer)\n",
    "    \n",
    "    maxpool = GlobalMaxPooling1D()(conv_layer)\n",
    "    attn = AttentionWeightedAverage()(conv_layer)\n",
    "    avg = GlobalAveragePooling1D()(conv_layer)\n",
    "    \n",
    "    merged_tensor = merge([maxpool, attn, avg], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.5)(merged_tensor)\n",
    "    output = Dense(units=120, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_av_rnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    recurrent_units = 64\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.25)(embedding_sequences)\n",
    "    \n",
    "    rnn_layer_0 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(final_embedding_sequences)\n",
    "    rnn_layer_1 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(rnn_layer_0)\n",
    "    merged_rnn_layer = merge([rnn_layer_0, rnn_layer_1], mode='concat', concat_axis=2)\n",
    "    \n",
    "    last_layer = Lambda(lambda t: t[:, -1], name='last_layer')(merged_rnn_layer)\n",
    "    maxpool = GlobalMaxPooling1D()(merged_rnn_layer)\n",
    "    attn = AttentionWeightedAverage()(merged_rnn_layer)\n",
    "    avg = GlobalAveragePooling1D()(merged_rnn_layer)\n",
    "    \n",
    "    merged_tensor = merge([last_layer, maxpool, attn, avg], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.5)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_av_pos_rnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    pos_embedding_layer = Embedding(50,\n",
    "                                    35,\n",
    "                                    input_length=max_sequence_length,\n",
    "                                    trainable=True)\n",
    "    \n",
    "    recurrent_units = 64\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    pos_input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='POS')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    pos_sequences = pos_embedding_layer(pos_input_layer)\n",
    "    merged_embedding_layer = concatenate([embedding_sequences, pos_sequences], axis=2)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.2)(merged_embedding_layer)\n",
    "    \n",
    "    rnn_layer_0 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(final_embedding_sequences)\n",
    "    rnn_layer_0 = SpatialDropout1D(0.3)(rnn_layer_0)\n",
    "    rnn_layer_1 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(rnn_layer_0)\n",
    "    \n",
    "    last_layer = Lambda(lambda t: t[:, -1], name='last_layer')(rnn_layer_1)\n",
    "    maxpool = GlobalMaxPooling1D()(rnn_layer_1)\n",
    "    attn = AttentionWeightedAverage()(rnn_layer_1)\n",
    "    avg = GlobalAveragePooling1D()(rnn_layer_1)\n",
    "    \n",
    "    merged_tensor = merge([last_layer, maxpool, attn, avg], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.5)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=[input_layer, pos_input_layer], outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dropout_bigru(nb_words, embedding_dims, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Emebdding(nb_words,\n",
    "                                embedding_dims,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    recurrent_units = 64\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.2)(embedding_sequences)\n",
    "    \n",
    "    rnn_layer = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(final_embedding_sequences)\n",
    "    rnn_layer = Dropout(0.35)(rnn_layer)\n",
    "    rnn_layer = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(rnn_layer)\n",
    "    \n",
    "    last_layer = Lambda(lambda t: t[:, -1])(rnn_layer)\n",
    "    maxpool = GlobalMaxPooling1D()(rnn_layer)\n",
    "    avg = GlobalAveragePooling1D()(rnn_layer)\n",
    "    \n",
    "    merged_tensor = merge([last_layer, maxpool, avg], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.5)(merged_tensor)\n",
    "    output = Dense(units=72, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Onehot (InputLayer)             (None, 350)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 350, 300)     30000000    Onehot[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_7 (SpatialDro (None, 350, 300)     0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 350, 128)     140544      spatial_dropout1d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 350, 128)     74496       bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "merge_8 (Merge)                 (None, 350, 256)     0           bidirectional_11[0][0]           \n",
      "                                                                 bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "last_layer (Lambda)             (None, 256)          0           merge_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 256)          0           merge_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_weighted_average_4 (A (None, 256)          256         merge_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 256)          0           merge_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge_9 (Merge)                 (None, 1024)         0           last_layer[0][0]                 \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 attention_weighted_average_4[0][0\n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1024)         0           merge_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 144)          147600      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 6)            870         dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 30,363,766\n",
      "Trainable params: 363,766\n",
      "Non-trainable params: 30,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:16: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python3.5/dist-packages/keras/legacy/layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:23: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'fasttext-avrnn-' + str(nb_words) + 'vocabulary-' + str(MAX_SEQUENCE_LENGTH) + 'length'\n",
    "model = get_av_rnn(nb_words, EMBEDDING_DIM, embedding_matrix, MAX_SEQUENCE_LENGTH, 6)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## In fold 1 ## : \n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 155s 1ms/step - loss: 0.0300 - acc: 0.9876 - val_loss: 0.0365 - val_acc: 0.9856\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 161s 1ms/step - loss: 0.0292 - acc: 0.9880 - val_loss: 0.0375 - val_acc: 0.9848\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0285 - acc: 0.9883 - val_loss: 0.0386 - val_acc: 0.9841\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 163s 1ms/step - loss: 0.0277 - acc: 0.9887 - val_loss: 0.0400 - val_acc: 0.9843\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 156s 1ms/step - loss: 0.0273 - acc: 0.9887 - val_loss: 0.0403 - val_acc: 0.9838\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 163s 1ms/step - loss: 0.0265 - acc: 0.9890 - val_loss: 0.0398 - val_acc: 0.9849\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0261 - acc: 0.9891 - val_loss: 0.0415 - val_acc: 0.9838\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0253 - acc: 0.9895 - val_loss: 0.0428 - val_acc: 0.9842\n",
      "AUC Score 0.9899274000267283\n",
      "## In fold 2 ## : \n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 157s 1ms/step - loss: 0.0271 - acc: 0.9891 - val_loss: 0.0220 - val_acc: 0.9912\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 157s 1ms/step - loss: 0.0261 - acc: 0.9893 - val_loss: 0.0237 - val_acc: 0.9902\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 157s 1ms/step - loss: 0.0250 - acc: 0.9898 - val_loss: 0.0250 - val_acc: 0.9897\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 156s 1ms/step - loss: 0.0244 - acc: 0.9900 - val_loss: 0.0243 - val_acc: 0.9899\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 157s 1ms/step - loss: 0.0242 - acc: 0.9901 - val_loss: 0.0248 - val_acc: 0.9896\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 156s 1ms/step - loss: 0.0235 - acc: 0.9903 - val_loss: 0.0263 - val_acc: 0.9889\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 157s 1ms/step - loss: 0.0232 - acc: 0.9905 - val_loss: 0.0273 - val_acc: 0.9885\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 156s 1ms/step - loss: 0.0226 - acc: 0.9907 - val_loss: 0.0265 - val_acc: 0.9889\n",
      "AUC Score 0.9963323077433461\n",
      "## In fold 3 ## : \n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 156s 1ms/step - loss: 0.0234 - acc: 0.9905 - val_loss: 0.0154 - val_acc: 0.9939\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 158s 1ms/step - loss: 0.0227 - acc: 0.9907 - val_loss: 0.0183 - val_acc: 0.9926\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 156s 1ms/step - loss: 0.0224 - acc: 0.9908 - val_loss: 0.0173 - val_acc: 0.9930\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 158s 1ms/step - loss: 0.0217 - acc: 0.9911 - val_loss: 0.0185 - val_acc: 0.9926\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 156s 1ms/step - loss: 0.0213 - acc: 0.9913 - val_loss: 0.0186 - val_acc: 0.9925\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 158s 1ms/step - loss: 0.0210 - acc: 0.9914 - val_loss: 0.0192 - val_acc: 0.9923\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0205 - acc: 0.9916 - val_loss: 0.0195 - val_acc: 0.9919\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0202 - acc: 0.9918 - val_loss: 0.0202 - val_acc: 0.9917\n",
      "AUC Score 0.9978572327170067\n",
      "## In fold 4 ## : \n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 163s 1ms/step - loss: 0.0209 - acc: 0.9914 - val_loss: 0.0129 - val_acc: 0.9951\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 161s 1ms/step - loss: 0.0205 - acc: 0.9916 - val_loss: 0.0140 - val_acc: 0.9943\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0199 - acc: 0.9919 - val_loss: 0.0150 - val_acc: 0.9940\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 159s 1ms/step - loss: 0.0197 - acc: 0.9919 - val_loss: 0.0154 - val_acc: 0.9936\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 167s 1ms/step - loss: 0.0192 - acc: 0.9921 - val_loss: 0.0163 - val_acc: 0.9936\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 167s 1ms/step - loss: 0.0191 - acc: 0.9921 - val_loss: 0.0164 - val_acc: 0.9932\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 167s 1ms/step - loss: 0.0184 - acc: 0.9925 - val_loss: 0.0179 - val_acc: 0.9924\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0183 - acc: 0.9926 - val_loss: 0.0181 - val_acc: 0.9922\n",
      "AUC Score 0.9981328358006345\n",
      "## In fold 5 ## : \n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0191 - acc: 0.9922 - val_loss: 0.0107 - val_acc: 0.9963\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0190 - acc: 0.9922 - val_loss: 0.0116 - val_acc: 0.9959\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0183 - acc: 0.9925 - val_loss: 0.0120 - val_acc: 0.9955\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0181 - acc: 0.9926 - val_loss: 0.0136 - val_acc: 0.9949\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0177 - acc: 0.9929 - val_loss: 0.0131 - val_acc: 0.9951\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0175 - acc: 0.9928 - val_loss: 0.0137 - val_acc: 0.9946\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0172 - acc: 0.9931 - val_loss: 0.0149 - val_acc: 0.9941\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0170 - acc: 0.9931 - val_loss: 0.0159 - val_acc: 0.9937\n",
      "AUC Score 0.998630232097261\n",
      "## In fold 6 ## : \n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0175 - acc: 0.9930 - val_loss: 0.0089 - val_acc: 0.9968\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0173 - acc: 0.9930 - val_loss: 0.0104 - val_acc: 0.9963\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0166 - acc: 0.9933 - val_loss: 0.0110 - val_acc: 0.9957\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0168 - acc: 0.9932 - val_loss: 0.0108 - val_acc: 0.9958\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0163 - acc: 0.9934 - val_loss: 0.0111 - val_acc: 0.9955\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0163 - acc: 0.9934 - val_loss: 0.0115 - val_acc: 0.9953\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0162 - acc: 0.9936 - val_loss: 0.0121 - val_acc: 0.9951\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0159 - acc: 0.9936 - val_loss: 0.0127 - val_acc: 0.9949\n",
      "AUC Score 0.9989301539205808\n",
      "## In fold 7 ## : \n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0166 - acc: 0.9934 - val_loss: 0.0082 - val_acc: 0.9974\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0162 - acc: 0.9935 - val_loss: 0.0084 - val_acc: 0.9970\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0160 - acc: 0.9936 - val_loss: 0.0090 - val_acc: 0.9969\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0158 - acc: 0.9937 - val_loss: 0.0110 - val_acc: 0.9958\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0156 - acc: 0.9938 - val_loss: 0.0101 - val_acc: 0.9961\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0155 - acc: 0.9938 - val_loss: 0.0102 - val_acc: 0.9962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0149 - acc: 0.9940 - val_loss: 0.0122 - val_acc: 0.9954\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0150 - acc: 0.9940 - val_loss: 0.0116 - val_acc: 0.9954\n",
      "AUC Score 0.9991669972401808\n",
      "## In fold 8 ## : \n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0155 - acc: 0.9939 - val_loss: 0.0069 - val_acc: 0.9977\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0153 - acc: 0.9939 - val_loss: 0.0076 - val_acc: 0.9976\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0150 - acc: 0.9940 - val_loss: 0.0090 - val_acc: 0.9971\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0147 - acc: 0.9941 - val_loss: 0.0106 - val_acc: 0.9961\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 167s 1ms/step - loss: 0.0149 - acc: 0.9941 - val_loss: 0.0095 - val_acc: 0.9965\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0144 - acc: 0.9943 - val_loss: 0.0107 - val_acc: 0.9961\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0144 - acc: 0.9943 - val_loss: 0.0100 - val_acc: 0.9962\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0140 - acc: 0.9944 - val_loss: 0.0111 - val_acc: 0.9956\n",
      "AUC Score 0.999321419008738\n",
      "## In fold 9 ## : \n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0151 - acc: 0.9941 - val_loss: 0.0068 - val_acc: 0.9979\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0147 - acc: 0.9941 - val_loss: 0.0070 - val_acc: 0.9977\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0144 - acc: 0.9943 - val_loss: 0.0068 - val_acc: 0.9977\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0142 - acc: 0.9943 - val_loss: 0.0078 - val_acc: 0.9971\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0140 - acc: 0.9945 - val_loss: 0.0076 - val_acc: 0.9973\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0139 - acc: 0.9945 - val_loss: 0.0081 - val_acc: 0.9971\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0137 - acc: 0.9946 - val_loss: 0.0092 - val_acc: 0.9964\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0138 - acc: 0.9946 - val_loss: 0.0099 - val_acc: 0.9962\n",
      "Epoch 9/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0138 - acc: 0.9946 - val_loss: 0.0093 - val_acc: 0.9964\n",
      "Epoch 10/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0133 - acc: 0.9947 - val_loss: 0.0091 - val_acc: 0.9965\n",
      "AUC Score 0.9994505890254554\n",
      "## In fold 10 ## : \n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/50\n",
      "143613/143613 [==============================] - 165s 1ms/step - loss: 0.0140 - acc: 0.9945 - val_loss: 0.0054 - val_acc: 0.9984\n",
      "Epoch 2/50\n",
      "143613/143613 [==============================] - 164s 1ms/step - loss: 0.0140 - acc: 0.9945 - val_loss: 0.0053 - val_acc: 0.9984\n",
      "Epoch 3/50\n",
      "143613/143613 [==============================] - 164s 1ms/step - loss: 0.0137 - acc: 0.9946 - val_loss: 0.0061 - val_acc: 0.9981\n",
      "Epoch 4/50\n",
      "143613/143613 [==============================] - 164s 1ms/step - loss: 0.0133 - acc: 0.9947 - val_loss: 0.0074 - val_acc: 0.9973\n",
      "Epoch 5/50\n",
      "143613/143613 [==============================] - 164s 1ms/step - loss: 0.0133 - acc: 0.9947 - val_loss: 0.0067 - val_acc: 0.9977\n",
      "Epoch 6/50\n",
      "143613/143613 [==============================] - 164s 1ms/step - loss: 0.0133 - acc: 0.9948 - val_loss: 0.0073 - val_acc: 0.9974\n",
      "Epoch 7/50\n",
      "143613/143613 [==============================] - 164s 1ms/step - loss: 0.0132 - acc: 0.9949 - val_loss: 0.0087 - val_acc: 0.9968\n",
      "Epoch 8/50\n",
      "143613/143613 [==============================] - 164s 1ms/step - loss: 0.0130 - acc: 0.9949 - val_loss: 0.0091 - val_acc: 0.9964\n",
      "Epoch 9/50\n",
      "143613/143613 [==============================] - 164s 1ms/step - loss: 0.0131 - acc: 0.9948 - val_loss: 0.0086 - val_acc: 0.9967\n",
      "AUC Score 0.9996250240344368\n"
     ]
    }
   ],
   "source": [
    "models, val_loss, total_auc, fold_predictions = train_folds(train_data, pos_train_data, train_labels, FOLD_COUNT, BATCH_SIZE, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall val-loss: 0.013377069772156469, AUC 0.9977374191614368\n"
     ]
    }
   ],
   "source": [
    "print('Overall val-loss: {}, AUC {}'.format(val_loss, total_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting testing results...\n",
      "153164/153164 [==============================] - 67s 440us/step\n",
      "153164/153164 [==============================] - 67s 437us/step\n",
      "153164/153164 [==============================] - 67s 438us/step\n",
      "153164/153164 [==============================] - 67s 438us/step\n",
      "153164/153164 [==============================] - 67s 438us/step\n",
      "153164/153164 [==============================] - 67s 438us/step\n",
      "153164/153164 [==============================] - 67s 438us/step\n",
      "153164/153164 [==============================] - 67s 439us/step\n",
      "153164/153164 [==============================] - 67s 438us/step\n",
      "153164/153164 [==============================] - 67s 438us/step\n"
     ]
    }
   ],
   "source": [
    "submit_path_prefix = 'results/RNN_Based/' + model_name\n",
    "\n",
    "print('Predicting testing results...')\n",
    "test_predicts_list = []\n",
    "for fold_id, model in enumerate(models):\n",
    "    test_data_dict = {'Onehot': test_data, 'POS': pos_test_data}\n",
    "    test_predict = model.predict(test_data_dict, batch_size=BATCH_SIZE, verbose=1)\n",
    "    test_predicts_list.append(test_predict)\n",
    "    np.save('parameters_pool/AVRNN/{}-AV-POS-RNN.npy'.format(fold_id), test_predict)\n",
    "    \n",
    "test_predicts = np.zeros(test_predicts_list[0].shape)\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts += fold_predict\n",
    "test_predicts /= len(test_predicts_list)\n",
    "\n",
    "test_ids = test_df['id'].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "\n",
    "test_predicts = pd.DataFrame(data=test_predicts, columns=list_classes)\n",
    "test_predicts['id'] = test_ids\n",
    "test_predicts = test_predicts[['id'] + list_classes]\n",
    "submit_path = submit_path_prefix + '-L{:4f}-A{:4f}.csv'.format(val_loss, total_auc)\n",
    "test_predicts.to_csv(submit_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOB (Out-of-Bag) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC 0.9972381428584968\n"
     ]
    }
   ],
   "source": [
    "train_fold_predictions = np.concatenate((fold_predictions), axis=0)\n",
    "train_auc = roc_auc_score(train_labels, train_fold_predictions)\n",
    "print('Training AUC', train_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting training results...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Predicting training results...')\n",
    "train_ids = train_df['id'].values\n",
    "train_ids = train_ids.reshape((len(train_ids), 1))\n",
    "\n",
    "train_predicts = pd.DataFrame(data=train_fold_predictions, columns=list_classes)\n",
    "train_predicts['id'] = train_ids\n",
    "train_predicts = train_predicts[['id'] + list_classes]\n",
    "submit_path = submit_path_prefix + '-(Train)-L{:4f}-A{:4f}.csv'.format(val_loss, train_auc)\n",
    "train_predicts.to_csv(submit_path, index=False)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Ensemble (For Test Format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging(arrs, path):\n",
    "    print(\"Doing ensemble on\")\n",
    "    subs = []\n",
    "    for arr in arrs:\n",
    "        print(arr)\n",
    "        subs.append(pd.read_csv(arr))\n",
    "    \n",
    "    for sub in subs[1:]:\n",
    "        for c in list_classes:\n",
    "            subs[0][c] += sub[c]\n",
    "    \n",
    "    for c in list_classes:\n",
    "        subs[0][c] /= len(subs)\n",
    "        \n",
    "    subs[0].to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_corr(arr1, arr2):\n",
    "    res = 0\n",
    "    for col in arr1.columns.values[1:]:\n",
    "        cur = arr1[col].corr(arr2[col])\n",
    "        corr = (arr1[col].rank() / len(arr1)).corr(arr2[col].rank() / len(arr2))\n",
    "        print(col, corr)\n",
    "        res += corr\n",
    "    print(\"Avg Rank: \", res / len(arr1.columns.values[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Estimation (For Train Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
