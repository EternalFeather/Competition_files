{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re, csv, codecs, operator, sys\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from textblob import TextBlob    # For pos-tagging\n",
    "\n",
    "from keras import optimizers, initializers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Lambda, Embedding, Dropout, Activation, SpatialDropout1D, Reshape, \\\n",
    "GlobalAveragePooling1D, merge, Flatten, Bidirectional, CuDNNGRU, add, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'datasets/'\n",
    "FAST_TEXT_EMBEDDING = 'pretrain_embedding/crawl-300d-2M.vec'\n",
    "CLEAN_WORD_PATH = None\n",
    "TRAIN_DATA_FILE = 'train.csv'\n",
    "TEST_DATA_FILE = 'test.csv'\n",
    "MAX_SEQUENCE_LENGTH = 350\n",
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300\n",
    "FOLD_COUNT = 10\n",
    "BATCH_SIZE = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrain Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrain_embedding(file):\n",
    "    print('Indexing word vectors')\n",
    "    embeddings_index = {}\n",
    "    f = open(file, 'r', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        try:\n",
    "            word = values[0]\n",
    "            coefs = np.array(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        except:\n",
    "            print(\"Error on: \", values[:3])\n",
    "    f.close()\n",
    "    print(\"Total %s word vectors\" % len(embeddings_index))\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Total 2000000 word vectors\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = load_pretrain_embedding(FAST_TEXT_EMBEDDING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('datasets/train.csv')\n",
    "test_df = pd.read_csv('datasets/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      "id               159571 non-null object\n",
      "comment_text     159571 non-null object\n",
      "toxic            159571 non-null int64\n",
      "severe_toxic     159571 non-null int64\n",
      "obscene          159571 non-null int64\n",
      "threat           159571 non-null int64\n",
      "insult           159571 non-null int64\n",
      "identity_hate    159571 non-null int64\n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 9.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805  \n",
       "std         0.216627       0.093420  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.308619</td>\n",
       "      <td>0.676515</td>\n",
       "      <td>0.157058</td>\n",
       "      <td>0.647518</td>\n",
       "      <td>0.266009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>severe_toxic</th>\n",
       "      <td>0.308619</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.403014</td>\n",
       "      <td>0.123601</td>\n",
       "      <td>0.375807</td>\n",
       "      <td>0.201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obscene</th>\n",
       "      <td>0.676515</td>\n",
       "      <td>0.403014</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.141179</td>\n",
       "      <td>0.741272</td>\n",
       "      <td>0.286867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threat</th>\n",
       "      <td>0.157058</td>\n",
       "      <td>0.123601</td>\n",
       "      <td>0.141179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.150022</td>\n",
       "      <td>0.115128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insult</th>\n",
       "      <td>0.647518</td>\n",
       "      <td>0.375807</td>\n",
       "      <td>0.741272</td>\n",
       "      <td>0.150022</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.337736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>identity_hate</th>\n",
       "      <td>0.266009</td>\n",
       "      <td>0.201600</td>\n",
       "      <td>0.286867</td>\n",
       "      <td>0.115128</td>\n",
       "      <td>0.337736</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  toxic  severe_toxic   obscene    threat    insult  \\\n",
       "toxic          1.000000      0.308619  0.676515  0.157058  0.647518   \n",
       "severe_toxic   0.308619      1.000000  0.403014  0.123601  0.375807   \n",
       "obscene        0.676515      0.403014  1.000000  0.141179  0.741272   \n",
       "threat         0.157058      0.123601  0.141179  1.000000  0.150022   \n",
       "insult         0.647518      0.375807  0.741272  0.150022  1.000000   \n",
       "identity_hate  0.266009      0.201600  0.286867  0.115128  0.337736   \n",
       "\n",
       "               identity_hate  \n",
       "toxic               0.266009  \n",
       "severe_toxic        0.201600  \n",
       "obscene             0.286867  \n",
       "threat              0.115128  \n",
       "insult              0.337736  \n",
       "identity_hate       1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comments = train_df['comment_text'].values\n",
    "test_comments = test_df['comment_text'].values\n",
    "train_comments_lengths = [len(s) for s in train_comments]\n",
    "test_comments_lengths = [len(s) for s in test_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Train------\n",
      "MAX LENGTH:\t\t 5000\n",
      "AVG LENGTH:\t\t 394.0732213246768\n",
      "MIN LENGTH:\t\t 6\n",
      "STANDARD DIVISION:\t 590.7184309382144\n",
      "RANGE:\t\t\t 6  to  1575.5100832011055\n",
      "------Test------\n",
      "MAX LENGTH:\t\t 5000\n",
      "AVG LENGTH:\t\t 364.8751207855632\n",
      "MIN LENGTH:\t\t 1\n",
      "STANDARD DIVISION:\t 592.4901645516661\n",
      "RANGE:\t\t\t 1  to  1549.8554498888955\n"
     ]
    }
   ],
   "source": [
    "def explore_comments(arr):\n",
    "    print(\"MAX LENGTH:\\t\\t\", np.max(arr))\n",
    "    print(\"AVG LENGTH:\\t\\t\", np.average(arr))\n",
    "    print(\"MIN LENGTH:\\t\\t\", np.min(arr))\n",
    "    print(\"STANDARD DIVISION:\\t\", np.std(arr))\n",
    "    print(\"RANGE:\\t\\t\\t\", np.min(arr), \" to \", np.average(arr) + 2 * np.std(arr))\n",
    "    \n",
    "print(\"------Train------\")\n",
    "explore_comments(train_comments_lengths)\n",
    "\n",
    "print(\"------Test------\")\n",
    "explore_comments(test_comments_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Cleaned Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEAN_WORD_PATH == None:\n",
    "    ignored_words = set(stopwords.words('english'))\n",
    "else:\n",
    "    ignored_words = {}\n",
    "    with open(CLEAN_WORD_PATH, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip('\\n')\n",
    "            typo, correct = line.split(',')\n",
    "            ignored_words[typo] = correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal = re.compile(r'[^?!.,:a-z\\d ]', re.IGNORECASE)\n",
    "# Regex to remove all numerics\n",
    "replace_numbers = re.compile(r'd+', re.IGNORECASE)\n",
    "\n",
    "word_count_dict = defaultdict(int)\n",
    "\n",
    "def clean_datasets(text, remove_stopwords=False, stem_words=False, count_null_words=True, clean_wiki_tokens=True):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"”\", \"\\\"\", text)\n",
    "    text = re.sub(r\"“\", \"\\\"\", text)\n",
    "    text = replace_numbers.sub('', text)\n",
    "    \n",
    "    if count_null_words:\n",
    "        text = text.split()\n",
    "        for t in text:\n",
    "            word_count_dict[t] += 1\n",
    "        text = \" \".join(text)\n",
    "        \n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_train = train_df['comment_text'].fillna('no comment').values\n",
    "list_sentences_test = test_df['comment_text'].fillna('no comment').values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_labels = train_df[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data cleaning...\n"
     ]
    }
   ],
   "source": [
    "cleaned_train_comments, cleaned_test_comments = [], []\n",
    "print('Processing data cleaning...')\n",
    "\n",
    "for text in list_sentences_train:\n",
    "    cleaned_train_comments.append(clean_datasets(text))\n",
    "for text in list_sentences_test:\n",
    "    cleaned_test_comments.append(clean_datasets(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['comment_text_cleaned'] = cleaned_train_comments\n",
    "test_df['comment_text_cleaned'] = cleaned_test_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312735"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_comment_text = pd.concat([train_df['comment_text_cleaned'], test_df['comment_text_cleaned']], axis=0).fillna(\"unknown\")\n",
    "nrow_train = train_df.shape[0]\n",
    "all_comment_text.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312735, 50000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=50000)\n",
    "data = vectorizer.fit_transform(all_comment_text)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312735, 50000)\n"
     ]
    }
   ],
   "source": [
    "norm_data = MaxAbsScaler().fit_transform(data)\n",
    "print(norm_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='\"#$%&()+,-./:;<=>@[\\\\]^_`{|}~\\t\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically train vocab & tokenizer...\n",
      "Found 420473 unique tokens\n",
      "Shape of train_data tensor:  (159571, 350)\n",
      "Shape of train_label tensor:  (159571, 6)\n",
      "Shape of test_data tensor:  (153164, 350)\n"
     ]
    }
   ],
   "source": [
    "print('Automatically train vocab & tokenizer...')\n",
    "tokenizer.fit_on_texts(cleaned_train_comments + cleaned_test_comments)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(cleaned_train_comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(cleaned_test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of train_data tensor: ', train_data.shape)\n",
    "print('Shape of train_label tensor: ', train_labels.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor: ', test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pos Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2pos(sentence):\n",
    "    try:\n",
    "        tag = TextBlob(sentence).tags\n",
    "    except:\n",
    "        print(sentence)\n",
    "        \n",
    "    updated_sentence = ' '.join([i[0] for i in tag])\n",
    "    tagged = ' '.join([i[1] for i in tag])\n",
    "    return updated_sentence, tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_word_index = {v: k for k, v in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pos_comments = []\n",
    "Pos_updated_sentence = []\n",
    "for text in train_sequences:\n",
    "    text_ = ' '.join([inverse_word_index[word] for word in text])    # convert to word format\n",
    "    if not isinstance(text_, str):\n",
    "        print(text, '\\n', text_)\n",
    "    updated_sentence, tags = sent2pos(text_)\n",
    "    Pos_updated_sentence.append(updated_sentence)\n",
    "    Pos_comments.append(tags)\n",
    "    assert len(updated_sentence.split(' ')) == len(tags.split(' ')), \"T1 {} T2 {}\".format(len(text), len(tags.split()))\n",
    "    \n",
    "Pos_test_comments = []\n",
    "Pos_test_updated_sentence = []\n",
    "for text in test_sequences:\n",
    "    text_ = ' '.join([inverse_word_index[word] for word in text])\n",
    "    updated_test_sentence, test_tags = sent2pos(text_)\n",
    "    Pos_test_updated_sentence.append(updated_test_sentence)\n",
    "    Pos_test_comments.append(test_tags)\n",
    "    assert len(updated_test_sentence.split(' ')) == len(test_tags.split(' ')), \"T1 {} T2 {}\".format(len(text), len(test_tags.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tokenizer = Tokenizer(num_words=50, filters='\"#$%&()+,-./:;<=>@[\\\\]^_`{|}~\\t\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically train pos tokenizer...\n",
      "Found 34 unique tokens\n",
      "Shape of data tensor: (159571, 350)\n",
      "Shape of test_data tensor: (153164, 350)\n"
     ]
    }
   ],
   "source": [
    "print('Automatically train pos tokenizer...')\n",
    "pos_tokenizer.fit_on_texts(Pos_comments + Pos_test_comments)\n",
    "\n",
    "train_pos_sequences = pos_tokenizer.texts_to_sequences(Pos_comments)\n",
    "test_pos_sequences = pos_tokenizer.texts_to_sequences(Pos_test_comments)\n",
    "\n",
    "pos_word_index = pos_tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(pos_word_index))\n",
    "\n",
    "pos_train_data = pad_sequences(train_pos_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', pos_train_data.shape)\n",
    "\n",
    "pos_test_data = pad_sequences(test_pos_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', pos_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second time valid for tokenzier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically train pos tokenizer secondly...\n",
      "Found 420473 unique tokens\n",
      "Shape of train_data tensor: (159571, 350)\n",
      "Shape of train_label tensor: (159571, 6)\n",
      "Shape of test_data tensor: (153164, 350)\n"
     ]
    }
   ],
   "source": [
    "print('Automatically train pos tokenizer secondly...')\n",
    "cleaned_train_comments, cleaned_test_comments = [], []\n",
    "for text in Pos_updated_sentence:\n",
    "    cleaned_train_comments.append(clean_datasets(text))\n",
    "for text in Pos_test_updated_sentence:\n",
    "    cleaned_test_comments.append(clean_datasets(text))\n",
    "    \n",
    "train_sequences = tokenizer.texts_to_sequences(cleaned_train_comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(cleaned_test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of train_data tensor:', train_data.shape)\n",
    "print('Shape of train_label tensor:', train_labels.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['cleaned_comment_text'] = cleaned_train_comments\n",
    "test_df['cleaned_comment_text'] = cleaned_test_comments\n",
    "train_df.to_csv(PATH + 'cleaned_train.csv', index=False)\n",
    "test_df.to_csv(PATH + 'cleaned_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embedding (Build a matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix...\n",
      "Null_word embeddings: 41446\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix...')\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "null_words = open(PATH + 'null_words.txt', 'w', encoding='utf-8')\n",
    "\n",
    "for word, idx in word_index.items():\n",
    "    if idx >= MAX_NB_WORDS:\n",
    "        null_words.write(word + ', ' + str(word_count_dict[word]) + '\\n')\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "    else:\n",
    "        null_words.write(word + ', ' + str(word_count_dict[word]) + '\\n')\n",
    "print('Null_word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting null_words...\n",
      "Sorting operation Done!\n"
     ]
    }
   ],
   "source": [
    "print('Sorting null_words...')\n",
    "null_dict = {}\n",
    "with open(PATH + 'null_words.txt', 'r', encoding='utf-8') as nullword:\n",
    "    for line in nullword:\n",
    "        word, count = line.strip('\\n').split(', ')\n",
    "        null_dict[word] = int(count)\n",
    "\n",
    "null_dict = sorted(null_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "with open(PATH + 'null_words.txt', 'w', encoding='utf-8') as output:\n",
    "    for word, count in null_dict:\n",
    "        output.write(word + ', ' + str(count) + '\\n')\n",
    "print('Sorting operation Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAMP = 'model_pool/pavel_rnn_%.2f_%.2f'%(0.5, 0.5)\n",
    "\n",
    "def _train_model_by_auc(model, batch_size, train_x, train_y, val_x, val_y):\n",
    "    best_auc = -1\n",
    "    best_weight = None\n",
    "    best_epoch = 0\n",
    "    current_epoch = 1\n",
    "    \n",
    "    while True:\n",
    "        model.fit(train_x, train_y, batch_size=batch_size, epoch=1, validation_data=[val_x, val_y])\n",
    "        y_pred = model.predict(val_x, batch_size=batch_size)\n",
    "        current_auc = roc_auc_score(val_y, y_pred)\n",
    "        print('Epoch {} auc {:.6f} best_auc {:.6f}'.format(current_epoch, current_auc, best_auc))\n",
    "        current_epoch += 1\n",
    "        if best_auc < current_auc or best_auc == -1:\n",
    "            best_auc = current_auc\n",
    "            best_weight = model.get_weights()\n",
    "            best_epoch = current_epoch\n",
    "        else:\n",
    "            # early stop\n",
    "            if current_epoch - best_epoch == 5:\n",
    "                break\n",
    "                \n",
    "    model.set_weights(best_weights)\n",
    "    return model, best_auc\n",
    "\n",
    "def _train_model_by_logloss(model, batch_size, train_x, pos_train_x, train_y, val_x, pos_val_x, val_y, fold_id):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=7)\n",
    "    best_model_path = STAMP + str(fold_id) + '.h5'\n",
    "    model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n",
    "    train_data = {'Onehot': train_x, 'POS': pos_train_x}\n",
    "    val_data = {'Onehot': val_x, 'POS': pos_val_x}\n",
    "    \n",
    "    hist = model.fit(train_data, train_y, validation_data=(val_data, val_y), epochs=50, batch_size=batch_size, shuffle=True, callbacks=[early_stopping, model_checkpoint])\n",
    "    best_val_score = min(hist.history['val_loss'])\n",
    "    predictions = model.predict(val_data)\n",
    "    auc = roc_auc_score(val_y, predictions)\n",
    "    print('AUC Score', auc)\n",
    "    return model, best_val_score, auc, predictions\n",
    "\n",
    "def train_folds(x, pos_x, y, fold_count, batch_size, get_model_func):\n",
    "    fold_size = len(x) // fold_count\n",
    "    models = []\n",
    "    fold_predictions = []\n",
    "    score, total_auc = 0, 0\n",
    "    for fold_id in range(0, fold_count):\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "        \n",
    "        if fold_id == fold_count - 1:\n",
    "            fold_end = len(x)\n",
    "            \n",
    "        train_x = np.concatenate((x[:fold_start], x[fold_end:]))\n",
    "        train_y = np.concatenate((y[:fold_start], y[fold_end:]))\n",
    "        \n",
    "        val_x = x[fold_start: fold_end]\n",
    "        val_y = y[fold_start: fold_end]\n",
    "        \n",
    "        pos_train_x = np.concatenate((pos_x[:fold_start], pos_x[fold_end:]))\n",
    "        pos_val_x = pos_x[fold_start: fold_end]\n",
    "        print('In fold #', fold_id)\n",
    "        model, best_val_score, auc, fold_prediction = _train_model_by_logloss(get_model_func, batch_size, train_x, pos_train_x, train_y, val_x, pos_val_x, val_y, fold_id)\n",
    "        \n",
    "        score += best_val_score\n",
    "        total_auc += auc\n",
    "        fold_predictions.append(fold_prediction)\n",
    "        models.append(model)\n",
    "    return models, score / fold_count, total_auc / fold_count , fold_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Model for computing a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter per channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "        \n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        \n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        attn_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(attn_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, attn_weights]\n",
    "        return result\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "    \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMaxPooling(Layer):\n",
    "    \"\"\"\n",
    "    K-max pooling layer that extracts the k-highest activations from a sequence (2nd dimension).\n",
    "    With TensorFlow Backend.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_spec = InputSpec(ndim=3)\n",
    "        self.k = k\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[2] * self.k)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # top_k function can only be applied along the last dimension\n",
    "        shifted_input = tf.transpose(inputs, [0, 2, 1])\n",
    "        top_k = tf.nn.top_k(shifted_input, k=self.k, True, None)[0]\n",
    "        return Flatten()(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_av_cnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    filter_nums = 300\n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.25)(embedding_sequences)\n",
    "    \n",
    "    conv_0 = Conv1D(filter_nums, 1, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_1 = Conv1D(filter_nums, 2, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_2 = Conv1D(filter_nums, 3, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_3 = Conv1D(filter_nums, 4, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    \n",
    "    attn_0 = AttentionWeightedAverage()(conv_0)\n",
    "    avg_0 = GlobalAveragePooling1D()(conv_0)\n",
    "    maxpool_0 = GlobalMaxPooling1D()(conv_0)\n",
    "    \n",
    "    attn_1 = AttentionWeightedAverage()(conv_1)\n",
    "    avg_1 = GlobalAveragePooling1D()(conv_1)\n",
    "    maxpool_1 = GlobalMaxPooling1D()(conv_1)\n",
    "    \n",
    "    attn_2 = AttentionWeightedAverage()(conv_2)\n",
    "    avg_2 = GlobalAveragePooling1D()(conv_2)\n",
    "    maxpool_2 = GlobalMaxPooling1D()(conv_2)\n",
    "    \n",
    "    attn_3 = AttentionWeightedAverage()(conv_3)\n",
    "    avg_3 = GlobalAveragePooling1D()(conv_3)\n",
    "    maxpool_3 = GlobalMaxPooling1D()(conv_3)\n",
    "    \n",
    "    merged_maxpool = merge([maxpool_0, maxpool_1, maxpool_2, maxpool_3], mode='concat', concat_axis=1)\n",
    "    merged_attn = merge([attn_0, attn_1, attn_2, attn_3], mode='concat', concat_axis=1)\n",
    "    merged_avg = merge([avg_0, avg_1, avg_2, avg_3], mode='concat', concat_axis=1)\n",
    "    merged_tensor = merge([merged_maxpool, merged_attn, merged_avg], mode='concat', concat_axis=1)\n",
    "    \n",
    "    output = Dropout(0.7)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_av_pos_cnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words, \n",
    "                                embedding_dim, \n",
    "                                weights=[embedding_matrix], \n",
    "                                input_length=max_sequence_length, \n",
    "                                trainable=False)\n",
    "    \n",
    "    pos_embedding_layer = Embedding(50,\n",
    "                                    30,\n",
    "                                    input_length=max_sequence_length,\n",
    "                                    trainable=True)\n",
    "    \n",
    "    filter_nums = 325\n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    pos_input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='POS')\n",
    "    \n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    pos_sequences = pos_embedding_layer(pos_input_layer)\n",
    "    merged_embedding_layer = concatenate([embedding_sequences, pos_sequences])\n",
    "    final_embedding_sequences = SpatialDropout1D(0.25)(embedding_layer)\n",
    "    \n",
    "    conv_0 = Conv1D(filter_nums, 1, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_1 = Conv1D(filter_nums, 2, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_2 = Conv1D(filter_nums, 3, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_3 = Conv1D(filter_nums, 4, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "\n",
    "    attn_0 = AttentionWeightedAverage()(conv_0)\n",
    "    avg_0 = GlobalAveragePooling1D()(conv_0)\n",
    "    maxpool_0 = GlobalMaxPooling1D()(conv_0)\n",
    "    \n",
    "    attn_1 = AttentionWeightedAverage()(conv_1)\n",
    "    avg_1 = GlobalAveragePooling1D()(conv_1)\n",
    "    maxpool_1 = GlobalMaxPooling1D()(conv_1)\n",
    "    \n",
    "    attn_2 = AttentionWeightedAverage()(conv_2)\n",
    "    avg_2 = GlobalAveragePooling1D()(conv_2)\n",
    "    maxpool_2 = GlobalMaxPooling1D()(conv_2)\n",
    "    \n",
    "    attn_3 = AttentionWeightedAverage()(conv_3)\n",
    "    avg_3 = GlobalAveragePooling1D()(conv_3)\n",
    "    maxpool_3 = GlobalMaxPooling1D()(conv_3)\n",
    "    \n",
    "    merged_tensor_maxpool = merge([maxpool_0, maxpool_1, maxpool_2, maxpool_3], mode='concat', concat_axis=1)\n",
    "    merged_tensor_attn = merge([attn_0, attn_1, attn_2, attn_3], mode='concat', concat_axis=1)\n",
    "    merged_tensor_avg = merge([avg_0, avg_1, avg_2, avg_3], mode='concat', concat_axis=1)\n",
    "    merged_tensor = merge([merged_tensor_maxpool, merged_tensor_attn, merged_tensor_avg], mode='concat', concat_axis=1)\n",
    "    \n",
    "    output = Dropout(0.7)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=[input_layer, pos_input_layer], outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n",
    "    model.compile(loss='binary_corssentropy', optimizer=adam_optimizer, matrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmax_text_cnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    filter_nums = 180\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(embedding_sequences)\n",
    "    \n",
    "    conv_0 = Conv1D(filter_nums, 1, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_1 = Conv1D(filter_nums, 2, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_2 = Conv1D(filter_nums, 3, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_3 = Conv1D(filter_nums, 4, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    \n",
    "    maxpool_0 = KMaxPooling(k=3)(conv_0)\n",
    "    maxpool_1 = KMaxPooling(k=3)(conv_1)\n",
    "    maxpool_2 = KMaxPooling(k=3)(conv_2)\n",
    "    maxpool_3 = KMaxPooling(k=3)(conv_3)\n",
    "    \n",
    "    merged_tensor = merge([maxpool_0, maxpool_1, maxpool_2, maxpool_3], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.6)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rcnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    recurrent_units = 64\n",
    "    filter_nums = 128\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.2)(embedding_sequences)\n",
    "    \n",
    "    rnn_layer = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(final_embedding_sequences)\n",
    "    conv_layer = Conv1D(filter_nums, 2, kernel_initializer='normal', padding='valid', activation='relu', strides=1)(rnn_layer)\n",
    "    \n",
    "    maxpool = GlobalMaxPooling1D()(conv_layer)\n",
    "    attn = AttentionWeightedAverage()(conv_layer)\n",
    "    avg = GlobalAveragePooling1D()(conv_layer)\n",
    "    \n",
    "    merged_tensor = merge([maxpool, attn, avg], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.5)(merged_tensor)\n",
    "    output = Dense(units=120, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_av_rnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    recurrent_units = 64\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.25)(embedding_sequences)\n",
    "    \n",
    "    rnn_layer_0 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(final_embedding_sequences)\n",
    "    rnn_layer_1 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(rnn_layer_0)\n",
    "    merged_rnn_layer = merge([rnn_layer_0, rnn_layer_1], mode='concat', concat_axis=2)\n",
    "    \n",
    "    last_layer = Lambda(lambda t: t[:, -1], name='last_layer')(merged_rnn_layer)\n",
    "    maxpool = GlobalMaxPooling1D()(merged_rnn_layer)\n",
    "    attn = AttentionWegithedAverage()(merged_rnn_layer)\n",
    "    avg = GlobalAveragePooling1D()(merged_rnn_layer)\n",
    "    \n",
    "    merged_tensor = merge([last_layer, maxpool, attn, avg], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.5)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=input_layer, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_av_pos_rnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    pos_embedding_layer = Embedding(50,\n",
    "                                    35,\n",
    "                                    input_length=max_sequence_length,\n",
    "                                    trainable=True)\n",
    "    \n",
    "    recurrent_units = 64\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    pos_input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='POS')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    pos_sequences = pos_embedding_layer(pos_input_layer)\n",
    "    merged_embedding_layer = concatenate([embedding_sequences, pos_sequences], axis=2)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.2)(merged_embedding_layer)\n",
    "    \n",
    "    rnn_layer_0 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(final_embedding_sequences)\n",
    "    rnn_layer_0 = SpatialDropout1D(0.3)(rnn_layer_0)\n",
    "    rnn_layer_1 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(rnn_layer_0)\n",
    "    \n",
    "    last_layer = Lambda(lambda t: t[:, -1], name='last_layer')(rnn_layer_1)\n",
    "    maxpool = GlobalMaxPooling1D()(rnn_layer_1)\n",
    "    attn = AttentionWeightedAverage()(rnn_layer_1)\n",
    "    avg = GlobalAveragePooling1D()(rnn_layer_1)\n",
    "    \n",
    "    merged_tensor = merge([last_layer, maxpool, attn, avg], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.5)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=[input_layer, pos_input_layer], outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dropout_bigru(nb_words, embedding_dims, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Emebdding(nb_words,\n",
    "                                embedding_dims,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    recurrent_units = 64\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.2)(embedding_sequences)\n",
    "    \n",
    "    rnn_layer = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(final_embedding_sequences)\n",
    "    rnn_layer = Dropout(0.35)(rnn_layer)\n",
    "    rnn_layer = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(rnn_layer)\n",
    "    \n",
    "    last_layer = Lambda(lambda t: t[:, -1])(rnn_layer)\n",
    "    maxpool = GlobalMaxPooling1D()(rnn_layer)\n",
    "    avg = GlobalAveragePooling1D()(rnn_layer)\n",
    "    \n",
    "    merged_tensor = merge([last_layer, maxpool, avg], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.5)(merged_tensor)\n",
    "    output = Dense(units=72, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(loss='binary_cro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Onehot (InputLayer)             (None, 350)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "POS (InputLayer)                (None, 350)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 350, 300)     30000000    Onehot[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 350, 35)      1750        POS[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 350, 335)     0           embedding_7[0][0]                \n",
      "                                                                 embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_7 (SpatialDro (None, 350, 335)     0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 350, 128)     153984      spatial_dropout1d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_8 (SpatialDro (None, 350, 128)     0           bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 350, 128)     74496       spatial_dropout1d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "last_layer (Lambda)             (None, 128)          0           bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_weighted_average_4 (A (None, 128)          128         bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 128)          0           bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "merge_3 (Merge)                 (None, 512)          0           last_layer[0][0]                 \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 attention_weighted_average_4[0][0\n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 512)          0           merge_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 144)          73872       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 6)            870         dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 30,305,100\n",
      "Trainable params: 305,100\n",
      "Non-trainable params: 30,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:32: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python3.5/dist-packages/keras/legacy/layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "model_name = 'fasttext-avrnn-pos-' + str(nb_words) + 'vocabulary-' + str(MAX_SEQUENCE_LENGTH) + 'length'\n",
    "model = get_av_pos_rnn(nb_words, EMBEDDING_DIM, embedding_matrix, MAX_SEQUENCE_LENGTH, 6)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In fold # 0\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 168s 1ms/step - loss: 0.0716 - acc: 0.9758 - val_loss: 0.0509 - val_acc: 0.9817\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 167s 1ms/step - loss: 0.0524 - acc: 0.9812 - val_loss: 0.0481 - val_acc: 0.9824\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 168s 1ms/step - loss: 0.0495 - acc: 0.9820 - val_loss: 0.0472 - val_acc: 0.9825\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 168s 1ms/step - loss: 0.0479 - acc: 0.9823 - val_loss: 0.0461 - val_acc: 0.9827\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0462 - acc: 0.9829 - val_loss: 0.0458 - val_acc: 0.9826\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0450 - acc: 0.9831 - val_loss: 0.0446 - val_acc: 0.9830\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 167s 1ms/step - loss: 0.0437 - acc: 0.9836 - val_loss: 0.0440 - val_acc: 0.9835\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 167s 1ms/step - loss: 0.0426 - acc: 0.9839 - val_loss: 0.0435 - val_acc: 0.9835\n",
      "Epoch 9/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0415 - acc: 0.9843 - val_loss: 0.0434 - val_acc: 0.9835\n",
      "Epoch 10/50\n",
      "143614/143614 [==============================] - 169s 1ms/step - loss: 0.0402 - acc: 0.9847 - val_loss: 0.0449 - val_acc: 0.9827\n",
      "Epoch 11/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0394 - acc: 0.9848 - val_loss: 0.0435 - val_acc: 0.9830\n",
      "Epoch 12/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0384 - acc: 0.9850 - val_loss: 0.0442 - val_acc: 0.9833\n",
      "Epoch 13/50\n",
      "143614/143614 [==============================] - 168s 1ms/step - loss: 0.0375 - acc: 0.9854 - val_loss: 0.0452 - val_acc: 0.9827\n",
      "Epoch 14/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0365 - acc: 0.9857 - val_loss: 0.0455 - val_acc: 0.9828\n",
      "Epoch 15/50\n",
      "143614/143614 [==============================] - 167s 1ms/step - loss: 0.0359 - acc: 0.9859 - val_loss: 0.0447 - val_acc: 0.9835\n",
      "Epoch 16/50\n",
      "143614/143614 [==============================] - 168s 1ms/step - loss: 0.0350 - acc: 0.9862 - val_loss: 0.0461 - val_acc: 0.9827\n",
      "AUC Score 0.984756056571395\n",
      "In fold # 1\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 169s 1ms/step - loss: 0.0358 - acc: 0.9861 - val_loss: 0.0324 - val_acc: 0.9871\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0346 - acc: 0.9864 - val_loss: 0.0326 - val_acc: 0.9870\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0339 - acc: 0.9866 - val_loss: 0.0330 - val_acc: 0.9869\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0331 - acc: 0.9868 - val_loss: 0.0336 - val_acc: 0.9865\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0323 - acc: 0.9871 - val_loss: 0.0349 - val_acc: 0.9861\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 168s 1ms/step - loss: 0.0317 - acc: 0.9874 - val_loss: 0.0350 - val_acc: 0.9859\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 168s 1ms/step - loss: 0.0312 - acc: 0.9875 - val_loss: 0.0355 - val_acc: 0.9860\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 168s 1ms/step - loss: 0.0305 - acc: 0.9877 - val_loss: 0.0359 - val_acc: 0.9857\n",
      "AUC Score 0.9928424228111293\n",
      "In fold # 2\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0313 - acc: 0.9875 - val_loss: 0.0239 - val_acc: 0.9905\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0305 - acc: 0.9877 - val_loss: 0.0257 - val_acc: 0.9897\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0297 - acc: 0.9880 - val_loss: 0.0258 - val_acc: 0.9897\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0290 - acc: 0.9882 - val_loss: 0.0268 - val_acc: 0.9893\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0286 - acc: 0.9884 - val_loss: 0.0270 - val_acc: 0.9892\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 168s 1ms/step - loss: 0.0283 - acc: 0.9885 - val_loss: 0.0279 - val_acc: 0.9887\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0278 - acc: 0.9887 - val_loss: 0.0297 - val_acc: 0.9882\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0294 - acc: 0.9881 - val_loss: 0.0310 - val_acc: 0.9878\n",
      "AUC Score 0.99541853509794\n",
      "In fold # 3\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 168s 1ms/step - loss: 0.0286 - acc: 0.9884 - val_loss: 0.0223 - val_acc: 0.9908\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0278 - acc: 0.9888 - val_loss: 0.0222 - val_acc: 0.9911\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0272 - acc: 0.9890 - val_loss: 0.0230 - val_acc: 0.9903\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0268 - acc: 0.9891 - val_loss: 0.0237 - val_acc: 0.9899\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0263 - acc: 0.9892 - val_loss: 0.0245 - val_acc: 0.9898\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0261 - acc: 0.9894 - val_loss: 0.0248 - val_acc: 0.9897\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0257 - acc: 0.9895 - val_loss: 0.0249 - val_acc: 0.9896\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0254 - acc: 0.9896 - val_loss: 0.0260 - val_acc: 0.9890\n",
      "Epoch 9/50\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0252 - acc: 0.9896 - val_loss: 0.0263 - val_acc: 0.9889\n",
      "AUC Score 0.9960258036056061\n",
      "In fold # 4\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0260 - acc: 0.9894 - val_loss: 0.0174 - val_acc: 0.9933\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0255 - acc: 0.9897 - val_loss: 0.0186 - val_acc: 0.9927\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0253 - acc: 0.9896 - val_loss: 0.0228 - val_acc: 0.9909\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0259 - acc: 0.9893 - val_loss: 0.0212 - val_acc: 0.9913\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0247 - acc: 0.9899 - val_loss: 0.0209 - val_acc: 0.9915\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0242 - acc: 0.9900 - val_loss: 0.0204 - val_acc: 0.9916\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0242 - acc: 0.9900 - val_loss: 0.0211 - val_acc: 0.9914\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0237 - acc: 0.9903 - val_loss: 0.0211 - val_acc: 0.9913\n",
      "AUC Score 0.997161755055358\n",
      "In fold # 5\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0242 - acc: 0.9902 - val_loss: 0.0159 - val_acc: 0.9937\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0239 - acc: 0.9902 - val_loss: 0.0177 - val_acc: 0.9931\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0236 - acc: 0.9904 - val_loss: 0.0186 - val_acc: 0.9926\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0235 - acc: 0.9903 - val_loss: 0.0178 - val_acc: 0.9928\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0233 - acc: 0.9905 - val_loss: 0.0182 - val_acc: 0.9925\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0233 - acc: 0.9906 - val_loss: 0.0191 - val_acc: 0.9919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0230 - acc: 0.9906 - val_loss: 0.0197 - val_acc: 0.9919\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0229 - acc: 0.9906 - val_loss: 0.0209 - val_acc: 0.9913\n",
      "AUC Score 0.9975237139442849\n",
      "In fold # 6\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0233 - acc: 0.9905 - val_loss: 0.0155 - val_acc: 0.9939\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0230 - acc: 0.9905 - val_loss: 0.0157 - val_acc: 0.9938\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0228 - acc: 0.9906 - val_loss: 0.0161 - val_acc: 0.9934\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0227 - acc: 0.9907 - val_loss: 0.0165 - val_acc: 0.9933\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0223 - acc: 0.9909 - val_loss: 0.0171 - val_acc: 0.9929\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0222 - acc: 0.9910 - val_loss: 0.0186 - val_acc: 0.9923\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0218 - acc: 0.9911 - val_loss: 0.0185 - val_acc: 0.9925\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 169s 1ms/step - loss: 0.0218 - acc: 0.9911 - val_loss: 0.0183 - val_acc: 0.9925\n",
      "AUC Score 0.9978112128712229\n",
      "In fold # 7\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 167s 1ms/step - loss: 0.0222 - acc: 0.9909 - val_loss: 0.0160 - val_acc: 0.9943\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 167s 1ms/step - loss: 0.0222 - acc: 0.9909 - val_loss: 0.0161 - val_acc: 0.9937\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0218 - acc: 0.9911 - val_loss: 0.0168 - val_acc: 0.9933\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0214 - acc: 0.9913 - val_loss: 0.0163 - val_acc: 0.9937\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0213 - acc: 0.9912 - val_loss: 0.0168 - val_acc: 0.9932\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0215 - acc: 0.9913 - val_loss: 0.0176 - val_acc: 0.9926\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0213 - acc: 0.9913 - val_loss: 0.0175 - val_acc: 0.9929\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 169s 1ms/step - loss: 0.0208 - acc: 0.9915 - val_loss: 0.0196 - val_acc: 0.9920\n",
      "AUC Score 0.998177711570556\n",
      "In fold # 8\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 167s 1ms/step - loss: 0.0252 - acc: 0.9899 - val_loss: 0.0130 - val_acc: 0.9951\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0218 - acc: 0.9911 - val_loss: 0.0133 - val_acc: 0.9948\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0214 - acc: 0.9914 - val_loss: 0.0130 - val_acc: 0.9952\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 168s 1ms/step - loss: 0.0211 - acc: 0.9914 - val_loss: 0.0128 - val_acc: 0.9952\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0210 - acc: 0.9915 - val_loss: 0.0134 - val_acc: 0.9948\n",
      "Epoch 6/50\n",
      " 36800/143614 [======>.......................] - ETA: 1:58 - loss: 0.0199 - acc: 0.9920"
     ]
    }
   ],
   "source": [
    "models, val_loss, total_auc, fold_predictions = train_folds(train_data, pos_train_data, train_labels, FOLD_COUNT, BATCH_SIZE, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Overall val-loss: {}, AUC {}'.format(val_loss, total_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_path_prefix = 'results/RNN_Based/' + model_name\n",
    "\n",
    "print('Predicting testing results...')\n",
    "test_predicts_list = []\n",
    "for fold_id, model in enumerate(models):\n",
    "    test_data_dict = {'Onehot': test_data, 'POS': pos_test_data}\n",
    "    test_predict = model.predict(test_data_dict, batch_size=BATCH_SIZE, verbose=1)\n",
    "    test_predicts_list.append(test_predict)\n",
    "    np.save('predicts_pool/AVRNN/', test_predict)\n",
    "    \n",
    "test_predicts = np.zeros(test_predicts_list.shape[1])\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts += fold_predict\n",
    "test_predicts /= len(test_predicts_list)\n",
    "\n",
    "test_ids = test_df['id'].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "\n",
    "test_predicts = pd.DataFrame(data=test_predicts, columns=list_classes)\n",
    "test_predicts['id'] = test_ids\n",
    "test_predicts = test_predicts[['id'] + list_classes]\n",
    "submit_path = submit_path_prefix + '-L{:4f}-A{:4f}.csv'.format(val_loss, total_auc)\n",
    "test_predicts.to_csv(submit_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold_predictions = np.concatenate((fold_predictions, axis=0))\n",
    "train_auc = roc_auc_score(train_labels[:-1], train_fold_predictions)\n",
    "print('Training AUC', train_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicting training results...')\n",
    "train_ids = train_df['id'].values\n",
    "train_ids = train_ids.reshape((len(train_ids), 1))\n",
    "\n",
    "train_predicts = pd.DataFrame(data=train_fold_predictions, columns=list_classes)\n",
    "train_predicts['id'] = train_ids\n",
    "train_predicts = train_predicts[['id'] + list_classes]\n",
    "submit_path = submit_path_prefix + '-(Train)-L{:4f}-A{:4f}.csv'.format(val_loss, train_auc)\n",
    "train_predicts.to_csv(submit_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Ensemble (For Test Format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging(arrs):\n",
    "    print(\"Doing ensemble on\")\n",
    "    subs = []\n",
    "    for arr in arrs:\n",
    "        print(arr)\n",
    "        subs.append(pd.read_csv(arr))\n",
    "    \n",
    "    classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    for sub in subs[1:]:\n",
    "        for c in classes:\n",
    "            subs[0][c] += sub[c]\n",
    "    \n",
    "    for c in classes:\n",
    "        subs[0][c] /= len(subs)\n",
    "        \n",
    "    subs[0].to_csv('Bagging.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_corr(arr1, arr2):\n",
    "    res = 0\n",
    "    for col in arr1.columns.values[1:]:\n",
    "        cur = arr1[col].corr(arr2[col])\n",
    "        corr = (arr1[col].rank() / len(arr1)).corr(arr2[col].rank() / len(arr2))\n",
    "        print(col, corr)\n",
    "        res += corr\n",
    "    print(\"Avg Rank: \", res / len(arr1.columns.values[1:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
