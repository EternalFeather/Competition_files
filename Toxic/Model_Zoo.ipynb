{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re, csv, codecs, operator, sys\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from textblob import TextBlob    # For pos-tagging\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D, Reshape, \\\n",
    "GlobalAveragePooling1D, merge, Flatten, Bidirectional, CuDNNGRU, add, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'datasets/'\n",
    "FAST_TEXT_EMBEDDING = 'pretrain_embedding/crawl-300d-2M.vec'\n",
    "CLEAN_WORD_PATH = None\n",
    "TRAIN_DATA_FILE = 'train.csv'\n",
    "TEST_DATA_FILE = 'test.csv'\n",
    "MAX_SEQUENCE_LENGTH = 350\n",
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrain Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrain_embedding(file):\n",
    "    print('Indexing word vectors')\n",
    "    embeddings_index = {}\n",
    "    f = open(file, 'r', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        try:\n",
    "            word = values[0]\n",
    "            coefs = np.array(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        except:\n",
    "            print(\"Error on: \", values[:3])\n",
    "    f.close()\n",
    "    print(\"Total %s word vectors\" % len(embeddings_index))\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Total 2000000 word vectors\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = load_pretrain_embedding(FAST_TEXT_EMBEDDING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('datasets/train.csv')\n",
    "test_df = pd.read_csv('datasets/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      "id               159571 non-null object\n",
      "comment_text     159571 non-null object\n",
      "toxic            159571 non-null int64\n",
      "severe_toxic     159571 non-null int64\n",
      "obscene          159571 non-null int64\n",
      "threat           159571 non-null int64\n",
      "insult           159571 non-null int64\n",
      "identity_hate    159571 non-null int64\n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 9.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805  \n",
       "std         0.216627       0.093420  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.308619</td>\n",
       "      <td>0.676515</td>\n",
       "      <td>0.157058</td>\n",
       "      <td>0.647518</td>\n",
       "      <td>0.266009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>severe_toxic</th>\n",
       "      <td>0.308619</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.403014</td>\n",
       "      <td>0.123601</td>\n",
       "      <td>0.375807</td>\n",
       "      <td>0.201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obscene</th>\n",
       "      <td>0.676515</td>\n",
       "      <td>0.403014</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.141179</td>\n",
       "      <td>0.741272</td>\n",
       "      <td>0.286867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threat</th>\n",
       "      <td>0.157058</td>\n",
       "      <td>0.123601</td>\n",
       "      <td>0.141179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.150022</td>\n",
       "      <td>0.115128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insult</th>\n",
       "      <td>0.647518</td>\n",
       "      <td>0.375807</td>\n",
       "      <td>0.741272</td>\n",
       "      <td>0.150022</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.337736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>identity_hate</th>\n",
       "      <td>0.266009</td>\n",
       "      <td>0.201600</td>\n",
       "      <td>0.286867</td>\n",
       "      <td>0.115128</td>\n",
       "      <td>0.337736</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  toxic  severe_toxic   obscene    threat    insult  \\\n",
       "toxic          1.000000      0.308619  0.676515  0.157058  0.647518   \n",
       "severe_toxic   0.308619      1.000000  0.403014  0.123601  0.375807   \n",
       "obscene        0.676515      0.403014  1.000000  0.141179  0.741272   \n",
       "threat         0.157058      0.123601  0.141179  1.000000  0.150022   \n",
       "insult         0.647518      0.375807  0.741272  0.150022  1.000000   \n",
       "identity_hate  0.266009      0.201600  0.286867  0.115128  0.337736   \n",
       "\n",
       "               identity_hate  \n",
       "toxic               0.266009  \n",
       "severe_toxic        0.201600  \n",
       "obscene             0.286867  \n",
       "threat              0.115128  \n",
       "insult              0.337736  \n",
       "identity_hate       1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comments = train_df['comment_text'].values\n",
    "test_comments = test_df['comment_text'].values\n",
    "train_comments_lengths = [len(s) for s in train_comments]\n",
    "test_comments_lengths = [len(s) for s in test_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Train------\n",
      "MAX LENGTH:\t\t 5000\n",
      "AVG LENGTH:\t\t 394.0732213246768\n",
      "MIN LENGTH:\t\t 6\n",
      "STANDARD DIVISION:\t 590.7184309382144\n",
      "RANGE:\t\t\t 6  to  1575.5100832011055\n",
      "------Test------\n",
      "MAX LENGTH:\t\t 5000\n",
      "AVG LENGTH:\t\t 364.8751207855632\n",
      "MIN LENGTH:\t\t 1\n",
      "STANDARD DIVISION:\t 592.4901645516661\n",
      "RANGE:\t\t\t 1  to  1549.8554498888955\n"
     ]
    }
   ],
   "source": [
    "def explore_comments(arr):\n",
    "    print(\"MAX LENGTH:\\t\\t\", np.max(arr))\n",
    "    print(\"AVG LENGTH:\\t\\t\", np.average(arr))\n",
    "    print(\"MIN LENGTH:\\t\\t\", np.min(arr))\n",
    "    print(\"STANDARD DIVISION:\\t\", np.std(arr))\n",
    "    print(\"RANGE:\\t\\t\\t\", np.min(arr), \" to \", np.average(arr) + 2 * np.std(arr))\n",
    "    \n",
    "print(\"------Train------\")\n",
    "explore_comments(train_comments_lengths)\n",
    "\n",
    "print(\"------Test------\")\n",
    "explore_comments(test_comments_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Cleaned Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEAN_WORD_PATH == None:\n",
    "    ignored_words = set(stopwords.words('english'))\n",
    "else:\n",
    "    ignored_words = {}\n",
    "    with open(CLEAN_WORD_PATH, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip('\\n')\n",
    "            typo, correct = line.split(',')\n",
    "            ignored_words[typo] = correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal = re.compile(r'[^?!.,:a-z\\d ]', re.IGNORECASE)\n",
    "# Regex to remove all numerics\n",
    "replace_numbers = re.compile(r'd+', re.IGNORECASE)\n",
    "\n",
    "word_count_dict = defaultdict(int)\n",
    "\n",
    "def clean_datasets(text, remove_stopwords=False, stem_words=False, count_null_words=True, clean_wiki_tokens=True):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"”\", \"\\\"\", text)\n",
    "    text = re.sub(r\"“\", \"\\\"\", text)\n",
    "    text = replace_numbers.sub('', text)\n",
    "    \n",
    "    if count_null_words:\n",
    "        text = text.split()\n",
    "        for t in text:\n",
    "            word_count_dict[t] += 1\n",
    "        text = \" \".join(text)\n",
    "        \n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_train = train_df['comment_text'].fillna('no comment').values\n",
    "list_sentences_test = test_df['comment_text'].fillna('no comment').values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_labels = train_df[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data cleaning...\n"
     ]
    }
   ],
   "source": [
    "cleaned_train_comments, cleaned_test_comments = [], []\n",
    "print('Processing data cleaning...')\n",
    "\n",
    "for text in list_sentences_train:\n",
    "    cleaned_train_comments.append(clean_datasets(text))\n",
    "for text in list_sentences_test:\n",
    "    cleaned_test_comments.append(clean_datasets(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['comment_text_cleaned'] = cleaned_train_comments\n",
    "test_df['comment_text_cleaned'] = cleaned_test_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312735"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_comment_text = pd.concat([train_df['comment_text_cleaned'], test_df['comment_text_cleaned']], axis=0).fillna(\"unknown\")\n",
    "nrow_train = train_df.shape[0]\n",
    "all_comment_text.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312735, 50000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=50000)\n",
    "data = vectorizer.fit_transform(all_comment_text)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312735, 50000)\n"
     ]
    }
   ],
   "source": [
    "norm_data = MaxAbsScaler().fit_transform(data)\n",
    "print(norm_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='\"#$%&()+,-./:;<=>@[\\\\]^_`{|}~\\t\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically train vocab & tokenizer...\n",
      "Found 420473 unique tokens\n",
      "Shape of train_data tensor:  (159571, 350)\n",
      "Shape of train_label tensor:  (159571, 6)\n",
      "Shape of test_data tensor:  (153164, 350)\n"
     ]
    }
   ],
   "source": [
    "print('Automatically train vocab & tokenizer...')\n",
    "tokenizer.fit_on_texts(cleaned_train_comments + cleaned_test_comments)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(cleaned_train_comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(cleaned_test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of train_data tensor: ', train_data.shape)\n",
    "print('Shape of train_label tensor: ', train_labels.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor: ', test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pos Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2pos(sentence):\n",
    "    try:\n",
    "        tag = TextBlob(sentence).tags\n",
    "    except:\n",
    "        print(sentence)\n",
    "        \n",
    "    updated_sentence = ' '.join([i[0] for i in tag])\n",
    "    tagged = ' '.join([i[1] for i in tag])\n",
    "    return updated_sentence, tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_word_index = {v: k for k, v in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pos_comments = []\n",
    "Pos_updated_sentence = []\n",
    "for text in train_sequences:\n",
    "    text_ = ' '.join([inverse_word_index[word] for word in text])    # convert to word format\n",
    "    if not isinstance(text_, str):\n",
    "        print(text, '\\n', text_)\n",
    "    updated_sentence, tags = sent2pos(text_)\n",
    "    Pos_updated_sentence.append(updated_sentence)\n",
    "    Pos_comments.append(tags)\n",
    "    assert len(updated_sentence.split(' ')) == len(tags.split(' ')), \"T1 {} T2 {}\".format(len(text), len(tags.split()))\n",
    "    \n",
    "Pos_test_comments = []\n",
    "Pos_test_updated_sentence = []\n",
    "for text in test_sequences:\n",
    "    text_ = ' '.join([inverse_word_index[word] for word in text])\n",
    "    updated_test_sentence, test_tags = sent2pos(text_)\n",
    "    Pos_test_updated_sentence.append(updated_test_sentence)\n",
    "    Pos_test_comments.append(test_tags)\n",
    "    assert len(updated_test_sentence.split(' ')) == len(test_tags.split(' ')), \"T1 {} T2 {}\".format(len(text), len(test_tags.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tokenizer = Tokenizer(num_words=50, filters='\"#$%&()+,-./:;<=>@[\\\\]^_`{|}~\\t\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically train pos tokenizer...\n",
      "Found 34 unique tokens\n",
      "Shape of data tensor: (159571, 350)\n",
      "Shape of test_data tensor: (153164, 350)\n"
     ]
    }
   ],
   "source": [
    "print('Automatically train pos tokenizer...')\n",
    "pos_tokenizer.fit_on_texts(Pos_comments + Pos_test_comments)\n",
    "\n",
    "train_pos_sequences = pos_tokenizer.texts_to_sequences(Pos_comments)\n",
    "test_pos_sequences = pos_tokenizer.texts_to_sequences(Pos_test_comments)\n",
    "\n",
    "pos_word_index = pos_tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(pos_word_index))\n",
    "\n",
    "pos_train_data = pad_sequences(train_pos_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', pos_train_data.shape)\n",
    "\n",
    "pos_test_data = pad_sequences(test_pos_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', pos_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second time valid for tokenzier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically train pos tokenizer secondly...\n",
      "Found 420473 unique tokens\n",
      "Shape of train_data tensor: (159571, 350)\n",
      "Shape of train_label tensor: (159571, 6)\n",
      "Shape of test_data tensor: (153164, 350)\n"
     ]
    }
   ],
   "source": [
    "print('Automatically train pos tokenizer secondly...')\n",
    "cleaned_train_comments, cleaned_test_comments = [], []\n",
    "for text in Pos_updated_sentence:\n",
    "    cleaned_train_comments.append(clean_datasets(text))\n",
    "for text in Pos_test_updated_sentence:\n",
    "    cleaned_test_comments.append(clean_datasets(text))\n",
    "    \n",
    "train_sequences = tokenizer.texts_to_sequences(cleaned_train_comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(cleaned_test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of train_data tensor:', train_data.shape)\n",
    "print('Shape of train_label tensor:', train_labels.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix...\n",
      "Null_word embeddings: 41446\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix...')\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "null_words = open('null_words.txt', 'w', encoding='utf-8')\n",
    "\n",
    "for word, idx in word_index.items():\n",
    "    if idx >= MAX_NB_WORDS:\n",
    "        null_words.write(word + ', ' + str(word_count_dict[word]) + '\\n')\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "    else:\n",
    "        null_words.write(word + ', ' + str(word_count_dict[word]) + '\\n')\n",
    "print('Null_word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Ensemble (For Test Format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging(arrs):\n",
    "    print(\"Doing ensemble on\")\n",
    "    subs = []\n",
    "    for arr in arrs:\n",
    "        print(arr)\n",
    "        subs.append(pd.read_csv(arr))\n",
    "    \n",
    "    classes = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    for sub in subs[1:]:\n",
    "        for c in classes:\n",
    "            subs[0][c] += sub[c]\n",
    "    \n",
    "    for c in classes:\n",
    "        subs[0][c] /= len(subs)\n",
    "        \n",
    "    subs[0].to_csv('Bagging.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_corr(arr1, arr2):\n",
    "    res = 0\n",
    "    for col in arr1.columns.values[1:]:\n",
    "        cur = arr1[col].corr(arr2[col])\n",
    "        corr = (arr1[col].rank() / len(arr1)).corr(arr2[col].rank() / len(arr2))\n",
    "        print(col, corr)\n",
    "        res += corr\n",
    "    print(\"Avg Rank: \", res / len(arr1.columns.values[1:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
