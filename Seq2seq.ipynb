{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.ops import tensor_array_ops, control_flow_ops\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 64\n",
    "vocab_size = 5000\n",
    "embed_dims = 32\n",
    "enc_hidden_size = 32\n",
    "dec_hidden_size = 64\n",
    "seq_length = 8\n",
    "learning_rate = 0.01\n",
    "pointer = 0\n",
    "start_word = 2\n",
    "training_epoches = 100\n",
    "data_path = \"Chinese_quatrains_7.txt\"\n",
    "vocab_path = \"Chinese_quatrains_7.vocab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_helper\n",
    "def build_vocab(vocab_path, data_path):\n",
    "    files = open(data_path, 'r', encoding='utf-8').read()\n",
    "    words = files.split()\n",
    "    wordcount = collections.Counter(words)\n",
    "    with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"{}\\t1000000000\\n{}\\t1000000000\\n{}\\t1000000000\\n{}\\t1000000000\\n\".format(\"<UNK>\", \"<PAD>\", \"<SOS>\", \"<EOS>\"))\n",
    "        for word, count in wordcount.most_common(vocab_size-4):\n",
    "            f.write(\"{}\\t{}\\n\".format(word, count))\n",
    "        \n",
    "def mini_batch(vocab_path, data_path):\n",
    "    token_seqs, sentences = load_datasets(vocab_path, data_path)\n",
    "    num_batch = int(len(sentences) / batch_size)\n",
    "    sentences = sentences[:num_batch * batch_size]\n",
    "    token_seqs = token_seqs[:num_batch * batch_size]\n",
    "    sentences, tokens = np.array(sentences), np.array(token_seqs)\n",
    "    sentence_batch = np.split(sentences, num_batch, 0)\n",
    "    token_batch = np.split(tokens, num_batch, 0)\n",
    "    return token_batch, sentence_batch, num_batch\n",
    "    \n",
    "def load_datasets(vocab_path, data_path):\n",
    "    sentences = [line for line in open(data_path, 'r', encoding='utf-8').read().split(\"\\n\") if line]\n",
    "    word2idx, idx2word = load_vocab(vocab_path)\n",
    "    \n",
    "    token_list, sources = [], []\n",
    "    for source in sentences:\n",
    "        x = [word2idx.get(word, 1) for word in (source + \" <EOS>\").split()]\n",
    "        token_list.append(x)\n",
    "        sources.append(source)\n",
    "    return token_list, sources\n",
    "    \n",
    "def load_vocab(vocab_path):\n",
    "    vocab = [line.split()[0] for line in open(vocab_path, 'r', encoding='utf-8').read().splitlines()]\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {word2idx[word]: word for word in word2idx}\n",
    "    return word2idx, idx2word\n",
    "\n",
    "def next_batch(token_batches, pointer, num_batch):\n",
    "    result = token_batches[pointer]\n",
    "    pointer = (pointer + 1) % num_batch\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Chinese_quatrains_7.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-85835503b1fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtoken_sequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;31m# word2idx, idx2word = load_vocab(vocab_path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;31m# result = next_batch(token_sequences, pointer, num_batch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-694054ffe68d>\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(vocab_path, data_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# data_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwordcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Chinese_quatrains_7.txt'"
     ]
    }
   ],
   "source": [
    "build_vocab(vocab_path, data_path)\n",
    "token_sequences, sentences, num_batch = mini_batch(vocab_path, data_path)\n",
    "\n",
    "# word2idx, idx2word = load_vocab(vocab_path)\n",
    "# result = next_batch(token_sequences, pointer, num_batch)\n",
    "# trans_sentences = []\n",
    "# for sentence in sentences[0]:\n",
    "#     transform = [word2idx.get(word, 1) for word in (sentence + \" <EOS>\").split()]\n",
    "#     trans_sentences.append(transform)\n",
    "# print(result[:3], \"\\n\\n\", np.array(trans_sentences)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(object):\n",
    "    def __init__(self, vocab_size, batch_size, embed_size, hidden_size, seq_length, start_word):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        self.start_token = tf.constant([start_word] * self.batch_size, dtype=tf.int32)\n",
    "        self.enc_params = []\n",
    "        \n",
    "        with tf.variable_scope(\"encoder\"):\n",
    "            self.enc_embeddings = tf.Variable(self.init_matrix([self.vocab_size, self.embed_size]))\n",
    "            self.enc_params.append(self.enc_embeddings)\n",
    "            self.forward_layer = self.recurrent_lstm_forward(self.enc_params)\n",
    "            \n",
    "        # placeholder\n",
    "        self.x = tf.placeholder(tf.int32, shape=[self.batch_size, self.seq_length])\n",
    "        \n",
    "        # initialize\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.processed_x = tf.transpose(tf.nn.embedding_lookup(self.enc_embeddings, self.x), perm=[1, 0, 2])\n",
    "            \n",
    "        h0 = tf.zeros([self.batch_size, self.hidden_size])\n",
    "        self.hidden_memory_0 = tf.stack([h0, h0])\n",
    "        \n",
    "        # training_step\n",
    "        ta_embed_x_forward = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.seq_length, dynamic_size=False, infer_shape=True)\n",
    "        ta_embed_x_forward = ta_embed_x_forward.unstack(self.processed_x)\n",
    "        ta_embed_x_backward = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.seq_length, dynamic_size=False, infer_shape=True)\n",
    "        ta_embed_x_backward = ta_embed_x_backward.unstack(self.processed_x)\n",
    "\n",
    "        def _recurrence_lstm_forward(i, x_t, h_tm):\n",
    "            h_t = self.forward_layer(x_t, h_tm)\n",
    "            x_ = ta_embed_x_forward.read(i)\n",
    "            return i + 1, x_, h_t\n",
    "        \n",
    "        def _recurrence_lstm_backward(i, x_t, h_tm):\n",
    "            h_t = self.forward_layer(x_t, h_tm)\n",
    "            x_ = ta_embed_x_backward.read(i)\n",
    "            return i - 1, x_, h_t\n",
    "\n",
    "        _, _, hidden_memory_forward = control_flow_ops.while_loop(\n",
    "            cond=lambda i, _1, _2: i < seq_length,\n",
    "            body=_recurrence_lstm_forward,\n",
    "            loop_vars=(tf.constant(0, dtype=tf.int32), tf.nn.embedding_lookup(self.enc_embeddings, self.start_token), self.hidden_memory_0)\n",
    "        )\n",
    "        \n",
    "        _, _, hidden_memory_backward = control_flow_ops.while_loop(\n",
    "            cond=lambda i, _1, _2: i >= 0,\n",
    "            body=_recurrence_lstm_backward,\n",
    "            loop_vars=(tf.constant(self.seq_length - 1, dtype=tf.int32), tf.nn.embedding_lookup(self.enc_embeddings, self.start_token), self.hidden_memory_0)\n",
    "        )\n",
    "\n",
    "        hidden_forward, _ = tf.unstack(hidden_memory_forward)\n",
    "        hidden_backward, _ = tf.unstack(hidden_memory_backward)\n",
    "        self.hidden_state = tf.concat((hidden_forward, hidden_backward), 1)\n",
    "        \n",
    "    def init_matrix(self, shape):\n",
    "        return tf.random_normal(shape, stddev=0.1)\n",
    "    \n",
    "    def recurrent_lstm_forward(self, params):\n",
    "        self.Wi = tf.Variable(self.init_matrix([self.embed_size, self.hidden_size]))\n",
    "        self.Ui = tf.Variable(self.init_matrix([self.hidden_size, self.hidden_size]))\n",
    "        self.bi = tf.Variable(self.init_matrix([self.hidden_size]))\n",
    "        \n",
    "        self.Wf = tf.Variable(self.init_matrix([self.embed_size, self.hidden_size]))\n",
    "        self.Uf = tf.Variable(self.init_matrix([self.hidden_size, self.hidden_size]))\n",
    "        self.bf = tf.Variable(self.init_matrix([self.hidden_size]))\n",
    "        \n",
    "        self.Wo = tf.Variable(self.init_matrix([self.embed_size, self.hidden_size]))\n",
    "        self.Uo = tf.Variable(self.init_matrix([self.hidden_size, self.hidden_size]))\n",
    "        self.bo = tf.Variable(self.init_matrix([self.hidden_size]))\n",
    "        \n",
    "        self.Wc = tf.Variable(self.init_matrix([self.embed_size, self.hidden_size]))\n",
    "        self.Uc = tf.Variable(self.init_matrix([self.hidden_size, self.hidden_size]))\n",
    "        self.bc = tf.Variable(self.init_matrix([self.hidden_size]))\n",
    "        \n",
    "        params.extend([\n",
    "            self.Wi, self.Ui, self.bi,\n",
    "            self.Wf, self.Uf, self.bf,\n",
    "            self.Wo, self.Uo, self.bo,\n",
    "            self.Wc, self.Uc, self.bc\n",
    "        ])\n",
    "        \n",
    "        def forward(x, hidden_memory):\n",
    "            hidden_state, cell_state = tf.unstack(hidden_memory)\n",
    "            \n",
    "            i = tf.sigmoid(\n",
    "                tf.matmul(x, self.Wi) + tf.matmul(hidden_state, self.Ui) + self.bi\n",
    "            )\n",
    "            \n",
    "            f = tf.sigmoid(\n",
    "                tf.matmul(x, self.Wf) + tf.matmul(hidden_state, self.Uf) + self.bf\n",
    "            )\n",
    "            \n",
    "            o = tf.sigmoid(\n",
    "                tf.matmul(x, self.Wo) + tf.matmul(hidden_state, self.Uo) + self.bo\n",
    "            )\n",
    "            \n",
    "            c_ = tf.nn.tanh(\n",
    "                tf.matmul(x, self.Wc) + tf.matmul(hidden_state, self.Uc) + self.bc\n",
    "            )\n",
    "            \n",
    "            c = f * cell_state + i * c_\n",
    "            current_hidden_state = tf.nn.tanh(c)\n",
    "            \n",
    "            return tf.stack([current_hidden_state, c])\n",
    "            \n",
    "        return forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Decoder\n",
    "class Decoder(object):\n",
    "    def __init__(self, vocab_size, batch_size, embed_size, hidden_size, seq_length, start_word, learning_rate, dec_params):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        self.start_token = tf.constant([start_word] * self.batch_size, dtype=tf.int32)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dec_params = dec_params\n",
    "        self.grad_clip = 5.0\n",
    "        \n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            self.dec_embeddings = tf.Variable(self.init_matrix([self.vocab_size, self.embed_size]))\n",
    "            self.dec_params.append(self.dec_embeddings)\n",
    "            self.forward_layer = self.recurrent_lstm_forward(self.dec_params)\n",
    "            self.linear_layer = self.recurrent_linear_forward(self.dec_params)\n",
    "            \n",
    "        # placeholder\n",
    "        self.x = tf.placeholder(tf.int32, shape=[self.batch_size, self.seq_length])\n",
    "        self.hidden_state = tf.placeholder(tf.float32, shape=[self.batch_size, self.hidden_size])\n",
    "        \n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.processed_x = tf.transpose(tf.nn.embedding_lookup(self.dec_embeddings, self.x), perm=[1, 0, 2])\n",
    "            \n",
    "        cell_state = tf.zeros([self.batch_size, self.hidden_size])\n",
    "        self.hidden_memory = tf.stack([self.hidden_state, cell_state])\n",
    "            \n",
    "        # training_step\n",
    "        predictions = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.seq_length, dynamic_size=False, infer_shape=True)\n",
    "        ta_embed_x = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.seq_length, dynamic_size=False, infer_shape=True)\n",
    "        ta_embed_x = ta_embed_x.unstack(self.processed_x)\n",
    "        \n",
    "        def _training_recurrence(i, x_t, h_tm, predictions):\n",
    "            h_t = self.forward_layer(x_t, h_tm)\n",
    "            o_t = self.linear_layer(h_t)\n",
    "            predictions = predictions.write(i, o_t)\n",
    "            x_ = ta_embed_x.read(i)\n",
    "            return i + 1, x_, h_t, predictions\n",
    "        \n",
    "        _, _, _, predictions = control_flow_ops.while_loop(\n",
    "            cond=lambda i, _1, _2, _3: i < self.seq_length,\n",
    "            body=_training_recurrence,\n",
    "            loop_vars=(tf.constant(0, dtype=tf.int32), tf.nn.embedding_lookup(self.dec_embeddings, self.start_token), self.hidden_memory, predictions)\n",
    "        )\n",
    "        \n",
    "        self.predictions = predictions.stack()\n",
    "        self.predictions = tf.transpose(self.predictions, perm=[1, 0, 2])\n",
    "                \n",
    "        self.loss = -tf.reduce_sum(\n",
    "            tf.one_hot(tf.cast(tf.reshape(self.x, [-1]), tf.int32), self.vocab_size, 1.0, 0.0) * tf.log(\n",
    "                tf.clip_by_value(tf.reshape(self.predictions, [-1, vocab_size]), 1e-20, 1.0)\n",
    "            )\n",
    "        ) / (self.seq_length * self.batch_size)\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.gradients, _ = tf.clip_by_global_norm(tf.gradients(self.loss, self.dec_params), self.grad_clip)\n",
    "        self.update = self.optimizer.apply_gradients(zip(self.gradients, self.dec_params))\n",
    "        \n",
    "        # testing_step\n",
    "        output_prob_sequences = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.seq_length, dynamic_size=False, infer_shape=True)\n",
    "        token_sequences = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.seq_length, dynamic_size=False, infer_shape=True)\n",
    "        \n",
    "        def _pred_recurrence(i, x_t, h_tm, gen_o, gen_x):\n",
    "            h_t = self.forward_layer(x_t, h_tm)\n",
    "            o_t = self.linear_layer(h_t)\n",
    "            log_prob = tf.log(o_t)\n",
    "            next_token = tf.cast(tf.reshape(tf.multinomial(log_prob, 1), [self.batch_size]), tf.int32)\n",
    "            x_ = tf.nn.embedding_lookup(self.dec_embeddings, next_token)\n",
    "            gen_o = gen_o.write(i, tf.reduce_sum(tf.multiply(tf.one_hot(next_token, self.vocab_size, 1.0, 0.0), o_t), 1))\n",
    "            gen_x = gen_x.write(i, next_token)\n",
    "            return i + 1, x_, h_t, gen_o, gen_x\n",
    "        \n",
    "        _, _, _, self.output_prob_sequences, self.token_sequences = control_flow_ops.while_loop(\n",
    "            cond=lambda i, _1, _2, _3, _4: i < self.seq_length,\n",
    "            body=_pred_recurrence,\n",
    "            loop_vars=(tf.constant(0, dtype=tf.int32), tf.nn.embedding_lookup(self.dec_embeddings, self.start_token), self.hidden_memory, output_prob_sequences, token_sequences)\n",
    "        )\n",
    "        \n",
    "        self.token_sequences = self.token_sequences.stack()\n",
    "        self.token_sequences = tf.transpose(self.token_sequences, perm=[1, 0])\n",
    "            \n",
    "    def init_matrix(self, shape):\n",
    "        return tf.random_normal(shape, stddev=0.1)\n",
    "    \n",
    "    def recurrent_lstm_forward(self, params):\n",
    "        self.Wi = tf.Variable(self.init_matrix([self.embed_size, self.hidden_size]))\n",
    "        self.Ui = tf.Variable(self.init_matrix([self.hidden_size, self.hidden_size]))\n",
    "        self.bi = tf.Variable(self.init_matrix([self.hidden_size]))\n",
    "        \n",
    "        self.Wf = tf.Variable(self.init_matrix([self.embed_size, self.hidden_size]))\n",
    "        self.Uf = tf.Variable(self.init_matrix([self.hidden_size, self.hidden_size]))\n",
    "        self.bf = tf.Variable(self.init_matrix([self.hidden_size]))\n",
    "        \n",
    "        self.Wo = tf.Variable(self.init_matrix([self.embed_size, self.hidden_size]))\n",
    "        self.Uo = tf.Variable(self.init_matrix([self.hidden_size, self.hidden_size]))\n",
    "        self.bo = tf.Variable(self.init_matrix([self.hidden_size]))\n",
    "        \n",
    "        self.Wc = tf.Variable(self.init_matrix([self.embed_size, self.hidden_size]))\n",
    "        self.Uc = tf.Variable(self.init_matrix([self.hidden_size, self.hidden_size]))\n",
    "        self.bc = tf.Variable(self.init_matrix([self.hidden_size]))\n",
    "        \n",
    "        params.extend([\n",
    "            self.Wi, self.Ui, self.bi,\n",
    "            self.Wf, self.Uf, self.bf,\n",
    "            self.Wo, self.Uo, self.bo,\n",
    "            self.Wc, self.Uc, self.bc\n",
    "        ])\n",
    "        \n",
    "        def forward(x, hidden_memory):\n",
    "            hidden_state, cell_state = tf.unstack(hidden_memory)\n",
    "            \n",
    "            i = tf.sigmoid(\n",
    "                tf.matmul(x, self.Wi) + tf.matmul(hidden_state, self.Ui) + self.bi\n",
    "            )\n",
    "            \n",
    "            f = tf.sigmoid(\n",
    "                tf.matmul(x, self.Wf) + tf.matmul(hidden_state, self.Uf) + self.bf\n",
    "            )\n",
    "            \n",
    "            o = tf.sigmoid(\n",
    "                tf.matmul(x, self.Wo) + tf.matmul(hidden_state, self.Uo) + self.bo\n",
    "            )\n",
    "\n",
    "            c_ = tf.nn.tanh(\n",
    "                tf.matmul(x, self.Wc) + tf.matmul(hidden_state, self.Uc) + self.bc\n",
    "            )\n",
    "\n",
    "            c = f * cell_state + i * c_\n",
    "            current_hidden_state = tf.nn.tanh(c)\n",
    "            \n",
    "            return tf.stack([current_hidden_state, c])\n",
    "        \n",
    "        return forward\n",
    "    \n",
    "    def recurrent_linear_forward(self, params):\n",
    "        self.V = tf.Variable(self.init_matrix([self.hidden_size, self.vocab_size]))\n",
    "        self.c = tf.Variable(self.init_matrix([self.vocab_size]))\n",
    "        \n",
    "        params.extend([\n",
    "            self.V, self.c\n",
    "        ])\n",
    "        \n",
    "        def forward(hidden_memory):\n",
    "            hidden_state, cell_state = tf.unstack(hidden_memory)\n",
    "            logits = tf.matmul(hidden_state, self.V) + self.c\n",
    "            output = tf.nn.softmax(logits)\n",
    "            return output\n",
    "        \n",
    "        return forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2seq(object):\n",
    "    def __init__(self, vocab_size, batch_size, embed_dims, enc_hidden_size, dec_hidden_size, seq_length, start_word, learning_rate):\n",
    "        self.encoder = Encoder(vocab_size, batch_size, embed_dims, enc_hidden_size, seq_length, start_word)\n",
    "        self.decoder = Decoder(vocab_size, batch_size, embed_dims, dec_hidden_size, seq_length, start_word, learning_rate, self.encoder.enc_params)\n",
    "        \n",
    "    def forward(self, sess, batch):\n",
    "        hidden = sess.run(self.encoder.hidden_state, feed_dict={self.encoder.x: batch})\n",
    "        loss, _ = sess.run([self.decoder.loss, self.decoder.update], feed_dict={self.decoder.x: batch, self.decoder.hidden_state: hidden})\n",
    "        return loss\n",
    "    \n",
    "    def pred(self, sess, batch):\n",
    "        hidden = sess.run(self.encoder.hidden_state, feed_dict={self.encoder.x: batch})\n",
    "        pred = sess.run(self.decoder.token_sequences, feed_dict={self.decoder.x: batch, self.decoder.hidden_state: hidden})\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq2seq_model = Seq2seq(vocab_size, batch_size, embed_dims, enc_hidden_size, dec_hidden_size, seq_length, start_word, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSG : Epoch 0/100, loss = 8.512969970703125\n",
      "MSG : Epoch 0/100, loss = 0.43367817997932434\n",
      "MSG : Epoch 0/100, loss = 0.023673906922340393\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b6d1c780a52f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_sequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseq2seq_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_batch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MSG : Epoch {}/{}, loss = {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_epoches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-9220e5cdc89d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sess, batch)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\CZJ\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\CZJ\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\CZJ\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\CZJ\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\CZJ\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epoches):\n",
    "    pointer = 0\n",
    "    for i in range(num_batch):\n",
    "        batch = next_batch(token_sequences, pointer, num_batch)\n",
    "        loss = seq2seq_model.forward(sess, batch)\n",
    "        if (epoch * num_batch + i) % 200 == 0:\n",
    "            print(\"MSG : Epoch {}/{}, loss = {}\".format(epoch * num_batch + i, training_epoches * num_batch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
