{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re, csv, codecs, operator, sys, gc\n",
    "from collections import defaultdict, OrderedDict\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import lightgbm as lgb\n",
    "from itertools import repeat\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from textblob import TextBlob    # For pos-tagging\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from scipy.special import logit, expit\n",
    "\n",
    "from keras import optimizers, initializers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Lambda, Embedding, Dropout, Activation, SpatialDropout1D, Reshape, \\\n",
    "GlobalAveragePooling1D, merge, Flatten, Bidirectional, CuDNNGRU, add, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras import backend as K\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'datasets/'\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "FAST_TEXT_EMBEDDING = 'pretrain_embedding/crawl-300d-2M.vec'\n",
    "GLOVE_EMBEDDING = 'pretrain_embedding/glove.840B.300d.txt'\n",
    "CLEAN_WORD_PATH = None\n",
    "TRAIN_DATA_FILE = 'train.csv'\n",
    "TEST_DATA_FILE = 'test.csv'\n",
    "MAX_SEQUENCE_LENGTH = 350\n",
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300\n",
    "FOLD_COUNT = 10\n",
    "BATCH_SIZE = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrain Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrain_embedding(file):\n",
    "    \"\"\"\n",
    "    Return a dictionary whose key is word, value is pretrained word embedding.\n",
    "    \"\"\"\n",
    "    print('Indexing word vectors')\n",
    "    embeddings_index = {}\n",
    "    f = open(file, 'r', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        try:\n",
    "            word = values[0]\n",
    "            coefs = np.array(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        except:\n",
    "            print(\"Error on: \", values[:3])\n",
    "    f.close()\n",
    "    print(\"Total %s word vectors\" % len(embeddings_index))\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_words(file):\n",
    "    \"\"\"\n",
    "    Return a dictionary whose key is typo, value is correct word.\n",
    "    \"\"\"\n",
    "    clean_word_dict = {}\n",
    "    with open(file, 'r', encoding='utf-8'):\n",
    "        for line in f:\n",
    "            line = line.strip('\\n')\n",
    "            typo, correct = line.split(',')\n",
    "            clean_word_dict[typo] = correct\n",
    "    return clean_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Total 2000000 word vectors\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = load_pretrain_embedding(FAST_TEXT_EMBEDDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Error on:  ['.', '.', '.']\n",
      "Error on:  ['at', 'name@domain.com', '0.0061218']\n",
      "Error on:  ['.', '.', '.']\n",
      "Error on:  ['to', 'name@domain.com', '0.33865']\n",
      "Error on:  ['.', '.', '0.035974']\n",
      "Error on:  ['.', '.', '.']\n",
      "Error on:  ['email', 'name@domain.com', '0.33529']\n",
      "Error on:  ['or', 'name@domain.com', '0.48374']\n",
      "Error on:  ['contact', 'name@domain.com', '0.016426']\n",
      "Error on:  ['Email', 'name@domain.com', '0.37344']\n",
      "Error on:  ['on', 'name@domain.com', '0.037295']\n",
      "Error on:  ['At', 'Killerseats.com', '-0.13854']\n",
      "Error on:  ['by', 'name@domain.com', '0.6882']\n",
      "Error on:  ['in', 'mylot.com', '-0.18148']\n",
      "Error on:  ['emailing', 'name@domain.com', '0.39173']\n",
      "Error on:  ['Contact', 'name@domain.com', '0.14933']\n",
      "Error on:  ['at', 'name@domain.com', '0.44321']\n",
      "Error on:  ['•', 'name@domain.com', '-0.13288']\n",
      "Error on:  ['at', 'Amazon.com', '-0.5275']\n",
      "Error on:  ['is', 'name@domain.com', '-0.1197']\n",
      "Total 2195885 word vectors\n"
     ]
    }
   ],
   "source": [
    "glove_embeddings_index = load_pretrain_embedding(GLOVE_EMBEDDING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('datasets/train.csv')\n",
    "test_df = pd.read_csv('datasets/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comments = train_df['comment_text'].values\n",
    "test_comments = test_df['comment_text'].values\n",
    "train_comments_lengths = [len(s) for s in train_comments]\n",
    "test_comments_lengths = [len(s) for s in test_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_comments(arr):\n",
    "    print(\"MAX LENGTH:\\t\\t\", np.max(arr))\n",
    "    print(\"AVG LENGTH:\\t\\t\", np.average(arr))\n",
    "    print(\"MIN LENGTH:\\t\\t\", np.min(arr))\n",
    "    print(\"STANDARD DIVISION:\\t\", np.std(arr))\n",
    "    print(\"RANGE:\\t\\t\\t\", np.min(arr), \" to \", np.average(arr) + 2 * np.std(arr))\n",
    "    \n",
    "print(\"------Train------\")\n",
    "explore_comments(train_comments_lengths)\n",
    "\n",
    "print(\"------Test------\")\n",
    "explore_comments(test_comments_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name in list_classes:\n",
    "    print('{}\\n{}\\n'.format(class_name, train_df[class_name].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_regexp_occ(regexp='', text=None):\n",
    "    \"\"\"\n",
    "    Simple way to calculate the number of occurence of a regex\n",
    "    \"\"\"\n",
    "    if len(text) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(re.findall(regexp, text)) / len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Cleaned Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEAN_WORD_PATH == None:\n",
    "    ignored_words = set(stopwords.words('english'))\n",
    "else:\n",
    "    ignored_words = load_clean_words(CLEAN_WORD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_files = sorted([f for f in listdir('datasets/feature_files/') if isfile(join('datasets/feature_files/', f))])\n",
    "for file in feature_files:\n",
    "    fixed_df = pd.read_csv('datasets/feature_files/' + file)\n",
    "    train_df = train_df.merge(fixed_df, on='id', how='left')\n",
    "    test_df = test_df.merge(fixed_df, on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_data(path):\n",
    "    files = sorted([f for f in listdir(path) if isfile(join(path, f))])\n",
    "    for i, file in enumerate(files):\n",
    "        temp_df = pd.read_csv(path + file)\n",
    "        print('Datasets before ensemble null number:', temp_df.isnull().sum().sum())\n",
    "        if i == 0:\n",
    "            ensembled = temp_df\n",
    "        else:\n",
    "            ensembled = ensembled.merge(temp_df, on='id', how='left')\n",
    "            print('Datasets after ensemble null number:', ensembled.isnull().sum().sum())\n",
    "    \n",
    "    column_numbers = ensembled.shape[1]\n",
    "    toxic = ensembled.iloc[:, [i for i in range(2, column_numbers, len(list_classes))]]\n",
    "    severe_toxic = ensembled.iloc[:, [i for i in range(3, column_numbers, len(list_classes))]]\n",
    "    obscene = ensembled.iloc[:, [i for i in range(4, column_numbers, len(list_classes))]]\n",
    "    threat = ensembled.iloc[:, [i for i in range(5, column_numbers, len(list_classes))]]\n",
    "    insult = ensembled.iloc[:, [i for i in range(6, column_numbers, len(list_classes))]]\n",
    "    identity_hate = ensembled.iloc[:, [i for i in range(7, column_numbers, len(list_classes))]]\n",
    "    \n",
    "    return toxic, severe_toxic, obscene, threat, insult, identity_hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets before ensemble null number: 0\n",
      "Datasets before ensemble null number: 0\n",
      "Datasets after ensemble null number: 0\n",
      "Datasets before ensemble null number: 0\n",
      "Datasets after ensemble null number: 0\n",
      "Datasets before ensemble null number: 0\n",
      "Datasets after ensemble null number: 0\n",
      "Datasets before ensemble null number: 0\n",
      "Datasets before ensemble null number: 0\n",
      "Datasets after ensemble null number: 0\n",
      "Datasets before ensemble null number: 0\n",
      "Datasets after ensemble null number: 0\n",
      "Datasets before ensemble null number: 0\n",
      "Datasets after ensemble null number: 0\n"
     ]
    }
   ],
   "source": [
    "train_toxic, train_severe_toxic, train_obscene, train_threat, train_insult, train_identity_hate = ensemble_data('datasets/ensemble_files/train/')\n",
    "test_toxic, test_severe_toxic, test_obscene, test_threat, test_insult, test_identity_hate = ensemble_data('datasets/ensemble_files/test/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal = re.compile(r'[^?!.,:a-z\\d ]', re.IGNORECASE)\n",
    "# Regex to remove all numerics\n",
    "replace_numbers = re.compile(r'\\d+', re.IGNORECASE)\n",
    "\n",
    "word_count_dict = defaultdict(int)\n",
    "\n",
    "def clean_datasets(text, remove_stopwords=False, stem_words=False, count_null_words=True, clean_wiki_tokens=True):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \"\", text)\n",
    "    text = re.sub(r\"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}\", \"\", text)\n",
    "    \n",
    "    if clean_wiki_tokens:\n",
    "        text = re.sub(r\"\\.jpg\", \" \", text)\n",
    "        text = re.sub(r\"\\.png\", \" \", text)\n",
    "        text = re.sub(r\"\\.gif\", \" \", text)\n",
    "        text = re.sub(r\"\\.bmp\", \" \", text)\n",
    "        text = re.sub(r\"\\.pdf\", \" \", text)\n",
    "        text = re.sub(r\"image:\", \" \", text)\n",
    "        text = re.sub(r\"#([A-Fa-f0-9]{6}|[A-Fa-f0-9]{3})\", \" \",text)\n",
    "        \n",
    "        text = re.sub(r\"\\{\\|[^\\}]*\\|\\}\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[user:.*\\]\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[user:.*\\|\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[wikipedia:.*\\]\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[wikipedia:.*\\|\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[special:.*\\]\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[special:.*\\|\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[category:.*\\]\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[category:.*\\|\", \" \", text)\n",
    "        \n",
    "        text = re.sub(r\"wp:\", \" \", text)\n",
    "        text = re.sub(r\"file:\", \" \", text)\n",
    "    \n",
    "    # For Punctuation\n",
    "    text = re.sub(r\"”\", \"\\\"\", text)\n",
    "    text = re.sub(r\"“\", \"\\\"\", text)\n",
    "    text = re.sub(r\"´\", \"'\", text)\n",
    "    text = re.sub(r\"—\", \" \", text)\n",
    "    text = re.sub(r\"’\", \"'\", text)\n",
    "    text = re.sub(r\"‘\", \"'\", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\"−\", \" \", text)\n",
    "    text = re.sub(r\"\\?\", \" ? \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"#\", \" # \", text)\n",
    "    \n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"i’m\", \"i am\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"<3\", \" love \", text)\n",
    "        \n",
    "    text = replace_numbers.sub('', text)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        for typo, correct in ignored_words.items():\n",
    "            text = re.sub(typo, correct, text)\n",
    "    \n",
    "    if count_null_words:\n",
    "        text = text.split()\n",
    "        for t in text:\n",
    "            word_count_dict[t] += 1\n",
    "        text = \" \".join(text)\n",
    "        \n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_train = train_df['comment_text'].fillna('no comment').values\n",
    "list_sentences_test = test_df['comment_text'].fillna('no comment').values\n",
    "train_labels = train_df[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data cleaning...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "cleaned_train_comments, cleaned_test_comments = [], []\n",
    "print('Processing data cleaning...')\n",
    "\n",
    "for text in list_sentences_train:\n",
    "    cleaned_train_comments.append(clean_datasets(text))\n",
    "for text in list_sentences_test:\n",
    "    cleaned_test_comments.append(clean_datasets(text))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['comment_text_cleaned'] = cleaned_train_comments\n",
    "test_df['comment_text_cleaned'] = cleaned_test_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Meta (Extend) Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unknown_embedding(t, idx):\n",
    "    t = t.split()\n",
    "    res = 0\n",
    "    for word in t:\n",
    "        if word not in idx.keys():\n",
    "            res += 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_meta_feature(df):\n",
    "    df['total_length'] = df['comment_text'].apply(len)\n",
    "    df['capital'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.apply(lambda row: float(row['capital']) / float(row['total_length']), axis=1)\n",
    "    df[\"unknown_glove\"] = df['comment_text_cleaned'].apply(lambda x: count_unknown_embedding(x, glove_embeddings_index))\n",
    "    df[\"unknown_fasttext\"] = df['comment_text_cleaned'].apply(lambda x: count_unknown_embedding(x, embeddings_index))\n",
    "    df[\"unknown_glove_fasttext\"] = df[\"unknown_glove\"] + df[\"unknown_fasttext\"]\n",
    "    # Special Expressions Collection\n",
    "    df['num_exclamation_marks'] = df['comment_text'].apply(lambda comment: comment.count('!'))\n",
    "    df['num_question_marks'] = df['comment_text'].apply(lambda comment: comment.count('?'))\n",
    "    df['num_punctuation'] = df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '.,;:'))\n",
    "    df['num_symbols'] = df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '*&$%'))\n",
    "    df['num_words'] = df['comment_text'].apply(lambda comment: len(comment.split()))\n",
    "    df['num_unique_words'] = df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']\n",
    "    df['num_smilies'] = df['comment_text'].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))\n",
    "    df[\"ant_slash_n\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\n\", x))\n",
    "    df[\"start_with_columns\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"^\\:+\", x))\n",
    "    df[\"has_emphasize_equal\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\={2}.+\\={2}\", x))\n",
    "    df[\"has_emphasize_quotes\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\\"{4}\\S+\\\"{4}\", x))\n",
    "    df[\"has_star\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\*\", x))\n",
    "    ## Check for dates\n",
    "    df[\"has_timestamp\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\d{2}|:\\d{2}\", x))\n",
    "    df[\"has_date_long\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{2}:\\d{2}, \\d{1,2} \\w+ \\d{4}\", x))    # example: 18:44, 8 December 2010\n",
    "    df[\"has_date_short\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{1,2} \\w+ \\d{4}\", x))\n",
    "    ## Check for http links\n",
    "    df[\"has_http\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"http[s]{0,1}://\\S+\", x))\n",
    "    df[\"has_ip\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(\"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}\", x))\n",
    "    ## Check for mail\n",
    "    df[\"has_mail\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', x))\n",
    "    ## Check for image\n",
    "    df[\"has_image\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r'image\\:', x))\n",
    "    # Dirty Languages Collection\n",
    "    df[\"nb_fk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ff]\\S{2}[Kk]\", x))\n",
    "    df[\"nb_sk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ss]\\S{2}[Kk]\", x))\n",
    "    df[\"nb_dk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[dD]ick\", x))\n",
    "    df[\"nb_you\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\W[Yy]ou\\W\", x))\n",
    "    df[\"nb_mother\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wmother\\W\", x))\n",
    "    df[\"nb_ng\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wnigger\\W\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating meta features...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating meta features...\")\n",
    "create_meta_feature(train_df)\n",
    "create_meta_feature(test_df)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['comment_text_cleaned', 'total_length', 'capital', 'caps_vs_length',\n",
      "       'unknown_glove', 'unknown_fasttext', 'unknown_glove_fasttext',\n",
      "       'num_exclamation_marks', 'num_question_marks', 'num_punctuation',\n",
      "       'num_symbols', 'num_words', 'num_unique_words', 'words_vs_unique',\n",
      "       'num_smilies', 'ant_slash_n', 'start_with_columns',\n",
      "       'has_emphasize_equal', 'has_emphasize_quotes', 'has_star',\n",
      "       'has_timestamp', 'has_date_long', 'has_date_short', 'has_http',\n",
      "       'has_ip', 'has_mail', 'has_image', 'nb_fk', 'nb_sk', 'nb_dk', 'nb_you',\n",
      "       'nb_mother', 'nb_ng'],\n",
      "      dtype='object')\n",
      "41 33\n"
     ]
    }
   ],
   "source": [
    "train_cols = train_df.columns[8:]\n",
    "print(train_cols)\n",
    "column_numbers = len(train_df.columns)\n",
    "print(column_numbers, len(train_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_cols:\n",
    "    train_toxic[col] = train_df[col]\n",
    "    train_severe_toxic[col] = train_df[col]\n",
    "    train_obscene[col] = train_df[col]\n",
    "    train_threat[col] = train_df[col]\n",
    "    train_insult[col] = train_df[col]\n",
    "    train_identity_hate[col] = train_df[col]\n",
    "for col in train_cols:\n",
    "    test_toxic[col] = test_df[col]\n",
    "    test_severe_toxic[col] = test_df[col]\n",
    "    test_obscene[col] = test_df[col]\n",
    "    test_threat[col] = test_df[col]\n",
    "    test_insult[col] = test_df[col]\n",
    "    test_identity_hate[col] = test_df[col]\n",
    "train_ensembled_data = [train_toxic, train_severe_toxic, train_obscene, train_threat, train_insult, train_identity_hate]\n",
    "test_ensembled_data = [test_toxic, test_severe_toxic, test_obscene, test_threat, test_insult, test_identity_hate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del list_sentences_train, list_sentences_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312735"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_comment_text = pd.concat([train_df['comment_text_cleaned'], test_df['comment_text_cleaned']], axis=0).fillna(\"unknown\")\n",
    "nrow_train = train_df.shape[0]\n",
    "all_comment_text.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312735, 50000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=50000)\n",
    "data = vectorizer.fit_transform(all_comment_text)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312735, 50000)\n"
     ]
    }
   ],
   "source": [
    "norm_data = MaxAbsScaler().fit_transform(data)\n",
    "print(norm_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vectorization process Done!\n"
     ]
    }
   ],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=20000\n",
    ")\n",
    "word_vectorizer.fit(all_comment_text)\n",
    "train_word_features = word_vectorizer.transform(cleaned_train_comments)\n",
    "test_word_features = word_vectorizer.transform(cleaned_test_comments)\n",
    "print('Word vectorization process Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char vectorization process Done!\n"
     ]
    }
   ],
   "source": [
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 6),\n",
    "    max_features=30000\n",
    ")\n",
    "char_vectorizer.fit(all_comment_text)\n",
    "train_char_features = char_vectorizer.transform(cleaned_train_comments)\n",
    "test_char_features = char_vectorizer.transform(cleaned_test_comments)\n",
    "print('Char vectorization process Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del all_comment_text\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf_features = hstack([train_char_features, train_word_features]).tocsr()\n",
    "test_tfidf_features = hstack([test_char_features, test_word_features]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_char_features, train_word_features, test_char_features, test_word_features\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='\"#$%&()+,-./:;<=>@[\\\\]^_`{|}~\\t\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically train vocab & tokenizer...\n",
      "Found 330549 unique tokens\n",
      "Shape of train_data tensor:  (159571, 350)\n",
      "Shape of train_label tensor:  (159571, 6)\n",
      "Shape of test_data tensor:  (153164, 350)\n"
     ]
    }
   ],
   "source": [
    "print('Automatically train vocab & tokenizer...')\n",
    "tokenizer.fit_on_texts(cleaned_train_comments + cleaned_test_comments)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(cleaned_train_comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(cleaned_test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of train_data tensor: ', train_data.shape)\n",
    "print('Shape of train_label tensor: ', train_labels.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor: ', test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pos Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2pos(sentence):\n",
    "    try:\n",
    "        tag = TextBlob(sentence).tags\n",
    "    except:\n",
    "        print(sentence)\n",
    "        \n",
    "    updated_sentence = ' '.join([i[0] for i in tag])\n",
    "    tagged = ' '.join([i[1] for i in tag])\n",
    "    return updated_sentence, tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_word_index = {v: k for k, v in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159571/159571 [09:33<00:00, 278.24it/s]\n",
      "100%|██████████| 153164/153164 [08:40<00:00, 294.23it/s]\n"
     ]
    }
   ],
   "source": [
    "Pos_comments = []\n",
    "Pos_updated_sentence = []\n",
    "for text in tqdm(train_sequences):\n",
    "    text_ = ' '.join([inverse_word_index[word] for word in text])    # convert to word format\n",
    "    if not isinstance(text_, str):\n",
    "        print(text, '\\n', text_)\n",
    "    updated_sentence, tags = sent2pos(text_)\n",
    "    Pos_updated_sentence.append(updated_sentence)\n",
    "    Pos_comments.append(tags)\n",
    "    assert len(updated_sentence.split(' ')) == len(tags.split(' ')), \"T1 {} T2 {}\".format(len(text), len(tags.split()))\n",
    "    \n",
    "Pos_test_comments = []\n",
    "Pos_test_updated_sentence = []\n",
    "for text in tqdm(test_sequences):\n",
    "    text_ = ' '.join([inverse_word_index[word] for word in text])\n",
    "    updated_test_sentence, test_tags = sent2pos(text_)\n",
    "    Pos_test_updated_sentence.append(updated_test_sentence)\n",
    "    Pos_test_comments.append(test_tags)\n",
    "    assert len(updated_test_sentence.split(' ')) == len(test_tags.split(' ')), \"T1 {} T2 {}\".format(len(text), len(test_tags.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tokenizer = Tokenizer(num_words=50, filters='\"#$%&()+,-./:;<=>@[\\\\]^_`{|}~\\t\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically train pos tokenizer...\n",
      "Found 34 unique tokens\n",
      "Shape of data tensor: (159571, 350)\n",
      "Shape of test_data tensor: (153164, 350)\n"
     ]
    }
   ],
   "source": [
    "print('Automatically train pos tokenizer...')\n",
    "pos_tokenizer.fit_on_texts(Pos_comments + Pos_test_comments)\n",
    "\n",
    "train_pos_sequences = pos_tokenizer.texts_to_sequences(Pos_comments)\n",
    "test_pos_sequences = pos_tokenizer.texts_to_sequences(Pos_test_comments)\n",
    "\n",
    "pos_word_index = pos_tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(pos_word_index))\n",
    "\n",
    "pos_train_data = pad_sequences(train_pos_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', pos_train_data.shape)\n",
    "\n",
    "pos_test_data = pad_sequences(test_pos_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', pos_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second time valid for tokenzier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically train pos tokenizer secondly...\n",
      "Found 330549 unique tokens\n",
      "Shape of train_data tensor: (159571, 350)\n",
      "Shape of train_label tensor: (159571, 6)\n",
      "Shape of test_data tensor: (153164, 350)\n"
     ]
    }
   ],
   "source": [
    "print('Automatically train pos tokenizer secondly...')\n",
    "cleaned_train_comments, cleaned_test_comments = [], []\n",
    "for text in Pos_updated_sentence:\n",
    "    cleaned_train_comments.append(clean_datasets(text))\n",
    "for text in Pos_test_updated_sentence:\n",
    "    cleaned_test_comments.append(clean_datasets(text))\n",
    "    \n",
    "train_sequences = tokenizer.texts_to_sequences(cleaned_train_comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(cleaned_test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of train_data tensor:', train_data.shape)\n",
    "print('Shape of train_label tensor:', train_labels.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['cleaned_comment_text'] = cleaned_train_comments\n",
    "test_df['cleaned_comment_text'] = cleaned_test_comments\n",
    "train_df.to_csv(PATH + 'cleaned_train.csv', index=False)\n",
    "test_df.to_csv(PATH + 'cleaned_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del cleaned_train_comments, cleaned_test_comments\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embedding (Build a matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix...\n",
      "Null_word embeddings: 24244\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix...')\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "null_words = open(PATH + 'null_words.txt', 'w', encoding='utf-8')\n",
    "\n",
    "for word, idx in word_index.items():\n",
    "    if idx >= MAX_NB_WORDS:\n",
    "        null_words.write(word + ', ' + str(word_count_dict[word]) + '\\n')\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "    else:\n",
    "        null_words.write(word + ', ' + str(word_count_dict[word]) + '\\n')\n",
    "print('Null_word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting null_words...\n",
      "Sorting operation Done!\n"
     ]
    }
   ],
   "source": [
    "print('Sorting null_words...')\n",
    "null_dict = {}\n",
    "with open(PATH + 'null_words.txt', 'r', encoding='utf-8') as nullword:\n",
    "    for line in nullword:\n",
    "        word, count = line.strip('\\n').split(', ')\n",
    "        null_dict[word] = int(count)\n",
    "\n",
    "null_dict = sorted(null_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "with open(PATH + 'null_words.txt', 'w', encoding='utf-8') as output:\n",
    "    for word, count in null_dict:\n",
    "        output.write(word + ', ' + str(count) + '\\n')\n",
    "print('Sorting operation Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model with Using Tfidf Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=FOLD_COUNT, shuffle=False)\n",
    "tfidf_models = []\n",
    "for i, (train_idx, test_idx) in enumerate(kfold.split(train_tfidf_features)):\n",
    "    print('## In fold {} ##'.format(i + 1))\n",
    "    classifier = LogisticRegression(solver='sag', C=12.0)\n",
    "    \n",
    "    for class_name in list_classes:\n",
    "        print('Processing {} ...'.format(class_name))\n",
    "        train_target = train_df[class_name][train_idx]\n",
    "        \n",
    "        classifier.fit(train_tfidf_features[train_idx], train_target)\n",
    "        y_pred = classifier.predict(train_tfidf_features[train_idx])\n",
    "        \n",
    "        print('Training accuracy is {}'.format(accuracy_score(y_pred, train_target)))\n",
    "        \n",
    "        val_target = train_df[class_name][test_idx]\n",
    "        val_pred = classifier.predict(train_tfidf_features[test_idx])\n",
    "        \n",
    "        print('Validation accuracy is {}'.format(accuracy_score(val_pred, val_target)))\n",
    "        \n",
    "    tfidf_models.append(classifier)\n",
    "print('K-fold cross validation Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(tfidf_models):\n",
    "    print('## In Model {} ##'.format(i + 1))\n",
    "    # predictions = {'id': test_df['id']}\n",
    "    predictions = OrderedDict()\n",
    "    predictions['id'] = test_df['id']\n",
    "    \n",
    "    for class_name in list_classes:\n",
    "        predictions[class_name] = model.predict_proba(test_tfidf_features)[:, 1]\n",
    "        print('Predict the proba for {} Done!'.format(class_name))\n",
    "        print(predictions.keys())\n",
    "    \n",
    "    print(predictions.keys())\n",
    "    submission = pd.DataFrame.from_dict(predictions)\n",
    "    submission.to_csv('results/TFIDF_Based/TFIDF_Logistic_Regression_Submission_{}.csv'.format(i), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = ['results/TFIDF_Based/TFIDF_Logistic_Regression_Submission_{}.csv'.format(i) for i in range(0, FOLD_COUNT)]\n",
    "bagging(result_list, 'results/TFIDF_Based/TFIDF_bagging.csv')\n",
    "print('Bagging operation Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExtraTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "# et_predictions = {'id': test_df['id']}\n",
    "et_predictions = OrderedDict()\n",
    "et_predictions['id'] = test_df['id']\n",
    "\n",
    "for class_name in list_classes:\n",
    "    train_target = train_df[class_name]\n",
    "    classifier = ExtraTreesClassifier(n_estimators=30)\n",
    "    \n",
    "    cv_score = np.mean(cross_val_score(classifier, train_tfidf_features, train_target, cv=10, scoring='roc_auc'))\n",
    "    accs.append(cv_score)\n",
    "    print('CV Score for class {} is {}'.format(class_name, cv_score))\n",
    "    \n",
    "    classifier.fit(train_tfidf_features, train_target)\n",
    "    et_predictions[class_name] = classifier.predict_proba(test_tfidf_features)[:, 1]\n",
    "    \n",
    "submission = pd.DataFrame.from_dict(et_predictions)\n",
    "submission.to_csv('result/TFIDF_Based/TFIDF_ExtraTreesClassifier_Submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light-GBM For Ensembled_Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_every_feature_model(feature_data, label, feature_name, feature_test_data, fold_count, predict=False):\n",
    "    predictions = np.zeros(shape=(len(feature_test_data)))\n",
    "    fold_size = len(feature_data) // fold_count\n",
    "    \n",
    "    print('Feature name is {}'.format(feature_name))\n",
    "    auc_score = 0\n",
    "    for fold_id in range(0, fold_count):\n",
    "        print('## Fold In {} ##'.format(fold_id + 1))\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "        if fold_id == fold_size - 1:\n",
    "            fold_end = len(feature_data)\n",
    "            \n",
    "        train_x = np.concatenate((feature_data[:fold_start], feature_data[fold_end:]))\n",
    "        train_y = np.concatenate((label[:fold_start], label[fold_end:]))\n",
    "        \n",
    "        val_x = feature_data[fold_start:fold_end]\n",
    "        val_y = label[fold_start:fold_end]\n",
    "        \n",
    "        lgb_train = lgb.Dataset(train_x, train_y)\n",
    "        lgb_val = lgb.Dataset(val_x, val_y)\n",
    "        \n",
    "        lgbm_model = lgb.LGBMClassifier(max_depth=5, metric='auc', n_estimators=10000, num_leaves=32, boosting_type='gbdt', \\\n",
    "                                       learning_rate=0.01, feature_fraction=0.3, bagging_fraction=0.8, bagging_freq=5, reg_lambda=0)\n",
    "        lgbm_model.fit(X=train_x, y=train_y, eval_metric=['auc', 'binary_logloss'], eval_set=(val_x, val_y), early_stopping_rounds=1000, verbose=500)\n",
    "        auc_score += lgbm_model.best_score_['valid_0']['auc']\n",
    "        lgb.plot_importance(lgbm_model, max_num_features=30)\n",
    "        plt.show()\n",
    "        if predict:\n",
    "            prediction = lgbm_model.predict_proba(feature_test_data)[:, 1]\n",
    "            predictions += prediction\n",
    "            del lgbm_model\n",
    "    predictions /= fold_count\n",
    "    print('Training LightGBM Done!')\n",
    "    return predictions, auc_score / fold_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, auc_scores = [], []\n",
    "for i, feature_name in enumerate(list_classes):\n",
    "    prediction, auc = fit_every_feature_model(train_ensembled_data[i], train_df[feature_name].values, feature_name, test_ensembled_data[i], 10, predict=True)\n",
    "    auc_scores.append(auc)\n",
    "    predictions.append(prediction)\n",
    "print('Overall AUC Score is {}'.format(sum(auc_scores) / 6))\n",
    "print('For each:'.format(auc_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('datasets/sample_submission.csv')\n",
    "for i, feature_name in enumerate(list_classes):\n",
    "    submission[feature_name] = predictions[i]\n",
    "submission.to_csv('results/LightGBM/LightGBM_with_Meta_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAMP = 'model_pool/av_pos_cnn/pavel_cnn_%.2f_%.2f'%(0.5, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_model_by_auc(model, batch_size, train_x, train_y, val_x, val_y):\n",
    "    best_auc = -1\n",
    "    best_weight = None\n",
    "    best_epoch = 0\n",
    "    current_epoch = 1\n",
    "    \n",
    "    while True:\n",
    "        model.fit(train_x, train_y, batch_size=batch_size, epoch=1, validation_data=[val_x, val_y])\n",
    "        y_pred = model.predict(val_x, batch_size=batch_size)\n",
    "        current_auc = roc_auc_score(val_y, y_pred)\n",
    "        print('Epoch {} auc {:.6f} best_auc {:.6f}'.format(current_epoch, current_auc, best_auc))\n",
    "        current_epoch += 1\n",
    "        if best_auc < current_auc or best_auc == -1:\n",
    "            best_auc = current_auc\n",
    "            best_weight = model.get_weights()\n",
    "            best_epoch = current_epoch\n",
    "        else:\n",
    "            # early stop\n",
    "            if current_epoch - best_epoch == 5:\n",
    "                break\n",
    "                \n",
    "    model.set_weights(best_weights)\n",
    "    return model, best_auc\n",
    "\n",
    "def _train_model_by_logloss(model, batch_size, train_x, pos_train_x, train_y, val_x, pos_val_x, val_y, fold_id):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=7)\n",
    "    best_model_path = STAMP + str(fold_id) + '.h5'\n",
    "    model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n",
    "    train_data = {'Onehot': train_x, 'POS': pos_train_x}\n",
    "    val_data = {'Onehot': val_x, 'POS': pos_val_x}\n",
    "    \n",
    "    hist = model.fit(train_data, train_y, validation_data=(val_data, val_y), epochs=50, batch_size=batch_size, shuffle=True, callbacks=[early_stopping, model_checkpoint])\n",
    "    best_val_score = min(hist.history['val_loss'])\n",
    "    predictions = model.predict(val_data)\n",
    "    auc = roc_auc_score(val_y, predictions)\n",
    "    print('AUC Score', auc)\n",
    "    return model, best_val_score, auc, predictions\n",
    "\n",
    "def train_folds(x, pos_x, y, fold_count, batch_size, get_model_func):\n",
    "    fold_size = len(x) // fold_count\n",
    "    models = []\n",
    "    fold_predictions = []\n",
    "    score, total_auc = 0, 0\n",
    "    for fold_id in range(0, fold_count):\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "        \n",
    "        if fold_id == fold_count - 1:\n",
    "            fold_end = len(x)\n",
    "            \n",
    "        train_x = np.concatenate((x[:fold_start], x[fold_end:]))\n",
    "        train_y = np.concatenate((y[:fold_start], y[fold_end:]))\n",
    "        \n",
    "        val_x = x[fold_start: fold_end]\n",
    "        val_y = y[fold_start: fold_end]\n",
    "        \n",
    "        pos_train_x = np.concatenate((pos_x[:fold_start], pos_x[fold_end:]))\n",
    "        pos_val_x = pos_x[fold_start: fold_end]\n",
    "        print('## In fold {} ## : '.format(fold_id + 1))\n",
    "        model, best_val_score, auc, fold_prediction = _train_model_by_logloss(get_model_func, batch_size, train_x, pos_train_x, train_y, val_x, pos_val_x, val_y, fold_id)\n",
    "        \n",
    "        score += best_val_score\n",
    "        total_auc += auc\n",
    "        fold_predictions.append(fold_prediction)\n",
    "        models.append(model)\n",
    "    return models, score / fold_count, total_auc / fold_count , fold_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Model for computing a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter per channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "        \n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        \n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        attn_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(attn_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, attn_weights]\n",
    "        return result\n",
    "    \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "    \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMaxPooling(Layer):\n",
    "    \"\"\"\n",
    "    K-max pooling layer that extracts the k-highest activations from a sequence (2nd dimension).\n",
    "    With TensorFlow Backend.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_spec = InputSpec(ndim=3)\n",
    "        self.k = k\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[2] * self.k)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # top_k function can only be applied along the last dimension\n",
    "        shifted_input = tf.transpose(inputs, [0, 2, 1])\n",
    "        top_k = tf.nn.top_k(shifted_input, self.k, True, None)[0]\n",
    "        return Flatten()(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_av_cnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    filter_nums = 300\n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.25)(embedding_sequences)\n",
    "    \n",
    "    conv_0 = Conv1D(filter_nums, 1, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_1 = Conv1D(filter_nums, 2, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_2 = Conv1D(filter_nums, 3, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_3 = Conv1D(filter_nums, 4, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    \n",
    "    attn_0 = AttentionWeightedAverage()(conv_0)\n",
    "    avg_0 = GlobalAveragePooling1D()(conv_0)\n",
    "    maxpool_0 = GlobalMaxPooling1D()(conv_0)\n",
    "    \n",
    "    attn_1 = AttentionWeightedAverage()(conv_1)\n",
    "    avg_1 = GlobalAveragePooling1D()(conv_1)\n",
    "    maxpool_1 = GlobalMaxPooling1D()(conv_1)\n",
    "    \n",
    "    attn_2 = AttentionWeightedAverage()(conv_2)\n",
    "    avg_2 = GlobalAveragePooling1D()(conv_2)\n",
    "    maxpool_2 = GlobalMaxPooling1D()(conv_2)\n",
    "    \n",
    "    attn_3 = AttentionWeightedAverage()(conv_3)\n",
    "    avg_3 = GlobalAveragePooling1D()(conv_3)\n",
    "    maxpool_3 = GlobalMaxPooling1D()(conv_3)\n",
    "    \n",
    "    merged_maxpool = merge([maxpool_0, maxpool_1, maxpool_2, maxpool_3], mode='concat', concat_axis=1)\n",
    "    merged_attn = merge([attn_0, attn_1, attn_2, attn_3], mode='concat', concat_axis=1)\n",
    "    merged_avg = merge([avg_0, avg_1, avg_2, avg_3], mode='concat', concat_axis=1)\n",
    "    merged_tensor = merge([merged_maxpool, merged_attn, merged_avg], mode='concat', concat_axis=1)\n",
    "    \n",
    "    output = Dropout(0.7)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_av_pos_cnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words, \n",
    "                                embedding_dim, \n",
    "                                weights=[embedding_matrix], \n",
    "                                input_length=max_sequence_length, \n",
    "                                trainable=False)\n",
    "    \n",
    "    pos_embedding_layer = Embedding(50,\n",
    "                                    30,\n",
    "                                    input_length=max_sequence_length,\n",
    "                                    trainable=True)\n",
    "    \n",
    "    filter_nums = 325\n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    pos_input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='POS')\n",
    "    \n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    pos_sequences = pos_embedding_layer(pos_input_layer)\n",
    "    merged_embedding_layer = concatenate([embedding_sequences, pos_sequences])\n",
    "    final_embedding_sequences = SpatialDropout1D(0.25)(merged_embedding_layer)\n",
    "    \n",
    "    conv_0 = Conv1D(filter_nums, 1, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_1 = Conv1D(filter_nums, 2, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_2 = Conv1D(filter_nums, 3, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_3 = Conv1D(filter_nums, 4, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "\n",
    "    attn_0 = AttentionWeightedAverage()(conv_0)\n",
    "    avg_0 = GlobalAveragePooling1D()(conv_0)\n",
    "    maxpool_0 = GlobalMaxPooling1D()(conv_0)\n",
    "    \n",
    "    attn_1 = AttentionWeightedAverage()(conv_1)\n",
    "    avg_1 = GlobalAveragePooling1D()(conv_1)\n",
    "    maxpool_1 = GlobalMaxPooling1D()(conv_1)\n",
    "    \n",
    "    attn_2 = AttentionWeightedAverage()(conv_2)\n",
    "    avg_2 = GlobalAveragePooling1D()(conv_2)\n",
    "    maxpool_2 = GlobalMaxPooling1D()(conv_2)\n",
    "    \n",
    "    attn_3 = AttentionWeightedAverage()(conv_3)\n",
    "    avg_3 = GlobalAveragePooling1D()(conv_3)\n",
    "    maxpool_3 = GlobalMaxPooling1D()(conv_3)\n",
    "    \n",
    "    merged_tensor_maxpool = merge([maxpool_0, maxpool_1, maxpool_2, maxpool_3], mode='concat', concat_axis=1)\n",
    "    merged_tensor_attn = merge([attn_0, attn_1, attn_2, attn_3], mode='concat', concat_axis=1)\n",
    "    merged_tensor_avg = merge([avg_0, avg_1, avg_2, avg_3], mode='concat', concat_axis=1)\n",
    "    merged_tensor = merge([merged_tensor_maxpool, merged_tensor_attn, merged_tensor_avg], mode='concat', concat_axis=1)\n",
    "    \n",
    "    output = Dropout(0.7)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=[input_layer, pos_input_layer], outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmax_text_cnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    filter_nums = 180\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(embedding_sequences)\n",
    "    \n",
    "    conv_0 = Conv1D(filter_nums, 1, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_1 = Conv1D(filter_nums, 2, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_2 = Conv1D(filter_nums, 3, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    conv_3 = Conv1D(filter_nums, 4, kernel_initializer='normal', padding='valid', activation='relu')(final_embedding_sequences)\n",
    "    \n",
    "    maxpool_0 = KMaxPooling(k=3)(conv_0)\n",
    "    maxpool_1 = KMaxPooling(k=3)(conv_1)\n",
    "    maxpool_2 = KMaxPooling(k=3)(conv_2)\n",
    "    maxpool_3 = KMaxPooling(k=3)(conv_3)\n",
    "    \n",
    "    merged_tensor = merge([maxpool_0, maxpool_1, maxpool_2, maxpool_3], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.6)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rcnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    recurrent_units = 64\n",
    "    filter_nums = 128\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.2)(embedding_sequences)\n",
    "    \n",
    "    rnn_layer = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(final_embedding_sequences)\n",
    "    conv_layer = Conv1D(filter_nums, 2, kernel_initializer='normal', padding='valid', activation='relu', strides=1)(rnn_layer)\n",
    "    \n",
    "    maxpool = GlobalMaxPooling1D()(conv_layer)\n",
    "    attn = AttentionWeightedAverage()(conv_layer)\n",
    "    avg = GlobalAveragePooling1D()(conv_layer)\n",
    "    \n",
    "    merged_tensor = merge([maxpool, attn, avg], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.5)(merged_tensor)\n",
    "    output = Dense(units=120, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_av_rnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    recurrent_units = 64\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.25)(embedding_sequences)\n",
    "    \n",
    "    rnn_layer_0 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(final_embedding_sequences)\n",
    "    rnn_layer_1 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(rnn_layer_0)\n",
    "    merged_rnn_layer = merge([rnn_layer_0, rnn_layer_1], mode='concat', concat_axis=2)\n",
    "    \n",
    "    last_layer = Lambda(lambda t: t[:, -1], name='last_layer')(merged_rnn_layer)\n",
    "    maxpool = GlobalMaxPooling1D()(merged_rnn_layer)\n",
    "    attn = AttentionWeightedAverage()(merged_rnn_layer)\n",
    "    avg = GlobalAveragePooling1D()(merged_rnn_layer)\n",
    "    \n",
    "    merged_tensor = merge([last_layer, maxpool, attn, avg], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.5)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_av_pos_rnn(nb_words, embedding_dim, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    pos_embedding_layer = Embedding(50,\n",
    "                                    35,\n",
    "                                    input_length=max_sequence_length,\n",
    "                                    trainable=True)\n",
    "    \n",
    "    recurrent_units = 64\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    pos_input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='POS')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    pos_sequences = pos_embedding_layer(pos_input_layer)\n",
    "    merged_embedding_layer = concatenate([embedding_sequences, pos_sequences], axis=2)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.2)(merged_embedding_layer)\n",
    "    \n",
    "    rnn_layer_0 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(final_embedding_sequences)\n",
    "    rnn_layer_0 = SpatialDropout1D(0.3)(rnn_layer_0)\n",
    "    rnn_layer_1 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(rnn_layer_0)\n",
    "    \n",
    "    last_layer = Lambda(lambda t: t[:, -1], name='last_layer')(rnn_layer_1)\n",
    "    maxpool = GlobalMaxPooling1D()(rnn_layer_1)\n",
    "    attn = AttentionWeightedAverage()(rnn_layer_1)\n",
    "    avg = GlobalAveragePooling1D()(rnn_layer_1)\n",
    "    \n",
    "    merged_tensor = merge([last_layer, maxpool, attn, avg], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.5)(merged_tensor)\n",
    "    output = Dense(units=144, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=[input_layer, pos_input_layer], outputs=output)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dropout_bigru(nb_words, embedding_dims, embedding_matrix, max_sequence_length, out_size):\n",
    "    embedding_layer = Emebdding(nb_words,\n",
    "                                embedding_dims,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=False)\n",
    "    \n",
    "    recurrent_units = 64\n",
    "    \n",
    "    input_layer = Input(shape=(max_sequence_length,), dtype='int32', name='Onehot')\n",
    "    embedding_sequences = embedding_layer(input_layer)\n",
    "    final_embedding_sequences = SpatialDropout1D(0.2)(embedding_sequences)\n",
    "    \n",
    "    rnn_layer = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(final_embedding_sequences)\n",
    "    rnn_layer = Dropout(0.35)(rnn_layer)\n",
    "    rnn_layer = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(rnn_layer)\n",
    "    \n",
    "    last_layer = Lambda(lambda t: t[:, -1])(rnn_layer)\n",
    "    maxpool = GlobalMaxPooling1D()(rnn_layer)\n",
    "    avg = GlobalAveragePooling1D()(rnn_layer)\n",
    "    \n",
    "    merged_tensor = merge([last_layer, maxpool, avg], mode='concat', concat_axis=1)\n",
    "    output = Dropout(0.5)(merged_tensor)\n",
    "    output = Dense(units=72, activation='relu')(output)\n",
    "    output = Dense(units=out_size, activation='sigmoid')(output)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        super(DotAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn_vector = nn.Parameter(torch.Tensor(1, hidden_size), requires_grad=True)\n",
    "        \n",
    "        init.xavier_uniform(self.attn_vector.data)\n",
    "        \n",
    "    def forward(self, inputs, lengths=None):\n",
    "        \"\"\"\n",
    "        Return a scalar (All hiddens to only one weight for one output)\n",
    "        (batch_size, max_len, hidden_size) * (batch_size, hidden_size, 1) --> (batch_size, max_len, 1)\n",
    "        \"\"\"\n",
    "        batch_size, max_len = inputs.size()[:2]\n",
    "        \n",
    "        weights = torch.bmm(inputs,\n",
    "                            self.attn_vector              # (1, hidden)\n",
    "                            .unsqueeze(0)                 # (1, 1, hidden)\n",
    "                            .transpose(2, 1)              # (1, hidden, 1)\n",
    "                            .repeat(batch_size, 1, 1))    # (batch_size, hidden, 1)\n",
    "        \n",
    "        attn_energies = F.softmax(F.relu(weights.squeeze()))\n",
    "        \n",
    "        # create mask based on the sentence length\n",
    "        mask = Variable（torch.ones(attn_energies.size()).cuda()\n",
    "        for i, l in enumerate(lengths):\n",
    "            if l < max_len:\n",
    "                mask[:, l:] = 0\n",
    "                \n",
    "        # apply mask and renormalize attention scores (weights)\n",
    "        masked = attn_energies * mask\n",
    "        _sums = masked.sum(-1).expand_as(attn_energies)\n",
    "        attention_weights = masked.div(_sum)\n",
    "        \n",
    "        # apply attention weights\n",
    "        weighted = torch.mul(inputs, attention_weights.unsqueeze(-1).expand_as(inputs))\n",
    "        \n",
    "        # get the final fixed vector representations of the sentences\n",
    "        representations = weighted.sum(1).squeeze()\n",
    "        \n",
    "        return representations, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDropout():\n",
    "    \"\"\"\n",
    "    Implement of word embedding dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, p=0.5):\n",
    "        super(EmbeddingDropout, self).__init__()\n",
    "        if p < 0 or p > 1:\n",
    "            raise ValueError(\"dropout probability has to be between 0 and 1, but got {}\".format(p))\n",
    "            \n",
    "        self.p = p\n",
    "        self.trainable = True\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        if self.p > 0 and self.trainable:\n",
    "            dim = inputs.dim()\n",
    "            if dim == 1:\n",
    "                inputs = inputs.view(1, -1)\n",
    "            batch_size = inputs.size(0)\n",
    "            for i in range(batch_size):\n",
    "                x = np.unique(inputs[i].numpy())\n",
    "                x = np.nonzero(x)[0]\n",
    "                if len(x) == 0:\n",
    "                    return inputs\n",
    "                x = torch.from_numpy(x)\n",
    "                noise = x.new().resize_as_(x)\n",
    "                noise.bernoulli_(self.p)\n",
    "                x = x.mul(noise)\n",
    "                for value in x:\n",
    "                    if value > 0:\n",
    "                        mask = inputs[i].eq(value)\n",
    "                        inputs[i].masked_fill_(mask, 0)\n",
    "            if dim == 1:\n",
    "                inputs = inputs.view(-1)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDropout(nn.Module):\n",
    "    \n",
    "    def __init__(self, p=0.5):\n",
    "        super(SequentialDropout, self).__init__()\n",
    "        if p < 0 or p > 1:\n",
    "            raise ValueError(\"dropout probability has to be between 0 and 1, but got {}\".format(p))\n",
    "            \n",
    "        self.p = p\n",
    "        self.restart = True\n",
    "        self.trainable = True\n",
    "        \n",
    "    def _make_noise(self, inputs):\n",
    "        return Variable(inputs.data.new().resize_as_(inputs.data))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        if self.p > 0 and self.trainable:\n",
    "            if self.restart:\n",
    "                self.noise = self._make_noise(inputs)\n",
    "                self.noise.data.bernoulli_(1 - self.p).div_(1 - self.p)\n",
    "                if self.p == 1:\n",
    "                    self.noise.data.fill_(0)\n",
    "                self.noise = self.noise.expand_as(inputs)\n",
    "                self.restart = False\n",
    "            return inputs.mul(self.noise)\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    def end_of_sequence(self):\n",
    "        self.restart = True\n",
    "        \n",
    "    def backward(self, grad_output):\n",
    "        self.end_of_sequence()\n",
    "        if self.p > 0 and self.trainable:\n",
    "            return grad_output.mul(self.noise)\n",
    "        else:\n",
    "            return grad_output\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return type(self).__name__ + '({:.4f})'.format(self.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Embedding & Sequential Dropout\n",
    "seq_drop_model = SequentialDropout(p=0.5)\n",
    "input_data= Variable(torch.ones(1, 10), volatile=True)\n",
    "\n",
    "dist_total = torch.zeros(1)\n",
    "output_last = seq_drop_model(input_data)\n",
    "for i in range(50):\n",
    "    output_new = seq_drop_model(input_data)\n",
    "    dist_total += torch.dist(output_new, output_last).data\n",
    "    output_last = output_new\n",
    "    \n",
    "if not torch.equal(dist_total, torch.zeros(1)):\n",
    "    print('Error')\n",
    "    print(dist_total)\n",
    "    \n",
    "seq_drop_model.end_of_sequence()\n",
    "\n",
    "dist_total = torch.zeros(1)\n",
    "for i in range(50):\n",
    "    dist_total += torch.dist(output_last, seq_drop_model(input_data)).data\n",
    "    seq_drop_model.end_of_sequence()\n",
    "    \n",
    "if torch.equal(dist_total, torch.zeros(1)):\n",
    "    print('Error')\n",
    "    \n",
    "emb_drop_model = EmbeddingDropout(p=0.15)\n",
    "input_data = torch.Tensor([[1,2,3,0,0], [5,3,2,2,0]]).long()\n",
    "print(input_data)\n",
    "print(emb_drop_model.forward(input_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'fasttext-avcnn-pos-' + str(nb_words) + 'vocabulary-' + str(MAX_SEQUENCE_LENGTH) + 'length'\n",
    "model = get_av_pos_cnn(nb_words, EMBEDDING_DIM, embedding_matrix, MAX_SEQUENCE_LENGTH, 6)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, val_loss, total_auc, fold_predictions = train_folds(train_data, pos_train_data, train_labels, FOLD_COUNT, BATCH_SIZE, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Overall val-loss: {}, AUC {}'.format(val_loss, total_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_path_prefix = 'results/CNN_Based/' + model_name\n",
    "\n",
    "print('Predicting testing results...')\n",
    "test_predicts_list = []\n",
    "for fold_id, model in enumerate(models):\n",
    "    test_data_dict = {'Onehot': test_data, 'POS': pos_test_data}\n",
    "    test_predict = model.predict(test_data_dict, batch_size=BATCH_SIZE, verbose=1)\n",
    "    test_predicts_list.append(test_predict)\n",
    "    np.save('parameters_pool/AVPOSCNN/{}-AV-POS-CNN.npy'.format(fold_id), test_predict)\n",
    "    \n",
    "test_predicts = np.zeros(test_predicts_list[0].shape)\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts += fold_predict\n",
    "test_predicts /= len(test_predicts_list)\n",
    "\n",
    "test_ids = test_df['id'].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "\n",
    "test_predicts = pd.DataFrame(data=test_predicts, columns=list_classes)\n",
    "test_predicts['id'] = test_ids\n",
    "test_predicts = test_predicts[['id'] + list_classes]\n",
    "submit_path = submit_path_prefix + '-L{:4f}-A{:4f}.csv'.format(val_loss, total_auc)\n",
    "test_predicts.to_csv(submit_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOB (Out-of-Bag) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold_predictions = np.concatenate((fold_predictions), axis=0)\n",
    "train_auc = roc_auc_score(train_labels, train_fold_predictions)\n",
    "print('Training AUC', train_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicting training results...')\n",
    "train_ids = train_df['id'].values\n",
    "train_ids = train_ids.reshape((len(train_ids), 1))\n",
    "\n",
    "train_predicts = pd.DataFrame(data=train_fold_predictions, columns=list_classes)\n",
    "train_predicts['id'] = train_ids\n",
    "train_predicts = train_predicts[['id'] + list_classes]\n",
    "submit_path = submit_path_prefix + '-(Train)-L{:4f}-A{:4f}.csv'.format(val_loss, train_auc)\n",
    "train_predicts.to_csv(submit_path, index=False)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Ensemble (For Test Format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging(arrs, path):\n",
    "    print(\"Doing ensemble on\")\n",
    "    subs = []\n",
    "    for arr in arrs:\n",
    "        print(arr)\n",
    "        subs.append(pd.read_csv(arr))\n",
    "    \n",
    "    for sub in subs[1:]:\n",
    "        for c in list_classes:\n",
    "            subs[0][c] += sub[c]\n",
    "    \n",
    "    for c in list_classes:\n",
    "        subs[0][c] /= len(subs)\n",
    "        \n",
    "    subs[0].to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_corr(arr1, arr2):\n",
    "    res = 0\n",
    "    for col in arr1.columns.values[1:]:\n",
    "        cur = arr1[col].corr(arr2[col])\n",
    "        corr = (arr1[col].rank() / len(arr1)).corr(arr2[col].rank() / len(arr2))\n",
    "        print(col, corr)\n",
    "        res += corr\n",
    "    print(\"Avg Rank: \", res / len(arr1.columns.values[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Estimation (For Train Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = train_df.iloc[:, :8]\n",
    "ground_truth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_prediction = pd.read_csv('results/RNN_Based/fasttext-avrnn-100000vocabulary-350length-(Train)-L0.013377-A0.998050.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_term_pos(train, pred, check_column):\n",
    "    sub_train = train[check_column]\n",
    "    sub_pred = pred[check_column]\n",
    "    sub_pred = sub_pred.round()\n",
    "    diff = (sub_pred != sub_train) & (sub_train == 1)\n",
    "    print('Wrong pos-predictions number is {}\\tWrong rate: {}'.format(diff.sum(), diff.sum() / len(sub_pred)))\n",
    "    pos = pd.DataFrame()\n",
    "    pos['id'] = train[diff]['id']\n",
    "    pos['text'] = train[diff]['comment_text']\n",
    "    pos['pred_val'] = pred[diff][check_column]\n",
    "    pos['label'] = train[diff][check_column]\n",
    "    return pos\n",
    "\n",
    "def get_error_term_neg(train, pred, check_column):\n",
    "    sub_train = train[check_column]\n",
    "    sub_pred = pred[check_column]\n",
    "    sub_pred = sub_pred.round()\n",
    "    diff = (sub_pred != sub_train) & (sub_train == 0)\n",
    "    print('Wrong neg-predictions number is {}\\tWrong rate: {}'.format(diff.sum(), diff.sum() / len(sub_pred)))\n",
    "    neg = pd.DataFrame()\n",
    "    neg['id'] = train[diff]['id']\n",
    "    neg['text'] = train[diff]['comment_text']\n",
    "    neg['pred_val'] = pred[diff][check_column]\n",
    "    neg['label'] = train[diff][check_column]\n",
    "    return neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in list_classes:\n",
    "    print('In term: ', term)\n",
    "    err_neg = get_error_term_neg(train_df, check_prediction, term)\n",
    "    err_pos = get_error_term_pos(train_df, check_prediction, term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
