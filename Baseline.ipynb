{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, re, csv, codecs, operator, sys, gc\n",
    "from collections import defaultdict, OrderedDict\n",
    "from tqdm import tqdm\n",
    "import jieba\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.set_dictionary('dict.txt.big.txt')\n",
    "CLEAN_WORD_PATH = None\n",
    "TRAIN_DATA_FILE = 'data_original.csv'\n",
    "TEST_DATA_FILE = 'test.csv'\n",
    "FOLD_COUNT = 10\n",
    "list_classes = ['不受理', '不成立', '成立', '當事人不到場', '聲請人撤回']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_comments = []\n",
    "temp = train_df['調解內容與決議'].fillna('no sentence').values\n",
    "for line in temp:\n",
    "    if '經調解結果如左：' in line:\n",
    "        a = line.split('經調解結果如左：')[0]\n",
    "    else:\n",
    "        a = line.split('經調解結果如下：')[0]\n",
    "    cleaned_comments.append(a)\n",
    "train_df['調解內容與決議'] = cleaned_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.any(train_df.isnull()) == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comments = train_df['調解內容與決議'].fillna('no comment').values\n",
    "train_comments_lengths = [len(str(s)) for s in train_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_comments(arr):\n",
    "    print(\"MAX LENGTH:\\t\\t\", np.max(arr))\n",
    "    print(\"AVG LENGTH:\\t\\t\", np.average(arr))\n",
    "    print(\"MIN LENGTH:\\t\\t\", np.min(arr))\n",
    "    print(\"STANDARD DIVISION:\\t\", np.std(arr))\n",
    "    print(\"RANGE:\\t\\t\\t\", np.min(arr), \" to \", np.average(arr) + 2 * np.std(arr))\n",
    "    \n",
    "print(\"------Train------\")\n",
    "explore_comments(train_comments_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df['是否成立'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "pd.Series(train_comments_lengths).astype(int).hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Lable to One-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Score = train_df['是否成立']\n",
    "data = pd.get_dummies(Score)\n",
    "train_df = pd.concat([train_df, data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_df.iloc[:, 22:].sum()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "ax = sns.barplot(x.index, x.values, alpha=0.8)\n",
    "plt.title(\"Summary\")\n",
    "plt.ylabel('Occurrences', fontsize=12)\n",
    "plt.xlabel('Results', fontsize=12)\n",
    "\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width() / 2, height + 5, ha='center', va='bottom', s='{:.1f}'.format(abs(label)))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = plt.cm.plasma\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('Correlation of features', y=1.05, size=14)\n",
    "sns.heatmap(data.astype(float).corr(), linewidths=0.1, vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pd.unique(train_df['people'].fillna('no people').values)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = train_df.apply(lambda row: row['是否成立'] == '成立' or row['是否成立'] == '不成立', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = train_df[new_df]\n",
    "len(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "people, ty, success, unsuc= [], [], [], []\n",
    "for n in pd.unique(df_new['people'].values):\n",
    "    for t in pd.unique(df_new['案件細節類型'].values):\n",
    "        a = df_new[df_new['people'] == n]\n",
    "        if a[a['案件細節類型'] == t]['是否成立'].empty:\n",
    "            continue\n",
    "        else:\n",
    "            b = a[a['案件細節類型'] == t]['是否成立'].value_counts()\n",
    "            people.append(n)\n",
    "            ty.append(t)\n",
    "            if len(b) == 2:\n",
    "                success.append(b['成立'])\n",
    "                unsuc.append(b['不成立'])\n",
    "            else:\n",
    "                try:\n",
    "                    success.append(b['成立'])\n",
    "                    unsuc.append('0')\n",
    "                except:\n",
    "                    success.append('0')\n",
    "                    unsuc.append(b['不成立'])\n",
    "print(len(people), len(ty), len(success), len(unsuc))\n",
    "result['people'] = people\n",
    "result['案件細節類型'] = ty\n",
    "result['成立'] = success\n",
    "result['不成立'] = unsuc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('results/success.csv', encoding='big5', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Clearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords=False):\n",
    "    # remove url\n",
    "    text = re.sub(r\"(https?:\\/\\/)*(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \"\", text)\n",
    "    \n",
    "    # Special expressions\n",
    "    text = re.sub(r'〈下同〉', '', text)\n",
    "    text = re.sub(r'（車號：[a-zA-Z0-9－]*號）', '', text)\n",
    "    text = re.sub(r'（[\\' \\']*）', '', text)\n",
    "    text = re.sub(r'（下同）', '', text)\n",
    "    text = re.sub(r'\\\\r\\\\n[0-9a-zA-Z\\\\r\\\\n]*', '', text)\n",
    "    text = re.sub(r'(口)', '', text)\n",
    "    \n",
    "    text = re.sub(r',', '，', text)\n",
    "    text = re.sub(r'\\.+', '...', text)\n",
    "    text = re.sub(r'\\.{6}', '...', text)\n",
    "    text = re.sub(r'…', '...', text)\n",
    "    text = re.sub(r';', '；', text)\n",
    "    text = re.sub(r'°', '。', text)\n",
    "    text = re.sub(r'】', ']', text)\n",
    "    text = re.sub(r'【', '[', text)\n",
    "    text = re.sub(r'\\)', '\\）', text)\n",
    "    text = re.sub(r'\\(', '\\（', text)\n",
    "    text = re.sub(r'“', '\"', text)\n",
    "    text = re.sub(r' ', '', text)\n",
    "    text = re.sub(r'”', '\"', text)\n",
    "    text = re.sub(r'～', '~', text)\n",
    "    text = re.sub(r'·', '。', text)\n",
    "    text = re.sub(r'!', '！', text)\n",
    "    text = re.sub(r'—', '-', text)\n",
    "    text = re.sub(r'》', '\\）', text)\n",
    "    text = re.sub(r'《', '\\（', text)\n",
    "    text = re.sub(r'\\?', '\\？', text)\n",
    "    text = re.sub(r'。。。', '...', text)\n",
    "    text = re.sub(r'。。。。。。', '...', text)\n",
    "    text = re.sub(r':', '：', text)\n",
    "    \n",
    "    text = special_alpha_removal.sub('', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal = re.compile(r'[^a-z\\d ]', re.IGNORECASE)\n",
    "\n",
    "# regex to remove all Alpha Numeric and space\n",
    "special_alpha_removal = re.compile(r'[a-z\\d ]', re.IGNORECASE)\n",
    "\n",
    "# regex to replace all numeric\n",
    "replace_numbers = re.compile(r'\\d+', re.IGNORECASE)\n",
    "\n",
    "# regex to replace ###...\n",
    "replace_sharp = re.compile(r'[#]+', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data cleaning...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "cleaned_train_comments = []\n",
    "print('Processing data cleaning...')\n",
    "\n",
    "for text in train_comments:\n",
    "    cleaned_train_comments.append(clean_text(text))\n",
    "    \n",
    "train_df['cleaned_comments'] = cleaned_train_comments\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word segmentation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6277 [00:00<?, ?it/s]Building prefix dict from /home/eternal/下載/Competition/CL_HW/dict.txt.big.txt ...\n",
      "Loading model from cache /tmp/jieba.u3a0a53d5fde3fc6cdb96c613fa12b168.cache\n",
      "Loading model cost 1.034 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "100%|██████████| 6277/6277 [00:05<00:00, 1088.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word_dict = defaultdict(int)\n",
    "\n",
    "for sentence in tqdm(train_df['調解內容與決議']):\n",
    "    seg_list = jieba.cut(str(sentence), cut_all=False)\n",
    "    for word in seg_list:\n",
    "        word_dict[word] += 1\n",
    "word_dict = sorted(word_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(len(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6277/6277 [00:03<00:00, 1628.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_word_dict = defaultdict(int)\n",
    "\n",
    "for sentence in tqdm(train_df['cleaned_comments']):\n",
    "    seg_list = jieba.cut(str(sentence), cut_all=False)\n",
    "    for word in seg_list:\n",
    "        cleaned_word_dict[word] += 1\n",
    "cleaned_word_dict = sorted(cleaned_word_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(len(cleaned_word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6277/6277 [00:03<00:00, 1759.98it/s]\n"
     ]
    }
   ],
   "source": [
    "cut_sentences = []\n",
    "for sentence in tqdm(train_df['cleaned_comments']):\n",
    "    seg_list = jieba.cut(str(sentence), cut_all=False)\n",
    "    cut_sentences.append(\" \".join(seg_list))\n",
    "train_df['cleaned_comments_cut'] = cut_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_sentences = train_df['cleaned_comments_cut']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick view of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_list, sign_list, dig_english_list = [], [], []\n",
    "for word, count in word_dict:\n",
    "    for char in word:\n",
    "        if char >= u'\\u4E00' and char <= u\"\\u9FA5\":\n",
    "            chinese_list.append((word, count))\n",
    "        elif (char >= u'\\u0041' and char <= u'\\u005A') or (char >= u'\\u0061' and char <= u'\\u007A') or (char >= u'\\u0030' and char <= u'\\u0039'):\n",
    "            dig_english_list.append((word, count))\n",
    "            break\n",
    "        else:\n",
    "            sign_list.append((word, count))\n",
    "            break\n",
    "sorted_dig_english_list = sorted(set(dig_english_list), key=lambda x: x[1], reverse=True)\n",
    "sorted_sign_list = sorted(set(sign_list), key=lambda x: x[1], reverse=True)\n",
    "sorted_chinese_list = sorted(set(chinese_list), key=lambda x: x[1], reverse=True)\n",
    "print(\"chinese_word: \", len(sorted_chinese_list))\n",
    "print(\"dig_english_word: \", len(sorted_dig_english_list))\n",
    "print(\"sign_count: \", len(sorted_sign_list))\n",
    "print(sorted_chinese_list[:10000], '\\n\\n', sorted_dig_english_list[:50], '\\n\\n', sorted_sign_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_list, sign_list, dig_english_list = [], [], []\n",
    "for word, count in cleaned_word_dict:\n",
    "    for char in word:\n",
    "        if char >= u'\\u4E00' and char <= u\"\\u9FA5\":\n",
    "            chinese_list.append((word, count))\n",
    "        elif (char >= u'\\u0041' and char <= u'\\u005A') or (char >= u'\\u0061' and char <= u'\\u007A') or (char >= u'\\u0030' and char <= u'\\u0039'):\n",
    "            dig_english_list.append((word, count))\n",
    "            break\n",
    "        else:\n",
    "            sign_list.append((word, count))\n",
    "            break\n",
    "sorted_dig_english_list = sorted(set(dig_english_list), key=lambda x: x[1], reverse=True)\n",
    "sorted_sign_list = sorted(set(sign_list), key=lambda x: x[1], reverse=True)\n",
    "sorted_chinese_list = sorted(set(chinese_list), key=lambda x: x[1], reverse=True)\n",
    "print(\"chinese_word: \", len(sorted_chinese_list))\n",
    "print(\"dig_english_word: \", len(sorted_dig_english_list))\n",
    "print(\"sign_count: \", len(sorted_sign_list))\n",
    "print(sorted_chinese_list[:10000], '\\n\\n', sorted_dig_english_list[:50], '\\n\\n', sorted_sign_list[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=20000, filters='\"#$%&()+,-./:;<=>@[\\\\]^_`{|}~\\t\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically train vocab & tokenizer...\n",
      "Found 16286 unique tokens\n"
     ]
    }
   ],
   "source": [
    "print('Automatically train vocab & tokenizer...')\n",
    "tokenizer.fit_on_texts(cut_sentences)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(cut_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary Veiw & Processing Col Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = np.array([cols[3]] + cols[5:13] + cols[14:21])\n",
    "label_column = np.array([cols[13]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['結案時間'] = [replace_sharp.sub('99:99', str(text)) for text in train_df['結案時間'].fillna(' ').values]\n",
    "train_df['收件時間'] = [replace_sharp.sub('99:99', str(text)) for text in train_df['收件時間'].fillna(' ').values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6277\n"
     ]
    }
   ],
   "source": [
    "new_col = []\n",
    "for lin in train_df['對照人'].fillna(' ').values:\n",
    "    temp = \"000000\"\n",
    "    if temp in str(lin):\n",
    "        new_col.append(lin)\n",
    "    else:\n",
    "        new_col.append(\"gb\")\n",
    "\n",
    "print(len(new_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([192, 616,  74, ...,  73,  79,  88]),\n",
       " array([46, 42, 50, ..., 13, 37, 37]),\n",
       " array([2, 2, 0, ..., 2, 2, 2]),\n",
       " array([0, 1, 0, ..., 1, 0, 1]),\n",
       " array([128, 224, 226, ..., 224, 128, 224]),\n",
       " array([182, 304, 142, ..., 133, 192, 182]),\n",
       " array([275, 645, 156, ..., 259, 327, 159]),\n",
       " array([3, 3, 3, ..., 1, 1, 1]),\n",
       " array([13, 18,  0, ...,  5,  0, 14]),\n",
       " array([   0,    0,    0, ...,   47, 1176,  532]),\n",
       " array([4, 4, 4, ..., 5, 5, 5]),\n",
       " array([1, 4, 4, ..., 1, 1, 1]),\n",
       " array([2, 4, 2, ..., 1, 1, 2]),\n",
       " array([0, 1, 0, ..., 0, 0, 0]),\n",
       " array([ 69, 200, 133, ...,  70,  70,  70]),\n",
       " array([0, 0, 0, ..., 1, 1, 1])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_data = []\n",
    "for column in data_columns:\n",
    "    try:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train_df[column].fillna(' ').values))\n",
    "        feature_data.append(le.transform(list(train_df[column].fillna(' ').values)))\n",
    "    except:\n",
    "        print(column)\n",
    "feature_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = []\n",
    "for line in zip(feature_data[0], feature_data[0], feature_data[1], feature_data[2], feature_data[3], feature_data[4], feature_data[5], \n",
    "                feature_data[6], feature_data[7], feature_data[8], feature_data[9], feature_data[10], feature_data[11], feature_data[12],\n",
    "                feature_data[13], feature_data[14], feature_data[15]):\n",
    "    feature_sets.append(line)\n",
    "feature_sets = np.array(feature_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6277, 0, (6277, 17), (0, 17))"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data, train_feature_sets, test_feature_sets = train_test_split(train_df, feature_sets, test_size=0, shuffle=True)\n",
    "len(train_data), len(test_data), train_feature_sets.shape, test_feature_sets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "train_data = train_df\n",
    "train_feature_sets = feature_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_data['cleaned_comments_cut']\n",
    "# test_sentences = test_data['cleaned_comments_cut']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vectorization process Done!\n"
     ]
    }
   ],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 3),\n",
    "    max_features=20000\n",
    ")\n",
    "word_vectorizer.fit(cut_sentences)\n",
    "train_word_features = word_vectorizer.transform(train_sentences)\n",
    "# test_word_features = word_vectorizer.transform(test_sentences)\n",
    "print('Word vectorization process Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6277, 20000)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf_features = train_word_features.tocsr()\n",
    "# test_tfidf_features = test_word_features.tocsr()\n",
    "train_tfidf_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6277, 20000)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf_features = vstack([train_tfidf_features, test_tfidf_features])\n",
    "tfidf_features = train_tfidf_features\n",
    "tfidf_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model with Tfidf Features (Multi-Lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## In fold 1 ##\n",
      "Processing 不受理 ...\n",
      "Training accuracy is 0.9968135953266065\n",
      "Validation accuracy is 0.9984076433121019\n",
      "Processing 不成立 ...\n",
      "Training accuracy is 0.958399716764029\n",
      "Validation accuracy is 0.7038216560509554\n",
      "Processing 成立 ...\n",
      "Training accuracy is 0.9702602230483272\n",
      "Validation accuracy is 0.6751592356687898\n",
      "Processing 當事人不到場 ...\n",
      "Training accuracy is 0.9674278633386441\n",
      "Validation accuracy is 0.8423566878980892\n",
      "Processing 聲請人撤回 ...\n",
      "Training accuracy is 0.9638874137015401\n",
      "Validation accuracy is 0.9585987261146497\n",
      "## In fold 2 ##\n",
      "Processing 不受理 ...\n",
      "Training accuracy is 0.9971676402903169\n",
      "Validation accuracy is 0.9936305732484076\n",
      "Processing 不成立 ...\n",
      "Training accuracy is 0.9534430872720836\n",
      "Validation accuracy is 0.7229299363057324\n",
      "Processing 成立 ...\n",
      "Training accuracy is 0.9607010090281466\n",
      "Validation accuracy is 0.697452229299363\n",
      "Processing 當事人不到場 ...\n",
      "Training accuracy is 0.9651265710745265\n",
      "Validation accuracy is 0.8471337579617835\n",
      "Processing 聲請人撤回 ...\n",
      "Training accuracy is 0.9651265710745265\n",
      "Validation accuracy is 0.9426751592356688\n",
      "## In fold 3 ##\n",
      "Processing 不受理 ...\n",
      "Training accuracy is 0.9968135953266065\n",
      "Validation accuracy is 0.9984076433121019\n",
      "Processing 不成立 ...\n",
      "Training accuracy is 0.9543281996813595\n",
      "Validation accuracy is 0.7054140127388535\n",
      "Processing 成立 ...\n",
      "Training accuracy is 0.9612320764737121\n",
      "Validation accuracy is 0.6831210191082803\n",
      "Processing 當事人不到場 ...\n",
      "Training accuracy is 0.9628252788104089\n",
      "Validation accuracy is 0.8726114649681529\n",
      "Processing 聲請人撤回 ...\n",
      "Training accuracy is 0.9638874137015401\n",
      "Validation accuracy is 0.9697452229299363\n",
      "## In fold 4 ##\n",
      "Processing 不受理 ...\n",
      "Training accuracy is 0.9969906178084617\n",
      "Validation accuracy is 0.9936305732484076\n",
      "Processing 不成立 ...\n",
      "Training accuracy is 0.9518498849353868\n",
      "Validation accuracy is 0.7468152866242038\n",
      "Processing 成立 ...\n",
      "Training accuracy is 0.9644184811471057\n",
      "Validation accuracy is 0.7229299363057324\n",
      "Processing 當事人不到場 ...\n",
      "Training accuracy is 0.9649495485926712\n",
      "Validation accuracy is 0.8869426751592356\n",
      "Processing 聲請人撤回 ...\n",
      "Training accuracy is 0.9642414586652505\n",
      "Validation accuracy is 0.9554140127388535\n",
      "## In fold 5 ##\n",
      "Processing 不受理 ...\n",
      "Training accuracy is 0.9969906178084617\n",
      "Validation accuracy is 0.9984076433121019\n",
      "Processing 不成立 ...\n",
      "Training accuracy is 0.9500796601168349\n",
      "Validation accuracy is 0.7595541401273885\n",
      "Processing 成立 ...\n",
      "Training accuracy is 0.9553903345724907\n",
      "Validation accuracy is 0.8996815286624203\n",
      "Processing 當事人不到場 ...\n",
      "Training accuracy is 0.9628252788104089\n",
      "Validation accuracy is 0.85828025477707\n",
      "Processing 聲請人撤回 ...\n",
      "Training accuracy is 0.9663657284475128\n",
      "Validation accuracy is 0.9235668789808917\n",
      "## In fold 6 ##\n",
      "Processing 不受理 ...\n",
      "Training accuracy is 0.9968135953266065\n",
      "Validation accuracy is 0.9984076433121019\n",
      "Processing 不成立 ...\n",
      "Training accuracy is 0.9474243228890069\n",
      "Validation accuracy is 0.7945859872611465\n",
      "Processing 成立 ...\n",
      "Training accuracy is 0.9525579748628076\n",
      "Validation accuracy is 0.9808917197452229\n",
      "Processing 當事人不到場 ...\n",
      "Training accuracy is 0.961055053991857\n",
      "Validation accuracy is 0.8407643312101911\n",
      "Processing 聲請人撤回 ...\n",
      "Training accuracy is 0.9642414586652505\n",
      "Validation accuracy is 0.910828025477707\n",
      "## In fold 7 ##\n",
      "Processing 不受理 ...\n",
      "Training accuracy is 0.9978757302177377\n",
      "Validation accuracy is 0.9888535031847133\n",
      "Processing 不成立 ...\n",
      "Training accuracy is 0.9481324128164277\n",
      "Validation accuracy is 0.8136942675159236\n",
      "Processing 成立 ...\n",
      "Training accuracy is 0.9523809523809523\n",
      "Validation accuracy is 0.9920382165605095\n",
      "Processing 當事人不到場 ...\n",
      "Training accuracy is 0.9621171888829881\n",
      "Validation accuracy is 0.8726114649681529\n",
      "Processing 聲請人撤回 ...\n",
      "Training accuracy is 0.9642414586652505\n",
      "Validation accuracy is 0.928343949044586\n",
      "## In fold 8 ##\n",
      "Processing 不受理 ...\n",
      "Training accuracy is 0.9968141592920354\n",
      "Validation accuracy is 0.9984051036682615\n",
      "Processing 不成立 ...\n",
      "Training accuracy is 0.9509734513274336\n",
      "Validation accuracy is 0.8054226475279107\n",
      "Processing 成立 ...\n",
      "Training accuracy is 0.9539823008849557\n",
      "Validation accuracy is 0.9585326953748007\n",
      "Processing 當事人不到場 ...\n",
      "Training accuracy is 0.9638938053097346\n",
      "Validation accuracy is 0.8452950558213717\n",
      "Processing 聲請人撤回 ...\n",
      "Training accuracy is 0.9654867256637168\n",
      "Validation accuracy is 0.9505582137161085\n",
      "## In fold 9 ##\n",
      "Processing 不受理 ...\n",
      "Training accuracy is 0.9966371681415929\n",
      "Validation accuracy is 1.0\n",
      "Processing 不成立 ...\n",
      "Training accuracy is 0.9499115044247788\n",
      "Validation accuracy is 0.7687400318979266\n",
      "Processing 成立 ...\n",
      "Training accuracy is 0.9582300884955752\n",
      "Validation accuracy is 0.9409888357256778\n",
      "Processing 當事人不到場 ...\n",
      "Training accuracy is 0.9647787610619469\n",
      "Validation accuracy is 0.8500797448165869\n",
      "Processing 聲請人撤回 ...\n",
      "Training accuracy is 0.9653097345132743\n",
      "Validation accuracy is 0.9441786283891547\n",
      "## In fold 10 ##\n",
      "Processing 不受理 ...\n",
      "Training accuracy is 0.9968141592920354\n",
      "Validation accuracy is 0.9984051036682615\n",
      "Processing 不成立 ...\n",
      "Training accuracy is 0.9497345132743363\n",
      "Validation accuracy is 0.8149920255183413\n",
      "Processing 成立 ...\n",
      "Training accuracy is 0.9550442477876107\n",
      "Validation accuracy is 0.9760765550239234\n",
      "Processing 當事人不到場 ...\n",
      "Training accuracy is 0.9612389380530973\n",
      "Validation accuracy is 0.8771929824561403\n",
      "Processing 聲請人撤回 ...\n",
      "Training accuracy is 0.9651327433628318\n",
      "Validation accuracy is 0.9473684210526315\n",
      "K-fold cross validation Done!\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=FOLD_COUNT, shuffle=False)\n",
    "tfidf_models = []\n",
    "for i, (train_idx, test_idx) in enumerate(kfold.split(train_tfidf_features)):\n",
    "    print('## In fold {} ##'.format(i + 1))\n",
    "    predictions = OrderedDict()\n",
    "    predictions['id'] = train_df['id']\n",
    "    \n",
    "    models = []\n",
    "    for class_name in list_classes:\n",
    "        print('Processing {} ...'.format(class_name))\n",
    "        classifier = LogisticRegression(solver='sag', C=12.0)\n",
    "        \n",
    "        train_target = train_data[class_name].values[train_idx]\n",
    "        \n",
    "        classifier.fit(train_tfidf_features[train_idx], train_target)\n",
    "        y_pred = classifier.predict(train_tfidf_features[train_idx])\n",
    "        \n",
    "        print('Training accuracy is {}'.format(accuracy_score(y_pred, train_target)))\n",
    "        \n",
    "        val_target = train_data[class_name].values[test_idx]\n",
    "        val_pred = classifier.predict(train_tfidf_features[test_idx])\n",
    "        \n",
    "        print('Validation accuracy is {}'.format(accuracy_score(val_pred, val_target)))\n",
    "        predictions[class_name] = classifier.predict_proba(tfidf_features)[:, 1]\n",
    "        \n",
    "        models.append(classifier)\n",
    "        \n",
    "    tfidf_models.append(models)\n",
    "    train_predicts = pd.DataFrame.from_dict(predictions)\n",
    "    train_predicts.to_csv('results/Submission_file_{}.csv'.format(i), index=False, encoding='big5')\n",
    "    \n",
    "print('K-fold cross validation Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_models), len(tfidf_models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.6555     -0.17816487 -0.12829888 ... -0.01202514 -0.00456259\n",
      " -0.03817875]\n",
      "[-1.48510979 -1.11622148 -0.72304082 ...  0.4455174  -0.04212252\n",
      " -0.03370598]\n",
      "[ 2.99570985  1.7158505   0.90395635 ... -0.23230583  0.04514713\n",
      "  0.17993775]\n",
      "[-2.3502621  -0.65767856 -0.23264265 ... -0.26944685 -0.02704333\n",
      "  0.20204152]\n",
      "[-0.91609046 -0.34169973  0.06669629 ... -0.07329201 -0.01767867\n",
      " -0.33254976]\n"
     ]
    }
   ],
   "source": [
    "for i in tfidf_models[0]:\n",
    "    print(i.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## In fold 1 ##\n",
      "Processing 不受理 ...\n",
      "Training accuracy is 0.9964595503628961\n",
      "Validation accuracy is 0.9984076433121019\n",
      "Processing 不成立 ...\n",
      "Training accuracy is 0.7682775712515489\n",
      "Validation accuracy is 0.7197452229299363\n",
      "Processing 成立 ...\n",
      "Training accuracy is 0.8364312267657993\n",
      "Validation accuracy is 0.6369426751592356\n",
      "Processing 當事人不到場 ...\n",
      "Training accuracy is 0.8707735882457072\n",
      "Validation accuracy is 0.85828025477707\n",
      "Processing 聲請人撤回 ...\n",
      "Training accuracy is 0.9433528058063374\n",
      "Validation accuracy is 0.9570063694267515\n",
      "## In fold 2 ##\n",
      "Processing 不受理 ...\n",
      "Training accuracy is 0.9969906178084617\n",
      "Validation accuracy is 0.9936305732484076\n",
      "Processing 不成立 ...\n",
      "Training accuracy is 0.7663303239511418\n",
      "Validation accuracy is 0.7388535031847133\n",
      "Processing 成立 ...\n",
      "Training accuracy is 0.8474066206408214\n",
      "Validation accuracy is 0.6178343949044586\n",
      "Processing 當事人不到場 ...\n",
      "Training accuracy is 0.8727208355461143\n",
      "Validation accuracy is 0.8439490445859873\n",
      "Processing 聲請人撤回 ...\n",
      "Training accuracy is 0.9449460081430342\n",
      "Validation accuracy is 0.9426751592356688\n",
      "## In fold 3 ##\n",
      "Processing 不受理 ...\n",
      "Training accuracy is 0.9964595503628961\n",
      "Validation accuracy is 0.9984076433121019\n",
      "Processing 不成立 ...\n",
      "Training accuracy is 0.7649141440963002\n",
      "Validation accuracy is 0.732484076433121\n",
      "Processing 成立 ...\n",
      "Training accuracy is 0.8341299345016817\n",
      "Validation accuracy is 0.6592356687898089\n",
      "Processing 當事人不到場 ...\n",
      "Training accuracy is 0.867056116126748\n",
      "Validation accuracy is 0.8885350318471338\n",
      "Processing 聲請人撤回 ...\n",
      "Training accuracy is 0.9419366259514959\n",
      "Validation accuracy is 0.9697452229299363\n",
      "## In fold 4 ##\n",
      "Processing 不受理 ...\n",
      "Training accuracy is 0.9969906178084617\n",
      "Validation accuracy is 0.9936305732484076\n",
      "Processing 不成立 ...\n",
      "Training accuracy is 0.7640290316870243\n",
      "Validation accuracy is 0.7643312101910829\n",
      "Processing 成立 ...\n",
      "Training accuracy is 0.8265179677819083\n",
      "Validation accuracy is 0.697452229299363\n",
      "Processing 當事人不到場 ...\n",
      "Training accuracy is 0.8665250486811825\n",
      "Validation accuracy is 0.89171974522293\n",
      "Processing 聲請人撤回 ...\n",
      "Training accuracy is 0.9435298282881927\n",
      "Validation accuracy is 0.9554140127388535\n",
      "## In fold 5 ##\n",
      "Processing 不受理 ...\n",
      "Training accuracy is 0.9964595503628961\n",
      "Validation accuracy is 0.9984076433121019\n",
      "Processing 不成立 ...\n",
      "Training accuracy is 0.7666843689148521\n",
      "Validation accuracy is 0.7531847133757962\n",
      "Processing 成立 ...\n",
      "Training accuracy is 0.806337404850416\n",
      "Validation accuracy is 0.8121019108280255\n",
      "Processing 當事人不到場 ...\n",
      "Training accuracy is 0.8702425208001416\n",
      "Validation accuracy is 0.8535031847133758\n",
      "Processing 聲請人撤回 ...\n",
      "Training accuracy is 0.9470702779252965\n",
      "Validation accuracy is 0.9235668789808917\n",
      "## In fold 6 ##\n",
      "Processing 不受理 ...\n",
      "Training accuracy is 0.9964595503628961\n",
      "Validation accuracy is 0.9984076433121019\n",
      "Processing 不成立 ...\n",
      "Training accuracy is 0.7596034696406444\n",
      "Validation accuracy is 0.7818471337579618\n",
      "Processing 成立 ...\n",
      "Training accuracy is 0.799964595503629\n",
      "Validation accuracy is 0.9394904458598726\n",
      "Processing 當事人不到場 ...\n",
      "Training accuracy is 0.8713046556912728\n",
      "Validation accuracy is 0.8487261146496815\n",
      "Processing 聲請人撤回 ...\n",
      "Training accuracy is 0.9481324128164277\n",
      "Validation accuracy is 0.9140127388535032\n",
      "## In fold 7 ##\n",
      "Processing 不受理 ...\n",
      "Training accuracy is 0.9975216852540273\n",
      "Validation accuracy is 0.9888535031847133\n",
      "Processing 不成立 ...\n",
      "Training accuracy is 0.7617277394229067\n",
      "Validation accuracy is 0.7531847133757962\n",
      "Processing 成立 ...\n",
      "Training accuracy is 0.8033280226588777\n",
      "Validation accuracy is 0.9490445859872612\n",
      "Processing 當事人不到場 ...\n",
      "Training accuracy is 0.8716587006549832\n",
      "Validation accuracy is 0.8503184713375797\n",
      "Processing 聲請人撤回 ...\n",
      "Training accuracy is 0.9461851655160205\n",
      "Validation accuracy is 0.9299363057324841\n",
      "## In fold 8 ##\n",
      "Processing 不受理 ...\n",
      "Training accuracy is 0.9964601769911504\n",
      "Validation accuracy is 0.9984051036682615\n",
      "Processing 不成立 ...\n",
      "Training accuracy is 0.7582300884955753\n",
      "Validation accuracy is 0.7910685805422647\n",
      "Processing 成立 ...\n",
      "Training accuracy is 0.8028318584070796\n",
      "Validation accuracy is 0.9553429027113237\n",
      "Processing 當事人不到場 ...\n",
      "Training accuracy is 0.8692035398230088\n",
      "Validation accuracy is 0.861244019138756\n",
      "Processing 聲請人撤回 ...\n",
      "Training accuracy is 0.9438938053097345\n",
      "Validation accuracy is 0.9521531100478469\n",
      "## In fold 9 ##\n",
      "Processing 不受理 ...\n",
      "Training accuracy is 0.996283185840708\n",
      "Validation accuracy is 1.0\n",
      "Processing 不成立 ...\n",
      "Training accuracy is 0.7608849557522124\n",
      "Validation accuracy is 0.7687400318979266\n",
      "Processing 成立 ...\n",
      "Training accuracy is 0.8115044247787611\n",
      "Validation accuracy is 0.9011164274322169\n",
      "Processing 當事人不到場 ...\n",
      "Training accuracy is 0.8686725663716814\n",
      "Validation accuracy is 0.8803827751196173\n",
      "Processing 聲請人撤回 ...\n",
      "Training accuracy is 0.9442477876106194\n",
      "Validation accuracy is 0.94896331738437\n",
      "## In fold 10 ##\n",
      "Processing 不受理 ...\n",
      "Training accuracy is 0.9964601769911504\n",
      "Validation accuracy is 0.9984051036682615\n",
      "Processing 不成立 ...\n",
      "Training accuracy is 0.7591150442477876\n",
      "Validation accuracy is 0.7894736842105263\n",
      "Processing 成立 ...\n",
      "Training accuracy is 0.8058407079646017\n",
      "Validation accuracy is 0.8724082934609251\n",
      "Processing 當事人不到場 ...\n",
      "Training accuracy is 0.8654867256637168\n",
      "Validation accuracy is 0.8963317384370016\n",
      "Processing 聲請人撤回 ...\n",
      "Training accuracy is 0.943716814159292\n",
      "Validation accuracy is 0.9537480063795853\n",
      "K-fold cross validation Done!\n"
     ]
    }
   ],
   "source": [
    "labels_models = []\n",
    "for i, (train_idx, test_idx) in enumerate(kfold.split(train_feature_sets)):\n",
    "    print('## In fold {} ##'.format(i + 1))\n",
    "    predictions = OrderedDict()\n",
    "    predictions['id'] = train_df['id']\n",
    "    \n",
    "    models = []\n",
    "    for class_name in list_classes:\n",
    "        print('Processing {} ...'.format(class_name))\n",
    "        classifier = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, \n",
    "                   max_iter=100, multi_class='ovr', penalty='l2', random_state=None, solver='liblinear', tol=0.0001,verbose=0)\n",
    "        \n",
    "        train_target = train_data[class_name].values[train_idx]\n",
    "        \n",
    "        classifier.fit(train_feature_sets[train_idx], train_target)\n",
    "        y_pred = classifier.predict(train_feature_sets[train_idx])\n",
    "        \n",
    "        print('Training accuracy is {}'.format(accuracy_score(y_pred, train_target)))\n",
    "        \n",
    "        val_target = train_data[class_name].values[test_idx]\n",
    "        val_pred = classifier.predict(train_feature_sets[test_idx])\n",
    "        \n",
    "        print('Validation accuracy is {}'.format(accuracy_score(val_pred, val_target)))\n",
    "        predictions[class_name] = classifier.predict_proba(feature_sets)[:, 1]\n",
    "        \n",
    "        models.append(classifier)\n",
    "        \n",
    "    labels_models.append(models)\n",
    "    train_predicts = pd.DataFrame.from_dict(predictions)\n",
    "    train_predicts.to_csv('results/Submission_file_{}.csv'.format(i + 10), index=False, encoding='big5')\n",
    "print('K-fold cross validation Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_models), len(labels_models[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOB (Out-of-Bag) Evaluation (Error ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicting training results...')\n",
    "\n",
    "for i, model in enumerate(tfidf_models):\n",
    "    predictions = OrderedDict()\n",
    "    predictions['id'] = train_df['id']\n",
    "    for class_name in list_classes:\n",
    "        predictions[class_name] = model.predict_proba(train_tfidf_features)[:, 1]\n",
    "    \n",
    "    train_predicts = pd.DataFrame.from_dict(predictions)\n",
    "    train_predicts.to_csv('results/Submission_file_{}.csv'.format(i), index=False, encoding='big5')\n",
    "    \n",
    "for i, model in enumerate(labels_models):\n",
    "    predictions = OrderedDict()\n",
    "    predictions['id'] = train_df['id']\n",
    "    for class_name in list_classes:\n",
    "        predictions[class_name] = model.predict_proba(train_feature_sets)[:, 1]\n",
    "        \n",
    "    train_predicts = pd.DataFrame.from_dict(predictions)\n",
    "    train_predicts.to_csv('results/Submission_file_{}.csv'.format(i + 10), index=False, encoding='big5')\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing ensemble on\n",
      "results/Submission_file_0.csv\n",
      "results/Submission_file_1.csv\n",
      "results/Submission_file_2.csv\n",
      "results/Submission_file_3.csv\n",
      "results/Submission_file_4.csv\n",
      "results/Submission_file_5.csv\n",
      "results/Submission_file_6.csv\n",
      "results/Submission_file_7.csv\n",
      "results/Submission_file_8.csv\n",
      "results/Submission_file_9.csv\n",
      "results/Submission_file_10.csv\n",
      "results/Submission_file_11.csv\n",
      "results/Submission_file_12.csv\n",
      "results/Submission_file_13.csv\n",
      "results/Submission_file_14.csv\n",
      "results/Submission_file_15.csv\n",
      "results/Submission_file_16.csv\n",
      "results/Submission_file_17.csv\n",
      "results/Submission_file_18.csv\n",
      "results/Submission_file_19.csv\n",
      "Bagging operation Done!\n"
     ]
    }
   ],
   "source": [
    "result_list = ['results/Submission_file_{}.csv'.format(i) for i in range(0, FOLD_COUNT * 2)]\n",
    "bagging(result_list, 'results/bagging.csv')\n",
    "print('Bagging operation Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('results/bagging.csv', encoding='big5')\n",
    "result_label = test_df[list_classes]\n",
    "results = result_label.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_target = train_df['是否成立']\n",
    "# for i in range(20):\n",
    "#     print(results[i], '\\t', val_target[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is 0.8631511868727099\n"
     ]
    }
   ],
   "source": [
    "print('Validation accuracy is {}'.format(accuracy_score(results, val_target)))\n",
    "# print('Validation AUC is {}'.format(roc_auc_score(results, val_target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '成立', 1: '不成立', 2: '當事人不到場', 3: '聲請人撤回', 4: '不受理'}\n"
     ]
    }
   ],
   "source": [
    "label_dict = {'成立': 0, '不成立': 1, '當事人不到場': 2, '聲請人撤回': 3, '不受理':4}\n",
    "reverse_dict = {idx: word for word, idx in label_dict.items()}\n",
    "print(reverse_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成立        2518\n",
      "不成立       1049\n",
      "當事人不到場     566\n",
      "聲請人撤回      245\n",
      "不受理         15\n",
      "Name: 是否成立, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data['是否成立'].value_counts())\n",
    "train_label = [label_dict.get(label, 1) for label in train_data['是否成立'].values]\n",
    "train_label = np.array(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## In fold 1 ##\n",
      "Training accuracy is 0.9433198380566802\n",
      "Validation accuracy is 0.7142857142857143\n",
      "## In fold 2 ##\n",
      "Training accuracy is 0.944078947368421\n",
      "Validation accuracy is 0.6870748299319728\n",
      "## In fold 3 ##\n",
      "Training accuracy is 0.9483805668016194\n",
      "Validation accuracy is 0.6689342403628118\n",
      "## In fold 4 ##\n",
      "Training accuracy is 0.9455971659919028\n",
      "Validation accuracy is 0.6530612244897959\n",
      "## In fold 5 ##\n",
      "Training accuracy is 0.9458502024291497\n",
      "Validation accuracy is 0.671201814058957\n",
      "## In fold 6 ##\n",
      "Training accuracy is 0.9456246838644411\n",
      "Validation accuracy is 0.7084282460136674\n",
      "## In fold 7 ##\n",
      "Training accuracy is 0.9433628318584071\n",
      "Validation accuracy is 0.6872146118721462\n",
      "## In fold 8 ##\n",
      "Training accuracy is 0.9451327433628318\n",
      "Validation accuracy is 0.6894977168949772\n",
      "## In fold 9 ##\n",
      "Training accuracy is 0.9459049544994944\n",
      "Validation accuracy is 0.6956521739130435\n",
      "## In fold 10 ##\n",
      "Training accuracy is 0.9441496082891079\n",
      "Validation accuracy is 0.7018348623853211\n",
      "K-fold cross validation Done!\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=FOLD_COUNT, shuffle=False)\n",
    "multi_classifier_tfidf_models = []\n",
    "for i, (train_idx, test_idx) in enumerate(kfold.split(train_tfidf_features, train_label)):\n",
    "    print('## In fold {} ##'.format(i + 1))\n",
    "    classifier = LogisticRegression(solver='sag', C=12.0)\n",
    "    predictions = OrderedDict()\n",
    "    predictions['id'] = train_df['id']\n",
    "    \n",
    "    train_target = train_label[train_idx]\n",
    "    classifier.fit(train_tfidf_features[train_idx], train_target)\n",
    "    y_pred = classifier.predict(train_tfidf_features[train_idx])\n",
    "\n",
    "    print('Training accuracy is {}'.format(accuracy_score(y_pred, train_target)))\n",
    "\n",
    "    val_target = train_label[test_idx]\n",
    "    val_pred = classifier.predict(train_tfidf_features[test_idx])\n",
    "\n",
    "    print('Validation accuracy is {}'.format(accuracy_score(val_pred, val_target)))\n",
    "        \n",
    "    multi_classifier_tfidf_models.append(classifier)\n",
    "    predictions['target'] = [reverse_dict[idx] for idx in classifier.predict(tfidf_features)]\n",
    "        \n",
    "    tfidf_models.append(classifier)\n",
    "    train_predicts = pd.DataFrame.from_dict(predictions)\n",
    "    train_predicts.to_csv('results/multi/Submission_file_{}.csv'.format(i), index=False, encoding='big5')\n",
    "    \n",
    "print('K-fold cross validation Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## In fold 1 ##\n",
      "Training accuracy is 0.6826923076923077\n",
      "Validation accuracy is 0.6984126984126984\n",
      "## In fold 2 ##\n",
      "Training accuracy is 0.6864878542510121\n",
      "Validation accuracy is 0.6757369614512472\n",
      "## In fold 3 ##\n",
      "Training accuracy is 0.6826923076923077\n",
      "Validation accuracy is 0.673469387755102\n",
      "## In fold 4 ##\n",
      "Training accuracy is 0.6895242914979757\n",
      "Validation accuracy is 0.655328798185941\n",
      "## In fold 5 ##\n",
      "Training accuracy is 0.680414979757085\n",
      "Validation accuracy is 0.7097505668934241\n",
      "## In fold 6 ##\n",
      "Training accuracy is 0.6853818917551846\n",
      "Validation accuracy is 0.662870159453303\n",
      "## In fold 7 ##\n",
      "Training accuracy is 0.6844500632111251\n",
      "Validation accuracy is 0.6757990867579908\n",
      "## In fold 8 ##\n",
      "Training accuracy is 0.6902654867256637\n",
      "Validation accuracy is 0.6666666666666666\n",
      "## In fold 9 ##\n",
      "Training accuracy is 0.6830131445904954\n",
      "Validation accuracy is 0.700228832951945\n",
      "## In fold 10 ##\n",
      "Training accuracy is 0.6863785696234521\n",
      "Validation accuracy is 0.6903669724770642\n",
      "K-fold cross validation Done!\n"
     ]
    }
   ],
   "source": [
    "for i, (train_idx, test_idx) in enumerate(kfold.split(train_feature_sets, train_label)):\n",
    "    print('## In fold {} ##'.format(i + 1))\n",
    "    classifier = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, \n",
    "                   max_iter=100, multi_class='ovr', penalty='l2', random_state=None, solver='liblinear', tol=0.0001,verbose=0)\n",
    "    predictions = OrderedDict()\n",
    "    predictions['id'] = train_df['id']\n",
    "    \n",
    "    train_target = train_label[train_idx]\n",
    "\n",
    "    classifier.fit(train_feature_sets[train_idx], train_target)\n",
    "    y_pred = classifier.predict(train_feature_sets[train_idx])\n",
    "\n",
    "    print('Training accuracy is {}'.format(accuracy_score(y_pred, train_target)))\n",
    "\n",
    "    val_target = train_label[test_idx]\n",
    "    val_pred = classifier.predict(train_feature_sets[test_idx])\n",
    "\n",
    "    print('Validation accuracy is {}'.format(accuracy_score(val_pred, val_target)))\n",
    "        \n",
    "    multi_classifier_tfidf_models.append(classifier)\n",
    "    predictions['target'] = [reverse_dict[idx] for idx in classifier.predict(feature_sets)]\n",
    "        \n",
    "    tfidf_models.append(classifier)\n",
    "    train_predicts = pd.DataFrame.from_dict(predictions)\n",
    "    train_predicts.to_csv('results/multi/Submission_file_{}.csv'.format(i + 10), index=False, encoding='big5')\n",
    "    \n",
    "print('K-fold cross validation Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "et_predictions = OrderedDict()\n",
    "et_predictions['id'] = test_df['id']\n",
    "\n",
    "for class_name in list_classes:\n",
    "    train_target = train_df[class_name]\n",
    "    classifier = ExtraTreesClassifier(n_estimators=30)\n",
    "    \n",
    "    cv_score = np.mean(cross_val_score(classifier, train_tfidf_features, train_target, cv=10, scoring='roc_auc'))\n",
    "    accs.append(cv_score)\n",
    "    print('CV Score for class {} is {}'.format(class_name, cv_score))\n",
    "    \n",
    "    classifier.fit(train_tfidf_features, train_target)\n",
    "    et_predictions[class_name] = classifier.predict_proba(test_tfidf_features)[:, 1]\n",
    "    \n",
    "submission = pd.DataFrame.from_dict(et_predictions)\n",
    "submission.to_csv('result/LR_Based/ExtraTreesClassifier_Submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(tfidf_models):\n",
    "    print('## In Model {} ##'.format(i + 1))\n",
    "    predictions = OrderedDict()\n",
    "    predictions['id'] = test_df['id']\n",
    "    \n",
    "    for class_name in list_classes:\n",
    "        predictions[class_name] = model.predict_proba(test_tfidf_features)[:, 1]\n",
    "        print('Predict the proba for {} Done!'.format(class_name))\n",
    "        print(predictions.keys())\n",
    "    \n",
    "    print(predictions.keys())\n",
    "    submission = pd.DataFrame.from_dict(predictions)\n",
    "    submission.to_csv('Logistic_Regression_Submission_{}.csv'.format(i), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging(arrs, path):\n",
    "    print(\"Doing ensemble on\")\n",
    "    subs = []\n",
    "    for arr in arrs:\n",
    "        print(arr)\n",
    "        subs.append(pd.read_csv(arr, encoding='big5'))\n",
    "    \n",
    "    for sub in subs[1:]:\n",
    "        for c in list_classes:\n",
    "            subs[0][c] += sub[c]\n",
    "    \n",
    "    for c in list_classes:\n",
    "        subs[0][c] /= len(subs)\n",
    "        \n",
    "    subs[0].to_csv(path, index=False, encoding='big5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
