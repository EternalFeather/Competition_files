{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re, os, math, random\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "from tqdm import tqdm\n",
    "import jieba\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from nltk.stem import SnowballStemmer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Embedding, Bidirectional, CuDNNGRU, CuDNNLSTM, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import tensor_array_ops, control_flow_ops\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.enable_parallel(8)\n",
    "train_corpus = \"train_first.csv\"\n",
    "test_corpus = \"predict_first.csv\"\n",
    "vocab_path = \"Chinese_corpus_dict.vocab\"\n",
    "clean_stop_word = False\n",
    "vocab_size = 20000\n",
    "embedding_dims = 300\n",
    "seq_length = 100\n",
    "pointer = 0\n",
    "\n",
    "#####################\n",
    "## Text RNN\n",
    "rnn_recurrent_units = 48\n",
    "rnn_dense_units = 32\n",
    "rnn_output_units = 5\n",
    "rnn_batch_size = 256\n",
    "#####################\n",
    "\n",
    "#####################\n",
    "## Text CNN\n",
    "cnn_filter_nums = 120\n",
    "cnn_dense_units = 72\n",
    "cnn_output_units = 5\n",
    "cnn_batch_size = 256\n",
    "#####################\n",
    "\n",
    "#####################\n",
    "## Hybrid Text NN\n",
    "hy_recurrent_units = 48\n",
    "hy_filter_nums = 64\n",
    "hy_output_units = 5\n",
    "hy_batch_size = 256\n",
    "#####################\n",
    "\n",
    "#####################\n",
    "## Seq2seq\n",
    "seq_batch_size = 50\n",
    "enc_hidden_size = 150\n",
    "dec_hidden_size = 300\n",
    "seq_learning_rate = 0.01\n",
    "start_word = 2    # token number of <SOS>\n",
    "train_epochs = 10\n",
    "dropout_keep_prob = 0.75\n",
    "l2_reg_lambda = 0.2\n",
    "seq_label_size = 5\n",
    "#####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_corpus)\n",
    "test_df = pd.read_csv(test_corpus)\n",
    "\n",
    "train_df.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = pd.unique(train_df['Score'])\n",
    "print(label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data length\n",
    "print(\"Total rows in train_corpus is {}\".format(len(train_df)))\n",
    "print(\"Total rows in test_corpus is {}\".format(len(test_df)))\n",
    "print(train_df['Score'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence length analysis\n",
    "train_df['Word_Length'] = train_df['Discuss'].apply(lambda x: len(str(x)))\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_df['Word_Length'].value_counts().head())\n",
    "\n",
    "# For test_data\n",
    "test_df['Word_Length'] = test_df['Discuss'].apply(lambda x: len(str(x)))\n",
    "# print(test_df['Word_Length'].value_counts().head())\n",
    "print(train_df['Word_Length'].describe(), \"\\n\", test_df['Word_Length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram graph\n",
    "sns.set()\n",
    "train_df['Word_Length'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(test_df['Word_Length'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sort_values('Word_Length', ascending=False).head(100)['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to one-hot\n",
    "Score = train_df['Score']\n",
    "data = pd.get_dummies(Score)\n",
    "train_df = pd.concat([train_df, data], axis=1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get positive labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_df.iloc[:, 4:].sum()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "ax = sns.barplot(x.index, x.values, alpha=0.8)\n",
    "plt.title(\"Summary\")\n",
    "plt.ylabel('Occurrences', fontsize=12)\n",
    "plt.xlabel('Score', fontsize=12)\n",
    "\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width() / 2, height + 5, ha='center', va='bottom', s='{:.1f}'.format(abs(label)))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No sentences contains up to 2 Score-labels\n",
    "colormap = plt.cm.plasma\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('Correlation of features', y=1.05, size=14)\n",
    "sns.heatmap(data.astype(float).corr(), linewidths=0.1, vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, stem_words=False):\n",
    "    text = re.sub(r',', '，', text)\n",
    "    text = re.sub(r'\\.+', '...', text)\n",
    "    text = re.sub(r'\\.{6}', '...', text)\n",
    "    text = re.sub(r'…', '...', text)\n",
    "    text = re.sub(r';', '；', text)\n",
    "    text = re.sub(r'°', '。', text)\n",
    "    text = re.sub(r'】', ']', text)\n",
    "    text = re.sub(r'【', '[', text)\n",
    "    text = re.sub(r'\\)', '\\）', text)\n",
    "    text = re.sub(r'\\(', '\\（', text)\n",
    "    text = re.sub(r'“', '\"', text)\n",
    "    text = re.sub(r' ', '', text)\n",
    "    text = re.sub(r'”', '\"', text)\n",
    "    text = re.sub(r'～', '~', text)\n",
    "    text = re.sub(r'·', '。', text)\n",
    "    text = re.sub(r'!', '！', text)\n",
    "    text = re.sub(r'—', '-', text)\n",
    "    text = re.sub(r'》', '\\）', text)\n",
    "    text = re.sub(r'《', '\\（', text)\n",
    "    text = re.sub(r'\\?', '\\？', text)\n",
    "    text = re.sub(r'。。。', '...', text)\n",
    "    text = re.sub(r'。。。。。。', '...', text)\n",
    "    text = re.sub(r':', '：', text)\n",
    "    \n",
    "#     # for English_sentence\n",
    "#     text = replace_numbers.sub('', text)\n",
    "#     text = special_character_removal.sub('', text)\n",
    "#     if stem_words:\n",
    "#         text = text.split()\n",
    "#         stemmer = SnowballStemmer('english')\n",
    "#         stemmed_words = [stemmer.stem(word) for word in text]\n",
    "#         text = \" \".join(stemmed_words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal = re.compile(r'[^a-z\\d ]', re.IGNORECASE)\n",
    "\n",
    "# regex to remove all Alpha Numeric and space\n",
    "special_alpha_removal = re.compile(r'[a-z\\d ]', re.IGNORECASE)\n",
    "\n",
    "# regex to replace all numeric\n",
    "replace_numbers = re.compile(r'\\d+', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check NaN\n",
    "print(np.any(train_df.isnull()) == True, np.any(test_df.isnull()) == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular dictionary\n",
    "print(\"MSG : Processing text datasets...\")\n",
    "cols_target = [1, 2, 3, 4, 5]\n",
    "\n",
    "list_sentence_train = train_df['Discuss'].fillna(\"no discuss\").values\n",
    "train_discusses = [clean_text(text) for text in list_sentence_train]\n",
    "train_df['Discuss'] = pd.Series(train_discusses).astype(str)\n",
    "\n",
    "list_sentence_test = test_df['Discuss'].fillna(\"no discuss\").values\n",
    "test_discusses = [clean_text(text) for text in list_sentence_test]\n",
    "test_df['Discuss'] = pd.Series(test_discusses).astype(str)\n",
    "\n",
    "# cleaned_train_discuss = []\n",
    "# for i in range(len(train_df)):\n",
    "#     cleaned_discuss = clean_text(train_df['Discuss'][i])\n",
    "#     cleaned_train_discuss.append(cleaned_discuss)\n",
    "# train_df['Discuss'] = pd.Series(cleaned_train_discuss).astype(str)\n",
    "\n",
    "# cleaned_test_discuss = []\n",
    "# for i in range(len(test_df)):\n",
    "#     cleaned_discuss = clean_text(test_df['Discuss'][i])\n",
    "#     cleaned_test_discuss.append(cleaned_discuss)\n",
    "# test_df['Discuss'] = pd.Series(cleaned_test_discuss).astype(str)\n",
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the sentence text & Word segmentation analysis\n",
    "word_dict = defaultdict(int)\n",
    "\n",
    "for sentence in tqdm(train_df['Discuss']):\n",
    "    seg_list = jieba.cut(sentence, cut_all=False)\n",
    "    for word in seg_list:\n",
    "        word_dict[word] += 1\n",
    "word_dict = sorted(word_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(len(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_sentences = []\n",
    "for sentence in tqdm(train_df['Discuss']):\n",
    "    seg_list = jieba.cut(str(sentence), cut_all=False)\n",
    "    cut_sentences.append(\" \".join(seg_list))\n",
    "train_df['Discuss_cut'] = cut_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_test_sentences = []\n",
    "for sentence in tqdm(test_df['Discuss']):\n",
    "    seg_list = jieba.cut(str(sentence), cut_all=False)\n",
    "    cut_test_sentences.append(\" \".join(seg_list))\n",
    "test_df['Discuss_cut'] = cut_test_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cut = train_df['Discuss_cut'].values\n",
    "test_data_cut = test_df['Discuss_cut'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick view of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_list, sign_list, dig_english_list = [], [], []\n",
    "for word, count in word_dict:\n",
    "    for char in word:\n",
    "        if char >= u'\\u4E00' and char <= u\"\\u9FA5\":\n",
    "            chinese_list.append((word, count))\n",
    "        elif (char >= u'\\u0041' and char <= u'\\u005A') or (char >= u'\\u0061' and char <= u'\\u007A') or (char >= u'\\u0030' and char <= u'\\u0039'):\n",
    "            dig_english_list.append((word, count))\n",
    "            break\n",
    "        else:\n",
    "            sign_list.append((word, count))\n",
    "            break\n",
    "sorted_dig_english_list = sorted(set(dig_english_list), key=lambda x: x[1], reverse=True)\n",
    "sorted_sign_list = sorted(set(sign_list), key=lambda x: x[1], reverse=True)\n",
    "sorted_chinese_list = sorted(set(chinese_list), key=lambda x: x[1], reverse=True)\n",
    "print(\"chinese_word: \", len(sorted_chinese_list))\n",
    "print(\"dig_english_word: \", len(sorted_dig_english_list))\n",
    "print(\"sign_count: \", len(sorted_sign_list))\n",
    "print(sorted_chinese_list[:10000], '\\n\\n', sorted_dig_english_list[:50], '\\n\\n', sorted_sign_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stop words\n",
    "chinese_stop_words = [word[0] for word in sorted_chinese_list[vocab_size - 24:]]\n",
    "english_stop_words = [word[0] for word in sorted_dig_english_list]\n",
    "sign_stop_words = [word[0] for word in sorted_sign_list[20:]]\n",
    "stop_words = chinese_stop_words + english_stop_words + sign_stop_words\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop('Word_Length', axis=1)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df.Discuss.values\n",
    "train_label = train_df['Score'].values\n",
    "test_data = test_df.Discuss.values\n",
    "print(train_data.shape, test_data.shape, train_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample balance datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_train_df = train_df.reindex(np.random.permutation(train_df.index))\n",
    "shuffled_train_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_number = 300\n",
    "balance_train_data = []\n",
    "balance_train_label = []\n",
    "val_number = 200\n",
    "balance_validate_data = []\n",
    "balance_validate_label = []\n",
    "\n",
    "for i in cols_target:\n",
    "    candidate = shuffled_train_df[shuffled_train_df['Score'] == i]['Discuss'].values[:sample_number]\n",
    "    balance_train_data += list(candidate)\n",
    "    balance_train_label += [i for _ in range(sample_number)]\n",
    "    candidate = shuffled_train_df[shuffled_train_df['Score'] == i]['Discuss'].values[sample_number:val_number + sample_number]\n",
    "    balance_validate_data += list(candidate)\n",
    "    balance_validate_label += [i for _ in range(val_number)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(balance_train_data[::300])\n",
    "print(balance_train_label[::300])\n",
    "print(balance_validate_data[::200])\n",
    "print(balance_validate_label[::200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_indices = np.random.permutation(np.arange(len(balance_train_data)))\n",
    "balance_train_data = np.array(balance_train_data)[shuffle_indices]\n",
    "balance_train_label = np.array(balance_train_label)[shuffle_indices]\n",
    "print(balance_train_data.shape, balance_train_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build own vocab & data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(vocab_path, data):\n",
    "    files = open(vocab_path, 'w', encoding='utf-8')\n",
    "    files.write(\"{}\\t1000000000\\n{}\\t1000000000\\n{}\\t1000000000\\n{}\\t1000000000\\n\".format(\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"))\n",
    "    for word, count in data:\n",
    "        files.write(\"{}\\t{}\\n\".format(word, count))\n",
    "        \n",
    "def mini_batch(vocab_path, data, padding):\n",
    "    token_seqs, sentences = load_datasets(vocab_path, data, padding)\n",
    "    num_batch = int(len(sentences) / seq_batch_size)\n",
    "    token_seqs = token_seqs[:num_batch * seq_batch_size]\n",
    "    sentences = sentences[:num_batch * seq_batch_size]\n",
    "    token_batch = np.split(np.array(token_seqs), num_batch, 0)\n",
    "    sentence_batch = np.split(np.array(sentences), num_batch, 0)\n",
    "    return token_batch, sentence_batch, num_batch\n",
    "    \n",
    "def load_datasets(vocab_path, data, padding=False):\n",
    "    sentences = [line for line in data if line]\n",
    "    word2idx, idx2word = load_vocab(vocab_path)\n",
    "    \n",
    "    token_list, sources = [], []\n",
    "    for source in sentences:\n",
    "        temp_seg = jieba.cut(source, cut_all=False)\n",
    "        seg_list = [i for i in temp_seg]\n",
    "        x = [word2idx.get(word, 1) for word in (\" \".join(str(i) for i in seg_list) + ' <EOS>').split()]\n",
    "        if padding:\n",
    "            if len(x) < seq_length:\n",
    "                x += [0 for _ in range(seq_length - len(x))]\n",
    "            else:\n",
    "                x = x[:100]\n",
    "        token_list.append(x)\n",
    "        sources.append(source)\n",
    "    return token_list, sources\n",
    "    \n",
    "def load_vocab(vocab_path):\n",
    "    vocab = [line.split()[0] for line in open(vocab_path, 'r', encoding='utf-8').read().splitlines()]\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {word2idx[word]: word for word in word2idx}\n",
    "    return word2idx, idx2word\n",
    "    \n",
    "def next_batch(token_batches, sentence_batches, pointer, num_batch):\n",
    "    result = token_batches[pointer]\n",
    "    sentence = sentence_batches[pointer]\n",
    "    pointer = (pointer + 1) % num_batch\n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "data_series = sorted(sorted_chinese_list[:9976] + sorted_sign_list[:20], key=lambda x: x[1], reverse=True)\n",
    "build_vocab(vocab_path, data_series)\n",
    "print(\"MSG : Finished building vocab file.\")\n",
    "\n",
    "# data tokenize & loaded by mini_batch\n",
    "token_sequences, sentences, num_batch = mini_batch(vocab_path, train_data, True)\n",
    "unpad_token_sequences, unpad_sentences, _ = mini_batch(vocab_path, train_data, False)\n",
    "print(\"MSG : Finished initialize batch datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx, idx2word = load_vocab(vocab_path)\n",
    "sample_indice = random.randint(0, num_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training set without padded index format: \")\n",
    "print(unpad_token_sequences[sample_indice][:3], '\\n batch_size is: ', len(unpad_token_sequences[sample_indice]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training set with padded(by zero) index format: \")\n",
    "print(token_sequences[sample_indice][:3], '\\n batch_size is: ', len(token_sequences[sample_indice][1]))\n",
    "tokens = token_sequences[sample_indice][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training set with padded(by <PAD>) word format: \")\n",
    "print(sentences[sample_indice][:3], '\\n batch_size is: ', len(sentences[sample_indice]))\n",
    "print(\"trainslated by tokens: \", [idx2word.get(word, 1) for sent in tokens for word in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_train_sequences, balance_train_sentences, balance_num_batch = mini_batch(vocab_path, balance_train_data, True)\n",
    "print(\"MSG : Finished initialize batch datasets for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token_sequences, test_sentences, test_num_batch = mini_batch(vocab_path, test_data, True)\n",
    "print(\"MSG : Finished initialize batch datasets for prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indice_test = random.randint(0, test_num_batch)\n",
    "print(sample_indice_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training set without padded index format: \")\n",
    "print(test_token_sequences[sample_indice_test][:3], '\\n batch_size is: ', len(test_token_sequences[sample_indice_test]))\n",
    "test_tokens = test_token_sequences[sample_indice_test][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training set with padded(by <PAD>) word format: \")\n",
    "print(test_sentences[sample_indice_test][:3], '\\n batch_size is: ', len(test_sentences[sample_indice_test]))\n",
    "print(\"trainslated by tokens: \", [idx2word.get(word, 1) for sent in test_tokens for word in sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without batch_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token_sequences = load_datasets(vocab_path, test_data, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocabulary by Keras tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove alpha numeric & space\n",
    "list_sentence_train = train_df['Discuss'].values\n",
    "train_discusses = [special_alpha_removal.sub('', text) for text in list_sentence_train]\n",
    "train_df['Discuss'] = pd.Series(train_discusses).astype(str)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentence_test = test_df['Discuss'].values\n",
    "test_discusses = [special_alpha_removal.sub('', text) for text in list_sentence_test]\n",
    "test_df['Discuss'] = pd.Series(test_discusses).astype(str)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_discusses_cut, test_discusses_cut = [], []\n",
    "\n",
    "for sentence in tqdm(train_discusses):\n",
    "    seg_list = jieba.cut(sentence, cut_all=False)\n",
    "    train_discusses_cut.append(\" \".join(seg_list))\n",
    "print(\"MSG : Segmentation for train_discusses is Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_discusses_cut[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in tqdm(test_discusses):\n",
    "    seg_list = jieba.cut(sentence, cut_all=False)\n",
    "    test_discusses_cut.append(\" \".join(seg_list))\n",
    "print(\"MSG : Segmentation for test_discusses is Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer which transform a sentence to a list of ids\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "\n",
    "# build the relation between words and ids\n",
    "tokenizer.fit_on_texts(train_discusses_cut + test_discusses_cut)\n",
    "\n",
    "# transform training/testing sentences to training/testing sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_discusses_cut)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_discusses_cut)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Found {} unique tokens\".format(len(word_index)))\n",
    "train_data_seq = pad_sequences(train_sequences, maxlen=seq_length)\n",
    "print(\"Shape of train data tensor: {}\".format(train_data_seq.shape))\n",
    "print(\"Shape of label tensor: {}\".format(train_label.shape))\n",
    "\n",
    "test_data_seq = pad_sequences(test_sequences, maxlen=seq_length)\n",
    "print(\"Shape of test data tensor: {}\".format(test_data_seq.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training set with word format:\")\n",
    "print(train_discusses[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training set with index format:\")\n",
    "print(train_sequences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training set with padded(by zero) index format:\")\n",
    "print(train_data_seq[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_df[cols_target].values\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic_Regression Model (sklearn version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Muilti-classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judgement_score(mod, obs, n):\n",
    "    summation = 0\n",
    "    for i, j in zip(mod, obs):\n",
    "        summation += math.pow(j - i, 2)\n",
    "    RMSE = math.sqrt(summation / n)\n",
    "    return 1 / (1 + RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic_Regression_Binary_Relevance\n",
    "logreg = LogisticRegression(C=12.0)\n",
    "vect = TfidfVectorizer(max_features=200000, stop_words=None)\n",
    "vect_cleaned = TfidfVectorizer(max_features=200000, stop_words=stop_words)\n",
    "\n",
    "# 10-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=False)\n",
    "\n",
    "# learn the vocabulary in the training data, then use it to create a document-term matrix\n",
    "for i, (train, test) in enumerate(kfold.split(train_data_cut, train_label)):\n",
    "    if not clean_stop_word:\n",
    "        dtm = vect.fit_transform(train_data_cut[train])\n",
    "    else:\n",
    "        dtm_cleaned = vect_cleaned.fit_transform(train_data_cut[train])\n",
    "    \n",
    "    # training\n",
    "    print(\"Processing {}-fold ...\".format(i + 1))\n",
    "    logreg.fit(dtm[train], train_label[train])\n",
    "    train_y_pred = logreg.predict(dtm[train])\n",
    "    print(\"Training accuracy is {}\".format(accuracy_score(train_label[train], train_y_pred)))\n",
    "    \n",
    "    # validation\n",
    "    validate_y_pred = logreg.predict(dtm[test])\n",
    "    print(\"Validating accuracy is {}\".format(accuracy_score(train_label[test], validate_y_pred)))\n",
    "    \n",
    "    # Judgement Score\n",
    "    print(\"Training Score is {}\\n Validating Score is {}\\n\".format(judgement_score(train_y_pred, train_label[train], len(train)), judgement_score(validate_y_pred, train_label[test], len(test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training for all corpus\n",
    "train_dtm = vect.fit_transform(train_data_cut)\n",
    "train_dtm_cleaned = vect_cleaned.fit_transform(train_data_cut)\n",
    "\n",
    "LR_model = LogisticRegression(C=12.0)\n",
    "LR_model.fit(train_dtm, train_label)\n",
    "train_y_pred = LR_model.predict(train_dtm)\n",
    "print(\"Training accuracy is {}\".format(accuracy_score(train_label, train_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "test_dtm = vect.transform(test_data_cut)\n",
    "test_dtm_cleaned = vect_cleaned.transform(test_data_cut)\n",
    "\n",
    "submission_binary = pd.read_csv('predict_first.csv')\n",
    "test_y_pred = LR_model.predict(test_dtm)\n",
    "submission_binary['label'] = test_y_pred\n",
    "print([len(submission_binary[submission_binary['label'] == i]) for i in range(1, 6)])\n",
    "submission_binary = submission_binary.drop('Discuss', axis=1)\n",
    "submission_binary.to_csv(\"output/LogisticRegression/evaluation_public_tfidf_200000.csv\", index=False, header=False)\n",
    "print(\"MSG : Finished generate submission file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_binary = pd.read_csv('predict_first.csv')\n",
    "\n",
    "model = LogisticRegression(C=12.0)\n",
    "for label in cols_target:\n",
    "    print(\"Processing {}\".format(label))\n",
    "    y = train_df[label]\n",
    "    model.fit(train_dtm, y)\n",
    "    y_pred = model.predict(train_dtm)\n",
    "    print(\"Training accuracy is {}\".format(accuracy_score(y, y_pred)))\n",
    "    test_y_prob = model.predict_proba(test_dtm)[:, 1]\n",
    "    submission_binary[label] = test_y_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(feature_sets, features):\n",
    "    return hstack([feature_sets, csr_matrix(features).T], 'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training with classifier chains\n",
    "submission_chains = pd.read_csv('predict_first.csv')\n",
    "\n",
    "for label in cols_target:\n",
    "    print(\"MSG : Processing {}...\".format(label))\n",
    "    y = train_df[label]\n",
    "    model.fit(train_dtm, y)\n",
    "    y_pred = model.predict(train_dtm)\n",
    "    print(\"Training accuracy is {}\".format(accuracy_score(y, y_pred)))\n",
    "    test_y = model.predict(test_dtm)\n",
    "    test_y_prob = model.predict_proba(test_dtm)[:, 1]\n",
    "    submission_chains[label] = test_y_prob\n",
    "    train_dtm = add_features(train_dtm, y)\n",
    "    print(\"Shape of train_dtm is now {}\".format(train_dtm.shape))\n",
    "    test_dtm = add_features(test_dtm, test_y)\n",
    "    print(\"Shape of test_dtm is now {}\".format(test_dtm.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_combined = pd.read_csv('predict_first.csv')\n",
    "for label in cols_target:\n",
    "    submission_combined[label] = 0.5 * (submission_chains[label] + submission_binary[label])\n",
    "submission_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_combined = submission_combined.drop('Discuss', axis=1)\n",
    "submission_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose max_index\n",
    "def check_argmax(table, start_col):\n",
    "    values = table.values[:, start_col:]\n",
    "    labels = np.argmax(values, axis=1)\n",
    "    return [i + 1 for i in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_new = check_argmax(submission_combined, 1)\n",
    "scores = pd.Series(labels_new, name='Score').astype(int)\n",
    "submission_combined = pd.concat([submission_combined, scores], axis=1)\n",
    "print([len(submission_combined[submission_combined['Score'] == i]) for i in range(1, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_combined = submission_combined.drop(cols_target, axis=1)\n",
    "submission_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('output/LogisticRegression'):\n",
    "    os.mkdir('output/LogisticRegression')\n",
    "submission_combined.to_csv('output/LogisticRegression/evaluation_public_tfidf_200000_multilabel.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks (Keras version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text RNN\n",
    "![](https://i.imgur.com/a4bzHwS.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_rnn():\n",
    "    input_layer = Input(shape=(seq_length,), dtype='int32')\n",
    "    embedding_layer = Embedding(vocab_size, embedding_dims, input_length=seq_length)(input_layer)\n",
    "    \n",
    "    # two layers BiLSTM\n",
    "    x = Bidirectional(CuDNNGRU(rnn_recurrent_units, return_sequences=True))(embedding_layer)\n",
    "    x = Bidirectional(CuDNNGRU(rnn_recurrent_units, return_sequences=False))(x)\n",
    "    \n",
    "    x = Dense(rnn_dense_units, activation='relu')(x)\n",
    "    output_layer = Dense(rnn_output_units, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = get_text_rnn()\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text CNN\n",
    "![](https://i.imgur.com/Q7mFO4w.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_cnn():\n",
    "    input_layer = Input(shape=(seq_length,), dtype='int32')\n",
    "    embedding_layer = Embedding(vocab_size, embedding_dims, input_length=seq_length)(input_layer)\n",
    "    \n",
    "    conv_0 = Conv1D(cnn_filter_nums, 2, kernel_initializer='normal', padding='valid', activation='relu')(embedding_layer)\n",
    "    conv_1 = Conv1D(cnn_filter_nums, 3, kernel_initializer='normal', padding='valid', activation='relu')(embedding_layer)\n",
    "    conv_2 = Conv1D(cnn_filter_nums, 4, kernel_initializer='normal', padding='valid', activation='relu')(embedding_layer)\n",
    "    \n",
    "    maxpool_0 = GlobalMaxPooling1D()(conv_0)\n",
    "    maxpool_1 = GlobalMaxPooling1D()(conv_1)\n",
    "    maxpool_2 = GlobalMaxPooling1D()(conv_2)\n",
    "    \n",
    "    flatten = concatenate([maxpool_0, maxpool_1, maxpool_2])\n",
    "    h1 = Dense(units=cnn_dense_units, activation='relu')(flatten)\n",
    "    output_layer = Dense(units=cnn_output_units, activation='sigmoid')(h1)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = get_text_rnn()\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Text NN\n",
    "![](https://i.imgur.com/ueVojVL.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hybrid_textcnn():\n",
    "    input_layer = Input(shape=(seq_length,))\n",
    "    embedding_layer = Embedding(vocab_size, embedding_dims, input_length=seq_length)(input_layer)\n",
    "    \n",
    "    x = Bidirectional(CuDNNLSTM(hy_recurrent_units, return_sequences=True))(embedding_layer)\n",
    "    x = Conv1D(hy_filter_nums, 2, kernel_initalizer='normal', padding='valid', activation='relu')(x)\n",
    "    \n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    max_pool = Dropout(0.5)(max_pool)\n",
    "    \n",
    "    output_layer = Dense(hy_output_units, activation='sigmoid')(max_pool)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hy_model = get_text_rnn()\n",
    "hy_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_model_by_logloss(model, batch_size, train_x, train_y, val_x, val_y, fold_id):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    bst_model_path = \"model_ckp/Keras/KerasModel\" + str(fold_id) + '.h5'\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "    \n",
    "    # training on given fold data\n",
    "    hist = model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=50, batch_size=batch_size, shuffle=True, callbacks=[early_stopping, model_checkpoint])\n",
    "    \n",
    "    # get the minimal validation log loss on this fold\n",
    "    bst_val_score = min(hist.history['val_loss'])\n",
    "    \n",
    "    # return the model with best weight, best fold-val score\n",
    "    return model, bst_val_score\n",
    "\n",
    "def train_folds(x, y, fold_count, batch_size, get_model_func):\n",
    "    fold_size = len(x) // fold_count\n",
    "    models = []\n",
    "    score = 0\n",
    "    \n",
    "    # split the whole dataset to 'fold count' fold, and train our model on each fold\n",
    "    for fold_id in range(fold_count):\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "        \n",
    "        if fold_id == fold_size - 1:\n",
    "            fold_end = len(x)\n",
    "            \n",
    "        # Generate the train/val data for fold i\n",
    "        train_x = np.concatenate([x[:fold_start], x[fold_end:]])\n",
    "        train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n",
    "        \n",
    "        val_x = x[fold_start:fold_end]\n",
    "        val_y = y[fold_start:fold_end]\n",
    "        \n",
    "        print(\"Training on fold {}\".format(fold_id))\n",
    "        model, bst_val_score = _train_model_by_logloss(get_model_func(), batch_size, train_x, train_y, val_x, val_y, fold_id)\n",
    "        score += bst_val_score\n",
    "        models.append(model)\n",
    "    return models, score / fold_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, val_loss = train_folds(train_data_seq, train_labels, 10, cnn_batch_size, get_text_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, val_loss = train_folds(train_data_seq, train_labels, 10, rnn_batch_size, get_text_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, val_loss = train_folds(train_data_seq, train_labels, 10, hy_batch_size, get_hybrid_textcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Overall val-loss: {}\".format(val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prediction testing results ...\")\n",
    "test_prediction_list = []\n",
    "for fold_id, model in enumerate(models):\n",
    "    test_prediction = model.predict(test_data_seq, batch_size=cnn_batch_size, verbose=1)\n",
    "    test_prediction_list.append(test_prediction)\n",
    "\n",
    "# merge each folds' predictions by averaging\n",
    "test_predicts = np.zeros(test_prediction_list[0].shape)\n",
    "for fold_predict in test_prediction_list:\n",
    "    test_predicts += fold_predict\n",
    "test_predicts /= len(test_prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the submission file\n",
    "test_ids = test_df['Id'].values\n",
    "test_id_col = pd.Series(test_ids, name='Id')\n",
    "nn_submission = pd.DataFrame(data=test_predicts, columns=cols_target)\n",
    "nn_submission = pd.concat([test_id_col, nn_submission], axis=1)\n",
    "score = pd.Series(check_argmax(nn_submission, 1), name='Score')\n",
    "nn_submission = nn_submission.drop(cols_target, axis=1)\n",
    "nn_submission = pd.concat([nn_submission, score], axis=1)\n",
    "print([len(nn_submission[nn_submission['Score'] == i]) for i in range(1, 6)])\n",
    "\n",
    "nn_submission.to_csv('output/Keras_NN/submission.csv', index=False, header=False)\n",
    "print(\"MSG : Done for dumpping csv files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Model (Tensorflow Version)\n",
    "![](https://i.imgur.com/vbqLMM6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(object):\n",
    "    def __init__(self, vocab_size, batch_size, embed_size, hidden_size, seq_length, start_word):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        self.start_token = tf.constant([start_word] * self.batch_size, dtype=tf.int32)\n",
    "        self.enc_params = []\n",
    "        \n",
    "        with tf.variable_scope('encoder'):\n",
    "            self.enc_embeddings = tf.Variable(self.init_matrix([self.vocab_size, self.embed_size]))\n",
    "            self.enc_params.append(self.enc_embeddings)\n",
    "            self.forward_layer = self.recurrent_lstm_forward(self.enc_params)\n",
    "            \n",
    "        # placeholder\n",
    "        self.x = tf.placeholder(tf.int32, shape=[self.batch_size, self.seq_length])\n",
    "        \n",
    "        # initialize\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.processed_x = tf.transpose(tf.nn.embedding_lookup(self.enc_embeddings, self.x), perm=[1, 0, 2])\n",
    "            \n",
    "        h0 = tf.zeros([self.batch_size, self.hidden_size])\n",
    "        self.hidden_memory_0 = tf.stack([h0, h0])\n",
    "        \n",
    "        # training step\n",
    "        ta_embed_x_forward = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.seq_length, dynamic_size=False, infer_shape=True)\n",
    "        ta_embed_x_forward = ta_embed_x_forward.unstack(self.processed_x)\n",
    "        ta_embed_x_backward = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.seq_length, dynamic_size=False, infer_shape=True)\n",
    "        ta_embed_x_backward = ta_embed_x_backward.unstack(self.processed_x)\n",
    "        \n",
    "        def _recurrence_lstm_forward(i, x_t, h_tm):\n",
    "            h_t = self.forward_layer(x_t, h_tm)\n",
    "            x_ = ta_embed_x_forward.read(i)\n",
    "            return i + 1, x_, h_t\n",
    "        \n",
    "        def _recurrence_lstm_backward(i, x_t, h_tm):\n",
    "            h_t = self.forward_layer(x_t, h_tm)\n",
    "            x_ = ta_embed_x_backward.read(i)\n",
    "            return i - 1, x_, h_t\n",
    "        \n",
    "        _, _, hidden_memory_forward = control_flow_ops.while_loop(\n",
    "            cond=lambda i, _1, _2: i < self.seq_length,\n",
    "            body=_recurrence_lstm_forward,\n",
    "            loop_vars=(tf.constant(0, dtype=tf.int32), tf.nn.embedding_lookup(self.enc_embeddings, self.start_token), self.hidden_memory_0)\n",
    "        )\n",
    "        \n",
    "        _, _, hidden_memory_backward = control_flow_ops.while_loop(\n",
    "            cond=lambda i, _1, _2: i >= 0,\n",
    "            body=_recurrence_lstm_backward,\n",
    "            loop_vars=(tf.constant(self.seq_length - 1, dtype=tf.int32), tf.nn.embedding_lookup(self.enc_embeddings, self.start_token), self.hidden_memory_0)\n",
    "        )\n",
    "        \n",
    "        hidden_forward, _ = tf.unstack(hidden_memory_forward)\n",
    "        hidden_backward, _ = tf.unstack(hidden_memory_backward)\n",
    "        self.hidden_state = tf.concat((hidden_forward, hidden_backward), axis=1)    # shape = [batch_size, 2 * enc_hidden_size]\n",
    "            \n",
    "    def init_matrix(self, shape):\n",
    "        return tf.random_normal(shape, stddev=0.1)\n",
    "    \n",
    "    def recurrent_lstm_forward(self, params):\n",
    "        self.Wi = tf.Variable(self.init_matrix([self.embed_size, self.hidden_size]))\n",
    "        self.Ui = tf.Variable(self.init_matrix([self.hidden_size, self.hidden_size]))\n",
    "        self.bi = tf.Variable(self.init_matrix([self.hidden_size]))\n",
    "        \n",
    "        self.Wf = tf.Variable(self.init_matrix([self.embed_size, self.hidden_size]))\n",
    "        self.Uf = tf.Variable(self.init_matrix([self.hidden_size, self.hidden_size]))\n",
    "        self.bf = tf.Variable(self.init_matrix([self.hidden_size]))\n",
    "        \n",
    "        self.Wo = tf.Variable(self.init_matrix([self.embed_size, self.hidden_size]))\n",
    "        self.Uo = tf.Variable(self.init_matrix([self.hidden_size, self.hidden_size]))\n",
    "        self.bo = tf.Variable(self.init_matrix([self.hidden_size]))\n",
    "        \n",
    "        self.Wc = tf.Variable(self.init_matrix([self.embed_size, self.hidden_size]))\n",
    "        self.Uc = tf.Variable(self.init_matrix([self.hidden_size, self.hidden_size]))\n",
    "        self.bc = tf.Variable(self.init_matrix([self.hidden_size]))\n",
    "        \n",
    "        params.extend([\n",
    "            self.Wi, self.Ui, self.bi,\n",
    "            self.Wf, self.Uf, self.bf,\n",
    "            self.Wo, self.Uo, self.bo,\n",
    "            self.Wc, self.Uc, self.bc\n",
    "        ])\n",
    "        \n",
    "        def forward(x, hidden_memory):\n",
    "            hidden_state, cell_state = tf.unstack(hidden_memory)\n",
    "            \n",
    "            i = tf.sigmoid(\n",
    "                tf.matmul(x, self.Wi) + tf.matmul(hidden_state, self.Ui) + self.bi\n",
    "            )\n",
    "            \n",
    "            f = tf.sigmoid(\n",
    "                tf.matmul(x, self.Wf) + tf.matmul(hidden_state, self.Uf) + self.bf\n",
    "            )\n",
    "            \n",
    "            o = tf.sigmoid(\n",
    "                tf.matmul(x, self.Wo) + tf.matmul(hidden_state, self.Uo) + self.bo\n",
    "            )\n",
    "            \n",
    "            c_ = tf.nn.tanh(\n",
    "                tf.matmul(x, self.Wc) + tf.matmul(hidden_state, self.Uc) + self.bc\n",
    "            )\n",
    "            \n",
    "            c = f * cell_state + i * c_\n",
    "            current_hidden_state = tf.nn.tanh(c)\n",
    "            \n",
    "            return tf.stack([current_hidden_state, c])\n",
    "        \n",
    "        return forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "class Decoder(object):\n",
    "    def __init__(self, vocab_size, batch_size, embed_size, enc_hidden_size, dec_hidden_size, seq_length, start_word, learning_rate, enc_params):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.embed_size = embed_size\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "        self.seq_length =seq_length\n",
    "        self.start_token = tf.constant([start_word] * self.batch_size, dtype=tf.int32)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.dec_params = enc_params\n",
    "        self.grad_clip = 5.0\n",
    "        \n",
    "        with tf.variable_scope('decode'):\n",
    "            self.dec_embeddings = tf.Variable(self.init_matrix([self.vocab_size, self.embed_size]))\n",
    "            self.dec_params.append(self.dec_embeddings)\n",
    "            self.forward_layer = self.recurrent_lstm_forward(self.dec_params)\n",
    "            self.linear_layer = self.recurrent_linear_forward(self.dec_params)\n",
    "            \n",
    "        # placeholder\n",
    "        self.x = tf.placeholder(tf.int32, shape=[self.batch_size, self.seq_length])\n",
    "        self.hidden_state = tf.placeholder(tf.float32, shape=[self.batch_size, 2 * self.enc_hidden_size])\n",
    "        \n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.processed_x = tf.transpose(tf.nn.embedding_lookup(self.dec_embeddings, self.x), perm=[1, 0, 2])\n",
    "            \n",
    "        cell_state = tf.zeros([self.batch_size, 2 * self.enc_hidden_size])\n",
    "        self.hidden_memory_0 = tf.stack([self.hidden_state, cell_state])\n",
    "        \n",
    "        # training step\n",
    "        predictions = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.seq_length, dynamic_size=False, infer_shape=True)\n",
    "        ta_embed_x = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.seq_length, dynamic_size=False, infer_shape=True)\n",
    "        ta_embed_x = ta_embed_x.unstack(self.processed_x)\n",
    "        \n",
    "        def _training_recurrence(i, x_t, h_tm, predictions):\n",
    "            h_t = self.forward_layer(x_t, h_tm)\n",
    "            o_t = self.linear_layer(h_t)\n",
    "            predictions = predictions.write(i, o_t)\n",
    "            x_ = ta_embed_x.read(i)\n",
    "            return i + 1, x_, h_t, predictions\n",
    "        \n",
    "        _, _, _, self.predictions = control_flow_ops.while_loop(\n",
    "            cond=lambda i, _1, _2, _3: i < self.seq_length,\n",
    "            body=_training_recurrence,\n",
    "            loop_vars=(tf.constant(0, dtype=tf.int32), tf.nn.embedding_lookup(self.dec_embeddings, self.start_token), self.hidden_memory_0, predictions)\n",
    "        )\n",
    "        \n",
    "        self.predictions = self.predictions.stack()\n",
    "        self.predictions = tf.transpose(self.predictions, perm=[1, 0, 2])\n",
    "        \n",
    "        self.loss = -tf.reduce_sum(\n",
    "            tf.one_hot(tf.cast(tf.reshape(self.x, [-1]), tf.int32), self.vocab_size, 1.0, 0.0) * tf.log(\n",
    "                tf.clip_by_value(tf.reshape(self.predictions, [-1, vocab_size]), 1e-20, 1.0)\n",
    "            )\n",
    "        ) / (self.seq_length * self.batch_size)\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.gradients, _ = tf.clip_by_global_norm(tf.gradients(self.loss, self.dec_params), self.grad_clip)\n",
    "        self.update = self.optimizer.apply_gradients(zip(self.gradients, self.dec_params))\n",
    "        \n",
    "        # testing step\n",
    "        output_prob_sequences = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.seq_length, dynamic_size=False, infer_shape=True)\n",
    "        token_sequences = tensor_array_ops.TensorArray(dtype=tf.int32, size=self.seq_length, dynamic_size=False, infer_shape=True)\n",
    "        \n",
    "        def _pred_recurrence(i, x_t, h_tm, gen_o, gen_x):\n",
    "            h_t = self.forward_layer(x_t, h_tm)\n",
    "            o_t = self.linear_layer(h_t)\n",
    "            log_prob = tf.log(o_t)\n",
    "            next_token = tf.cast(tf.reshape(tf.multinomial(log_prob, 1), [self.batch_size]), tf.int32)\n",
    "            x_ = tf.nn.embedding_lookup(self.dec_embeddings, next_token)\n",
    "            gen_o = gen_o.write(i, tf.reduce_sum(tf.multiply(tf.one_hot(next_token, self.vocab_size, 1.0, 0.0), o_t), axis=1))\n",
    "            gen_x = gen_x.write(i, next_token)\n",
    "            return i + 1, x_, h_t, gen_o, gen_x\n",
    "        \n",
    "        _, _, _, self.output_prob_sequences, self.token_sequences = control_flow_ops.while_loop(\n",
    "            cond=lambda i, _1, _2, _3, _4: i < self.seq_length,\n",
    "            body=_pred_recurrence,\n",
    "            loop_vars=(tf.constant(0, dtype=tf.int32), tf.nn.embedding_lookup(self.dec_embeddings, self.start_token), self.hidden_memory_0, output_prob_sequences, token_sequences)\n",
    "        )\n",
    "        \n",
    "        self.token_sequences = self.token_sequences.stack()\n",
    "        self.token_sequences = tf.transpose(self.token_sequences, perm=[1, 0])\n",
    "            \n",
    "    def init_matrix(self, shape):\n",
    "        return tf.random_normal(shape, stddev=0.1)\n",
    "        \n",
    "    def recurrent_lstm_forward(self, params):\n",
    "        self.Wi = tf.Variable(self.init_matrix([self.embed_size, self.dec_hidden_size]))\n",
    "        self.Ui = tf.Variable(self.init_matrix([2 * self.enc_hidden_size, self.dec_hidden_size]))\n",
    "        self.bi = tf.Variable(self.init_matrix([self.dec_hidden_size]))\n",
    "        \n",
    "        self.Wf = tf.Variable(self.init_matrix([self.embed_size, self.dec_hidden_size]))\n",
    "        self.Uf = tf.Variable(self.init_matrix([2 * self.enc_hidden_size, self.dec_hidden_size]))\n",
    "        self.bf = tf.Variable(self.init_matrix([self.dec_hidden_size]))\n",
    "        \n",
    "        self.Wo = tf.Variable(self.init_matrix([self.embed_size, self.dec_hidden_size]))\n",
    "        self.Uo = tf.Variable(self.init_matrix([2 * self.enc_hidden_size, self.dec_hidden_size]))\n",
    "        self.bo = tf.Variable(self.init_matrix([self.dec_hidden_size]))\n",
    "        \n",
    "        self.Wc = tf.Variable(self.init_matrix([self.embed_size, self.dec_hidden_size]))\n",
    "        self.Uc = tf.Variable(self.init_matrix([2 * self.enc_hidden_size, self.dec_hidden_size]))\n",
    "        self.bc = tf.Variable(self.init_matrix([self.dec_hidden_size]))\n",
    "        \n",
    "        params.extend([\n",
    "            self.Wi, self.Ui, self.bi,\n",
    "            self.Wf, self.Uf, self.bf,\n",
    "            self.Wo, self.Uo, self.bo,\n",
    "            self.Wc, self.Uc, self.bc\n",
    "        ])\n",
    "        \n",
    "        def forward(x, hidden_memory):\n",
    "            hidden_state, cell_state = tf.unstack(hidden_memory)\n",
    "            \n",
    "            i = tf.sigmoid(\n",
    "                tf.matmul(x, self.Wi) + tf.matmul(hidden_state, self.Ui) + self.bi\n",
    "            )\n",
    "            \n",
    "            f = tf.sigmoid(\n",
    "                tf.matmul(x, self.Wf) + tf.matmul(hidden_state, self.Uf) + self.bf\n",
    "            )\n",
    "            \n",
    "            o = tf.sigmoid(\n",
    "                tf.matmul(x, self.Wo) + tf.matmul(hidden_state, self.Uo) + self.bo\n",
    "            )\n",
    "\n",
    "            c_ = tf.nn.tanh(\n",
    "                tf.matmul(x, self.Wc) + tf.matmul(hidden_state, self.Uc) + self.bc\n",
    "            )\n",
    "\n",
    "            c = f * cell_state + i * c_\n",
    "            current_hidden_state = tf.nn.tanh(c)\n",
    "            \n",
    "            return tf.stack([current_hidden_state, c])\n",
    "        \n",
    "        return forward\n",
    "    \n",
    "    def recurrent_linear_forward(self, params):\n",
    "        self.V = tf.Variable(self.init_matrix([self.dec_hidden_size, self.vocab_size]))\n",
    "        self.c = tf.Variable(self.init_matrix([self.vocab_size]))\n",
    "        \n",
    "        params.extend([\n",
    "            self.V, self.c\n",
    "        ])\n",
    "        \n",
    "        def forward(hidden_memory):\n",
    "            hidden_state, cell_state = tf.unstack(hidden_memory)\n",
    "            logits = tf.matmul(hidden_state, self.V) + self.c\n",
    "            output = tf.nn.softmax(logits)\n",
    "            return output\n",
    "        \n",
    "        return forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2seq Autoencoder\n",
    "class Seq2seq(object):\n",
    "    def __init__(self, vocab_size, batch_size, embed_size, enc_hidden_size, dec_hidden_size, seq_length, start_word, learning_rate):\n",
    "        self.encoder = Encoder(vocab_size, batch_size, embed_size, enc_hidden_size, seq_length, start_word)\n",
    "        self.decoder = Decoder(vocab_size, batch_size, embed_size, enc_hidden_size, dec_hidden_size, seq_length, start_word, learning_rate, self.encoder.enc_params)\n",
    "        \n",
    "    def forward(self, sess, batch):\n",
    "        hidden = sess.run(self.encoder.hidden_state, feed_dict={self.encoder.x: batch})\n",
    "        loss, _ = sess.run([self.decoder.loss, self.decoder.update], feed_dict={self.decoder.x: batch, self.decoder.hidden_state: hidden})\n",
    "        return loss\n",
    "    \n",
    "    def pred(self, sess, batch):\n",
    "        hidden = sess.run(self.encoder.hidden_state, feed_dict={self.encoder.x: batch})\n",
    "        result = sess.run(self.decoder.token_sequences, feed_dict={self.decoder.x: batch, self.decoder.hidden_state: hidden})\n",
    "        return result\n",
    "    \n",
    "    def convert_to_vector(self, sess, batch):\n",
    "        return sess.run(self.encoder.hidden_state, feed_dict={self.encoder.x: batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "seq2seq_model = Seq2seq(vocab_size, seq_batch_size, embedding_dims, enc_hidden_size, dec_hidden_size, seq_length, start_word, seq_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN classification\n",
    "class DNN_Classifier(object):\n",
    "    def __init__(self, enc_hidden_size, batch_size, label_size, learning_rate, l2_reg_lambda):\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.label_size = label_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l2_reg_lambda = l2_reg_lambda\n",
    "            \n",
    "        # placeholder\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, 2 * self.enc_hidden_size])\n",
    "        self.y = tf.placeholder(tf.int32, shape=[None, self.label_size])\n",
    "        \n",
    "        l2_loss = tf.constant(0.0, dtype=tf.float32)\n",
    "        \n",
    "        # forward\n",
    "        with tf.name_scope('output'):\n",
    "            W = tf.Variable(self.init_matrix([2 * self.enc_hidden_size, self.label_size]))\n",
    "            b = tf.Variable(self.init_matrix([self.label_size]))\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.o_t = tf.nn.xw_plus_b(self.x, W, b, name='scores')\n",
    "            self.y_pred_for_acu = tf.nn.softmax(self.o_t)\n",
    "            self.predictions = tf.argmax(self.y_pred_for_acu, 1, name='predictions')\n",
    "        \n",
    "        # loss\n",
    "        with tf.name_scope('loss'):\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.o_t, labels=self.y))\n",
    "            self.loss = loss + self.l2_reg_lambda * l2_loss\n",
    "            \n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.update = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "    def init_matrix(self, shape):\n",
    "        return tf.random_normal(shape, stddev=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "dnn_classifier = DNN_Classifier(enc_hidden_size, seq_batch_size, seq_label_size, seq_learning_rate, l2_reg_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.shape\n",
    "train_labels = train_labels[:num_batch * seq_batch_size]\n",
    "train_label_batch = np.split(train_labels, num_batch, 0)\n",
    "len(train_label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_token_sequences, val_sentences, val_num_batch = mini_batch(vocab_path, balance_validate_data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_label_batch = np.split(np.array(balance_validate_label), val_num_batch, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training config\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training seq2seq autoencoder\n",
    "print(\"MSG : Processing ...\")\n",
    "for epoch in range(train_epochs):\n",
    "    pointer = 0\n",
    "    for i in range(balance_num_batch):\n",
    "        batch, _ = next_batch(balance_train_sequences, balance_train_sentences, pointer, balance_num_batch)\n",
    "        seq_loss = seq2seq_model.forward(sess, batch)\n",
    "        if (epoch * num_batch + i) % 1000 == 0:\n",
    "            print(\"MSG : Epoch {}/{}\\tseq_loss = {}\".format(epoch * num_batch + i, train_epochs * num_batch, seq_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate autoencoder\n",
    "pointer = 0\n",
    "batch, _ = next_batch(val_token_sequences, val_sentences, pointer, val_num_batch)\n",
    "results = seq2seq_model.pred(sess, batch)\n",
    "for sent, predict in zip(batch, results):\n",
    "    sent = \"\".join([idx2word.get(word, 1) for word in sent])\n",
    "    predict = \"\".join([idx2word.get(word, 1) for word in predict])\n",
    "    print(\"source: {}\\n pred: {}\\n\".format(sent, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dnn_classifier\n",
    "print(\"MSG : Processing ...\")\n",
    "for epoch in range(train_epochs):\n",
    "    pointer = 0\n",
    "    for i in range(num_batch):\n",
    "        batch, _ = next_batch(balance_train_sequences, balance_train_sentences, pointer, balance_num_batch)\n",
    "        labels = train_label_batch[i]\n",
    "        context_vector = seq2seq_model.convert_to_vector(sess, batch)    # shape = [batch_size, 2 * enc_hidden_size]\n",
    "        output_loss, _ = sess.run([dnn_classifier.loss, dnn_classifier.update], feed_dict={dnn_classifier.x: context_vector, dnn_classifier.y: labels})\n",
    "        if (epoch * num_batch + i) % 1000 == 0:\n",
    "            print(\"MSG : Epoch {}/{}\\toutput_loss = {}\".format(epoch * num_batch + i, train_epochs * num_batch, output_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "pointer = 0\n",
    "val_results = []\n",
    "for i in tqdm(range(val_num_batch)):\n",
    "    batch, _ = next_batch(val_token_sequences, val_sentences, pointer, val_num_batch)\n",
    "    context_vector = seq2seq_model.convert_to_vector(sess, batch)\n",
    "    pred = sess.run(dnn_classifier.predictions, feed_dict={dnn_classifier.x: context_vector})\n",
    "    val_results.append(pred + 1)\n",
    "    \n",
    "val_results = np.array(val_results)\n",
    "val_label_batch = np.array(val_label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_correct = np.equal(val_results, val_label_batch)\n",
    "accuracy = np.mean(pred_correct)\n",
    "print(\"MSG : Accuracy is {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "pointer = 0\n",
    "results = []\n",
    "for i in tqdm(range(test_num_batch)):\n",
    "    batch, _ = next_batch(test_token_sequences, test_sentences, pointer, test_num_batch)\n",
    "    context_vector = seq2seq_model.convert_to_vector(sess, batch)\n",
    "    pred = sess.run(dnn_classifier.predictions, feed_dict={dnn_classifier.x: context_vector})\n",
    "    results.append(pred + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_submission = test_df\n",
    "nn_submission = nn_submission.drop('Word_Length', axis=1)\n",
    "outputs = [p for turn in results for p in turn]\n",
    "nn_submission['Score'] = outputs\n",
    "print([len(submission_combined[submission_combined['Score'] == i]) for i in range(1, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch pack_paded_sequence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
